,count,cpu_time_total,cuda_time_total,self_cpu_time_total,self_cuda_time_total,input_shapes,cpu_memory_usage,cuda_memory_usage,self_cpu_memory_usage,self_cuda_memory_usage,device_type,flops
model_inference,1,7757636,1520466,3547970,0,,236800,8583168,-170083,-243664952320,DeviceType.CPU,0
aten::empty,8015,16346,34,15728,0,,99316,952122368,99316,952122368,DeviceType.CPU,0
aten::random_,2,17,0,17,0,,0,0,0,0,DeviceType.CPU,0
aten::item,59545,818836,76094,146263,0,,0,-1273856,0,-1273856,DeviceType.CPU,0
aten::_local_scalar_dense,59545,795247,82260,80821,16446,,0,0,0,0,DeviceType.CPU,0
enumerate(DataLoader)#_SingleProcessDataLoaderIter.__next__,2,4780,0,4665,0,,236800,0,-7800,0,DeviceType.CPU,0
aten::to,10520,19973,1465,2231,0,,63200,18087936,63200,2951168,DeviceType.CPU,0
aten::lift_fresh,27428,0,0,0,0,,55954,0,55954,0,DeviceType.CPU,0
aten::detach_,4,5,0,3,0,,0,0,0,0,DeviceType.CPU,0
detach_,4,2,0,2,0,,0,0,0,0,DeviceType.CPU,0
aten::new_zeros,1,16,0,10,0,,68800,0,0,0,DeviceType.CPU,0
aten::new_empty,1,2,0,2,0,,68800,0,68800,0,DeviceType.CPU,0
aten::zero_,578,4463,1637,741,0,,0,0,0,0,DeviceType.CPU,0
aten::slice,26406,36342,0,34553,0,,0,0,0,0,DeviceType.CPU,0
aten::as_strided,285179,17555,0,17555,0,,0,0,0,0,DeviceType.CPU,0
aten::clone,689,10103,4815,2264,0,,68000,914589184,0,3584,DeviceType.CPU,0
aten::empty_like,274,1759,34,400,0,,68000,922545664,0,266240,DeviceType.CPU,0
aten::empty_strided,1346,3397,0,3397,0,,131200,26753024,131200,26753024,DeviceType.CPU,0
aten::copy_,49630,347234,154183,130756,91148,,-423,0,-423,0,DeviceType.CPU,0
aten::select,235804,198220,0,186486,0,,0,0,0,0,DeviceType.CPU,0
aten::fill_,24362,186846,46333,95208,25509,,-15484,0,-15484,0,DeviceType.CPU,0
aten::eq,2,58,2,37,2,,8608,6656,8608,6656,DeviceType.CPU,0
aten::masked_fill_,1,15,0,15,0,,0,0,0,0,DeviceType.CPU,0
aten::_to_copy,468,18470,1647,1740,0,,63200,18088448,0,0,DeviceType.CPU,0
cudaMemcpyAsync,85114,883838,94039,883838,94039,,0,0,0,0,DeviceType.CPU,0
cudaStreamSynchronize,40100,46419,41568,46419,41568,,0,0,0,0,DeviceType.CPU,0
Memcpy HtoD (Pageable -> Device),3991,0,411,0,411,,0,0,0,0,DeviceType.CUDA,0
cudaLaunchKernel,85441,332298,83075,332298,80996,,-24,186880,-24,188928,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<long, long, bool, at::native::(anonymous namespace)::CompareEqFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<long, long, bool, at::native::(anonymous namespace)::CompareEqFunctor<long> >, at::detail::Array<char*, 2>)",2,0,4,0,4,,0,0,0,0,DeviceType.CUDA,0
aten::any,1,35,5,26,5,,0,512,0,512,DeviceType.CPU,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4> >(at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::or_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4>)",1,0,5,0,5,,0,0,0,0,DeviceType.CUDA,0
Memcpy DtoH (Device -> Pageable),36109,0,16449,0,16449,,0,0,0,0,DeviceType.CUDA,0
aten::ne,1,16,2,11,2,,0,6656,0,6656,DeviceType.CPU,0
"void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)",118,0,339,0,339,,0,0,0,0,DeviceType.CUDA,0
aten::view,4985,4384,0,4384,0,,0,0,0,0,DeviceType.CPU,0
aten::embedding,232,6303,1228,1116,0,,0,123495424,0,0,DeviceType.CPU,0
aten::reshape,11577,24952,2774,21736,0,,0,838041600,0,0,DeviceType.CPU,0
aten::_reshape_alias,11547,2781,0,2781,0,,0,0,0,0,DeviceType.CPU,0
aten::index_select,2995,526830,321484,20798,319335,,0,136032877056,0,35089277952,DeviceType.CPU,0
aten::resize_,3459,491902,18,6681,0,,0,100947999232,0,100947999232,DeviceType.CPU,0
cudaStreamIsCapturing,870,43,270,43,270,,0,0,0,0,DeviceType.CPU,0
cudaMalloc,873,163062,36,163062,30,,0,400556032,0,400556032,DeviceType.CPU,0
"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long)",2863,0,315777,0,315777,,0,0,0,0,DeviceType.CUDA,0
aten::unsqueeze,1615,3017,0,2879,0,,0,0,0,0,DeviceType.CPU,0
"void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>)",347,0,1058,0,1058,,0,0,0,0,DeviceType.CUDA,0
aten::rsub,231,3190,419,761,0,,664,22146048,688,99328,DeviceType.CPU,0
aten::sub,347,3934,810,2553,633,,-32,26317824,-32,26317824,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2>)",231,0,368,0,368,,0,0,0,0,DeviceType.CUDA,0
aten::mul,4975,44738,20264,28648,15941,,-144,4037590016,-144,4037590016,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>)",462,0,793,0,793,,0,0,0,0,DeviceType.CUDA,0
aten::dropout,4396,5,0,5,0,,0,0,0,0,DeviceType.CPU,0
aten::pow,2198,26720,6830,17352,4862,,0,1955020800,0,1955020800,DeviceType.CPU,0
aten::result_type,2198,2,0,2,0,,0,0,0,0,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::pow_tensor_scalar_kernel_impl<float, float>(at::TensorIteratorBase&, float)::{lambda(float)#1}, at::detail::Array<char*, 2>)",2198,0,4862,0,4862,,0,0,0,0,DeviceType.CUDA,0
aten::mean,2198,26233,10379,19276,8099,,0,4801024,0,4801024,DeviceType.CPU,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4>)",2198,0,8099,0,8099,,0,0,0,0,DeviceType.CUDA,0
aten::add,20271,211779,79621,126270,56410,,2704,8097948672,2704,8097948672,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2>)",2198,0,3431,0,3431,,0,0,0,0,DeviceType.CUDA,0
aten::rsqrt,2198,16640,5306,9887,3505,,0,4801024,0,4801024,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::rsqrt_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",2198,0,3505,0,3505,,0,0,0,0,DeviceType.CUDA,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})",4396,0,14914,0,14914,,0,0,0,0,DeviceType.CUDA,0
aten::linear,5683,770285,295973,22401,0,,0,14101405696,0,792510464,DeviceType.CPU,0
aten::t,5683,18280,0,8646,0,,0,0,0,0,DeviceType.CPU,0
aten::transpose,11245,18319,0,16259,0,,0,0,0,0,DeviceType.CPU,0
aten::matmul,8455,828471,497186,45100,0,,0,16629179392,0,-685670400,DeviceType.CPU,0
aten::mm,5683,701835,315128,92866,292499,,0,14101405696,0,14101405696,DeviceType.CPU,0
cudaFree,656,1323658,0,1323658,0,,0,0,0,0,DeviceType.CPU,0
cudaDeviceGetAttribute,707,7,320,7,320,,0,0,0,0,DeviceType.CPU,0
cudaGetSymbolAddress,1,0,0,0,0,,0,0,0,0,DeviceType.CPU,0
cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,15357,498,16200,498,16001,,0,0,0,0,DeviceType.CPU,0
ampere_sgemm_128x64_tn,1543,0,185806,0,185806,,0,0,0,0,DeviceType.CUDA,0
aten::_unsafe_view,8485,2912,0,2912,0,,0,0,0,0,DeviceType.CPU,0
aten::expand,5892,6851,0,6321,0,,0,0,0,0,DeviceType.CPU,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",36,0,2296,0,2296,,0,0,0,0,DeviceType.CUDA,0
aten::bmm,2772,47528,179284,34193,174915,,0,2375402496,0,2375402496,DeviceType.CPU,0
ampere_sgemm_128x128_nn,12,0,1173,0,1173,,0,0,0,0,DeviceType.CUDA,0
aten::arange,694,8121,1426,2082,524,,0,459776,0,28672,DeviceType.CPU,0
"void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*)",347,0,524,0,524,,0,0,0,0,DeviceType.CUDA,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<long> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<long> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<long> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<long> const&)::{lambda(int)#1})",115,0,263,0,263,,0,0,0,0,DeviceType.CUDA,0
aten::gt,1,20,2,15,2,,0,4096,0,4096,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2> >(int, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2>)",142,0,252,0,252,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 2>)",2,0,4,0,4,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",4342,0,8406,0,8406,,0,0,0,0,DeviceType.CUDA,0
aten::abs,2,35,12,12,2,,0,62464,0,0,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::AbsFunctor<long>, at::detail::Array<char*, 2> >(int, at::native::AbsFunctor<long>, at::detail::Array<char*, 2>)",1,0,2,0,2,,0,0,0,0,DeviceType.CUDA,0
aten::lt,216,2584,829,1733,328,,0,599552,24,599552,DeviceType.CPU,0
aten::div,347,3411,1409,2184,667,,40,4969984,40,4969984,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>)",232,0,378,0,378,,0,0,0,0,DeviceType.CUDA,0
aten::log,116,960,256,580,230,,0,2102272,0,2102272,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::log_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::log_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",116,0,230,0,230,,0,0,0,0,DeviceType.CUDA,0
aten::full_like,116,1357,168,313,0,,0,4171776,0,0,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<long>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<long>, at::detail::Array<char*, 1>)",16088,0,16838,0,16838,,0,0,0,0,DeviceType.CUDA,0
aten::min,232,2173,630,396,3,,0,8312832,0,1032704,DeviceType.CPU,0
aten::minimum,231,1968,677,1236,462,,0,8312320,0,8312320,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<long, long, long, at::native::minimum_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<long, long, long, at::native::minimum_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 3>)",231,0,462,0,462,,0,0,0,0,DeviceType.CUDA,0
aten::where,116,1708,340,918,233,,0,4171776,0,9728,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::where_kernel_impl(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(bool, long, long)#1}, at::detail::Array<char*, 4> >(int, at::native::(anonymous namespace)::where_kernel_impl(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(bool, long, long)#1}, at::detail::Array<char*, 4>)",116,0,233,0,233,,0,0,0,0,DeviceType.CUDA,0
aten::add_,1387,14168,5935,8850,4859,,0,0,0,0,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<long>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<long>, at::detail::Array<char*, 3>)",2,0,4,0,4,,0,0,0,0,DeviceType.CUDA,0
aten::permute,116,385,0,364,0,,0,0,0,0,DeviceType.CPU,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",1036,0,18005,0,18005,,0,0,0,0,DeviceType.CUDA,0
aten::softmax,1386,14789,5440,2387,0,,0,1197043712,0,96845824,DeviceType.CPU,0
aten::_softmax,1386,13462,5953,8132,4533,,0,1197043712,0,1197043712,DeviceType.CPU,0
aten::type_as,1386,284,0,284,0,,0,0,0,0,DeviceType.CPU,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 6, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",888,0,3120,0,3120,,0,0,0,0,DeviceType.CUDA,0
aten::contiguous,12,160,179,13,0,,0,76191744,0,12697600,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",2778,0,9469,0,9469,,0,0,0,0,DeviceType.CUDA,0
cudaMemsetAsync,1634,5812,1670,5812,1670,,0,17701376,0,17701376,DeviceType.CPU,0
aten::relu,696,9761,5472,2045,0,,0,2611544064,0,0,DeviceType.CPU,0
aten::clamp_min,696,7716,5472,4709,5054,,0,2611544064,0,2611544064,DeviceType.CPU,0
Memset (Device),1634,0,342,0,342,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",696,0,5054,0,5054,,0,0,0,0,DeviceType.CUDA,0
aten::ones,230,2876,419,833,0,,0,21213184,0,238592,DeviceType.CPU,0
aten::repeat_interleave,6,911,272,70,6,,0,51003904,0,-22016,DeviceType.CPU,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1})",121,0,243,0,243,,0,0,0,0,DeviceType.CUDA,0
aten::cumsum,3,99,21,51,11,,0,4096,0,1024,DeviceType.CPU,0
cudaPeekAtLastError,932,0,1118,0,1118,,0,-18432,0,-18432,DeviceType.CPU,0
"void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, true> >(at_cuda_detail::cub::ScanTileState<long, true>, int)",3,0,4,0,4,,0,0,0,0,DeviceType.CUDA,0
"void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long>::Policy600, long*, long*, at_cuda_detail::cub::ScanTileState<long, true>, std::plus<long>, at_cuda_detail::cub::NullType, int>(long*, long*, at_cuda_detail::cub::ScanTileState<long, true>, int, std::plus<long>, at_cuda_detail::cub::NullType, int)",3,0,7,0,7,,0,0,0,0,DeviceType.CUDA,0
aten::ge,3,40,8,27,6,,0,1536,0,1536,DeviceType.CPU,0
"void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1} const&)::{lambda(int)#1})",3,0,6,0,6,,0,0,0,0,DeviceType.CUDA,0
aten::all,118,1743,533,1294,524,,0,60416,0,60416,DeviceType.CPU,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::and_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4> >(at::native::ReduceOp<bool, at::native::func_wrapper_t<bool, at::native::and_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#12}::operator()() const::{lambda(bool, bool)#1}>, unsigned int, bool, 4>)",118,0,524,0,524,,0,0,0,0,DeviceType.CUDA,0
"void compute_cuda_kernel<long>(long*, long*, long*, long, long)",3,0,6,0,6,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::indexSelectLargeIndex<long, long, unsigned int, 1, 1, -2, true>(at::cuda::detail::TensorInfo<long, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long)",1,0,3,0,3,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::indexSelectLargeIndex<long, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<long, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long)",1,0,3,0,3,,0,0,0,0,DeviceType.CUDA,0
aten::zeros,462,5538,1207,1230,0,,0,14547456,0,2048,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",8272,0,8669,0,8669,,0,0,0,0,DeviceType.CUDA,0
"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::FillFunctor<float> >(at::TensorIteratorBase&, at::native::FillFunctor<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::FillFunctor<float> >(at::TensorIteratorBase&, at::native::FillFunctor<float> const&)::{lambda(int)#1})",1,0,2,0,2,,0,0,0,0,DeviceType.CUDA,0
aten::repeat,115,3406,263,1059,0,,0,412160,0,0,DeviceType.CPU,0
aten::alias,115,14,0,14,0,,0,0,0,0,DeviceType.CPU,0
aten::unfold,345,362,0,330,0,,0,0,0,0,DeviceType.CPU,0
aten::expand_as,230,453,0,200,0,,0,0,0,0,DeviceType.CPU,0
aten::le,115,1458,421,927,344,,0,58880,0,58880,DeviceType.CPU,0
"void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1})",115,0,344,0,344,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",115,0,230,0,230,,0,0,0,0,DeviceType.CUDA,0
ampere_sgemm_128x32_tn,4140,0,84501,0,84501,,0,0,0,0,DeviceType.CUDA,0
"void splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",4140,0,21955,0,21955,,0,0,0,0,DeviceType.CUDA,0
"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 5, false, cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",1182,0,65766,0,65766,,0,0,0,0,DeviceType.CUDA,0
aten::zeros_like,115,1701,342,367,0,,0,4140544,0,-266240,DeviceType.CPU,0
aten::neg,115,1001,453,621,222,,0,4140544,0,4140544,DeviceType.CPU,0
"void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>)",115,0,222,0,222,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::indexSelectSmallIndex<float, long, unsigned int, 2, 2, -2>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, long)",4,0,21,0,21,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 0, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",6,0,12,0,12,,0,0,0,0,DeviceType.CUDA,0
"void gemmk1_kernel<int, float, 256, 5, false, false, false, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, 0>(cublasGemmk1Params<float, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, biasType<cublasGemvTensorStridedBatched<float>::value_type, float>::type>)",6,0,72,0,72,,0,0,0,0,DeviceType.CUDA,0
"void gemv2N_kernel<int, int, float, float, float, float, 128, 2, 4, 4, 1, false, cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",1380,0,88406,0,88406,,0,0,0,0,DeviceType.CUDA,0
aten::log_softmax,115,1621,27565,181,0,,0,6029312000,0,157286400,DeviceType.CPU,0
aten::_log_softmax,115,1480,28299,744,28284,,0,6029312000,0,6029312000,DeviceType.CPU,0
aten::topk,115,11888,56816,6419,52318,,0,1177600,0,-16694272,DeviceType.CPU,0
cudaFuncGetAttributes,115,135,42,135,14,,0,0,0,0,DeviceType.CPU,0
aten::remainder,115,1351,340,924,236,,256,765440,256,765440,DeviceType.CPU,0
aten::is_nonzero,15504,587445,29666,41438,0,,0,-1024,0,-1024,DeviceType.CPU,0
"void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, float, float, at::native::(anonymous namespace)::LogSoftMaxForwardEpilogue>(float*, float*, int)",115,0,28284,0,28284,,0,0,0,0,DeviceType.CUDA,0
"void at::native::mbtopk::fill<unsigned int, unsigned int>(unsigned int*, unsigned int, unsigned int)",115,0,227,0,227,,0,0,0,0,DeviceType.CUDA,0
"void at::native::mbtopk::radixFindKthValues<float, unsigned int, unsigned int, 2>(at::cuda::detail::TensorInfo<float, unsigned int>, unsigned int, unsigned int*, unsigned int, unsigned int, int, int, unsigned int, unsigned int, unsigned int*, unsigned int*, short*, float*)",460,0,32185,0,32185,,0,0,0,0,DeviceType.CUDA,0
"void at::native::mbtopk::computeBlockwiseWithinKCounts<unsigned int>(unsigned int*, short*, unsigned int, int, bool, unsigned int*, unsigned int)",460,0,2048,0,2048,,0,0,0,0,DeviceType.CUDA,0
"void at::native::mbtopk::computeBlockwiseKthCounts<unsigned int>(unsigned int*, short*, unsigned int, unsigned int, unsigned int*)",115,0,382,0,382,,0,0,0,0,DeviceType.CUDA,0
"void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ReduceByKeyScanTileState<unsigned int, int, true> >(at_cuda_detail::cub::ReduceByKeyScanTileState<unsigned int, int, true>, int)",230,0,253,0,253,,0,0,0,0,DeviceType.CUDA,0
"void at_cuda_detail::cub::DeviceScanByKeyKernel<at_cuda_detail::cub::DeviceScanByKeyPolicy<at_cuda_detail::cub::TransformInputIterator<unsigned int, at::native::mbtopk::BlockIdxToKey, at_cuda_detail::cub::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int*, at_cuda_detail::cub::NullType>::Policy520, at_cuda_detail::cub::TransformInputIterator<unsigned int, at::native::mbtopk::BlockIdxToKey, at_cuda_detail::cub::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int*, unsigned int*, at_cuda_detail::cub::ReduceByKeyScanTileState<unsigned int, int, true>, at_cuda_detail::cub::Equality, at_cuda_detail::cub::Sum, at_cuda_detail::cub::NullType, int>(at_cuda_detail::cub::TransformInputIterator<unsigned int, at::native::mbtopk::BlockIdxToKey, at_cuda_detail::cub::CountingInputIterator<unsigned int, unsigned int>, long>, unsigned int*, unsigned int*, at_cuda_detail::cub::ReduceByKeyScanTileState<unsigned int, int, true>, int, at_cuda_detail::cub::Equality, at_cuda_detail::cub::Sum, at_cuda_detail::cub::NullType, int)",230,0,758,0,758,,0,0,0,0,DeviceType.CUDA,0
"void at::native::mbtopk::gatherTopK<float, unsigned int, 2>(at::cuda::detail::TensorInfo<float, unsigned int>, unsigned int, unsigned int, bool, unsigned int, unsigned int, at::cuda::detail::TensorInfo<float, unsigned int>, unsigned int, at::cuda::detail::TensorInfo<long, unsigned int>, unsigned int, unsigned int, unsigned int, float*, unsigned int*, unsigned int*, unsigned int)",115,0,15655,0,15655,,0,0,0,0,DeviceType.CUDA,0
"void at::native::bitonicSortKVInPlace<2, -1, 16, 16, float, long, at::native::GTOp<float, true>, unsigned int>(at::cuda::detail::TensorInfo<float, unsigned int>, unsigned int, unsigned int, unsigned int, at::cuda::detail::TensorInfo<long, unsigned int>, unsigned int, at::native::GTOp<float, true>)",115,0,705,0,705,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<long, long, long, at::native::binary_internal::div_floor_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<long, long, long, at::native::binary_internal::div_floor_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)",115,0,289,0,289,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<long, long, long, at::native::remainder_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<long, long, long, at::native::remainder_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)",115,0,236,0,236,,0,0,0,0,DeviceType.CUDA,0
aten::unbind,11067,108256,0,94180,0,,0,0,0,0,DeviceType.CPU,0
Memcpy DtoD (Device -> Device),45014,0,86798,0,86798,,0,0,0,0,DeviceType.CUDA,0
"void at::native::unrolled_elementwise_kernel<at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)",7565,0,14657,0,14657,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<2, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",3738,0,7299,0,7299,,0,0,0,0,DeviceType.CUDA,0
aten::max,3691,58080,14366,31574,10903,,0,1889792,0,10240,DeviceType.CPU,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::MaxNanFunctor<float> >, unsigned int, float, 4>)",3689,0,10897,0,10897,,0,0,0,0,DeviceType.CUDA,0
aten::index,115,2431,765,1460,599,,0,21366272,0,21366272,DeviceType.CPU,0
"void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIteratorBase&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})",115,0,599,0,599,,0,0,0,0,DeviceType.CUDA,0
aten::cat,1597,459260,167133,16938,164779,,0,65795872768,0,65395316736,DeviceType.CPU,0
"void at::native::(anonymous namespace)::CatArrayBatchedCopy<long, unsigned int, 2, 128, 1>(long*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<long, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",115,0,347,0,347,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, -1, -1, -1, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long)",126,0,3531,0,3531,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::CatArrayBatchedCopy<float, unsigned int, 3, 128, 1>(float*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<float, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",114,0,355,0,355,,0,0,0,0,DeviceType.CUDA,0
"void at::native::(anonymous namespace)::CatArrayBatchedCopy<float, unsigned int, 4, 128, 1>(float*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<float, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",1368,0,164077,0,164077,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 1, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",6,0,12,0,12,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",12,0,24,0,24,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",24,0,48,0,48,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 4, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",48,0,100,0,100,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 5, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",96,0,288,0,288,,0,0,0,0,DeviceType.CUDA,0
"void (anonymous namespace)::softmax_warp_forward<float, float, float, 7, false, false>(float*, float const*, int, int, int, bool const*, int, bool)",306,0,929,0,929,,0,0,0,0,DeviceType.CUDA,0
"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 6, false, cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParams<cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",192,0,19498,0,19498,,0,0,0,0,DeviceType.CUDA,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::MaxNanFunctor<long> >, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::MaxNanFunctor<long> >, unsigned int, long, 4>)",2,0,6,0,6,,0,0,0,0,DeviceType.CUDA,0
"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::MinNanFunctor<long> >, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::MinNanFunctor<long> >, unsigned int, long, 4>)",1,0,3,0,3,,0,0,0,0,DeviceType.CUDA,0
"void at::native::unrolled_elementwise_kernel<at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)",50,0,53,0,53,,0,0,0,0,DeviceType.CUDA,0
"void at::native::vectorized_elementwise_kernel<2, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2> >(int, at::native::compare_scalar_kernel<long>(at::TensorIteratorBase&, at::native::(anonymous namespace)::OpType, long)::{lambda(long)#1}, at::detail::Array<char*, 2>)",25,0,25,0,25,,0,0,0,0,DeviceType.CUDA,0
aten::detach,2,4,0,2,0,,0,0,0,0,DeviceType.CPU,0
detach,2,2,0,2,0,,0,0,0,0,DeviceType.CPU,0
aten::resolve_conj,2,0,0,0,0,,0,0,0,0,DeviceType.CPU,0
aten::resolve_neg,2,0,0,0,0,,0,0,0,0,DeviceType.CPU,0
cudaDeviceSynchronize,1,14,0,14,0,,0,0,0,0,DeviceType.CPU,0
