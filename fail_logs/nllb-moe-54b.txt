ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    # If experiment runs successfully, break out of the loop and continue with the next experiment.
  File "benchmark_suite.py", line 384, in call_batched
    # If path already exists for recording latencies using decorated function, it is deleted. This is to avoid rewrapping the model twice.
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 25802252), ('active.all.current', 78), ('active.all.freed', 25802174), ('active.all.peak', 456), ('active.large_pool.allocated', 2419962), ('active.large_pool.current', 64), ('active.large_pool.freed', 2419898), ('active.large_pool.peak', 188), ('active.small_pool.allocated', 23382290), ('active.small_pool.current', 14), ('active.small_pool.freed', 23382276), ('active.small_pool.peak', 341), ('active_bytes.all.allocated', 32390711847936), ('active_bytes.all.current', 1012137984), ('active_bytes.all.freed', 32389699709952), ('active_bytes.all.peak', 4089958400), ('active_bytes.large_pool.allocated', 29522133052416), ('active_bytes.large_pool.current', 1007362048), ('active_bytes.large_pool.freed', 29521125690368), ('active_bytes.large_pool.peak', 4077789184), ('active_bytes.small_pool.allocated', 2868578795520), ('active_bytes.small_pool.current', 4775936), ('active_bytes.small_pool.freed', 2868574019584), ('active_bytes.small_pool.peak', 79264256), ('allocated_bytes.all.allocated', 32390711847936), ('allocated_bytes.all.current', 1012137984), ('allocated_bytes.all.freed', 32389699709952), ('allocated_bytes.all.peak', 4089958400), ('allocated_bytes.large_pool.allocated', 29522133052416), ('allocated_bytes.large_pool.current', 1007362048), ('allocated_bytes.large_pool.freed', 29521125690368), ('allocated_bytes.large_pool.peak', 4077789184), ('allocated_bytes.small_pool.allocated', 2868578795520), ('allocated_bytes.small_pool.current', 4775936), ('allocated_bytes.small_pool.freed', 2868574019584), ('allocated_bytes.small_pool.peak', 79264256), ('allocation.all.allocated', 25802252), ('allocation.all.current', 78), ('allocation.all.freed', 25802174), ('allocation.all.peak', 456), ('allocation.large_pool.allocated', 2419962), ('allocation.large_pool.current', 64), ('allocation.large_pool.freed', 2419898), ('allocation.large_pool.peak', 188), ('allocation.small_pool.allocated', 23382290), ('allocation.small_pool.current', 14), ('allocation.small_pool.freed', 23382276), ('allocation.small_pool.peak', 341), ('inactive_split.all.allocated', 12906812), ('inactive_split.all.current', 79), ('inactive_split.all.freed', 12906733), ('inactive_split.all.peak', 212), ('inactive_split.large_pool.allocated', 1476639), ('inactive_split.large_pool.current', 59), ('inactive_split.large_pool.freed', 1476580), ('inactive_split.large_pool.peak', 187), ('inactive_split.small_pool.allocated', 11430173), ('inactive_split.small_pool.current', 20), ('inactive_split.small_pool.freed', 11430153), ('inactive_split.small_pool.peak', 66), ('inactive_split_bytes.all.allocated', 14043722753024), ('inactive_split_bytes.all.current', 332136448), ('inactive_split_bytes.all.freed', 14043390616576), ('inactive_split_bytes.all.peak', 900622848), ('inactive_split_bytes.large_pool.allocated', 11117891939328), ('inactive_split_bytes.large_pool.current', 322232320), ('inactive_split_bytes.large_pool.freed', 11117569707008), ('inactive_split_bytes.large_pool.peak', 892723200), ('inactive_split_bytes.small_pool.allocated', 2925830813696), ('inactive_split_bytes.small_pool.current', 9904128), ('inactive_split_bytes.small_pool.freed', 2925820909568), ('inactive_split_bytes.small_pool.peak', 29399552), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 31986341438876), ('requested_bytes.all.current', 1012137984), ('requested_bytes.all.freed', 31985329300892), ('requested_bytes.all.peak', 4065634728), ('requested_bytes.large_pool.allocated', 29120937318016), ('requested_bytes.large_pool.current', 1007362048), ('requested_bytes.large_pool.freed', 29119929955968), ('requested_bytes.large_pool.peak', 4053508096), ('requested_bytes.small_pool.allocated', 2865404120860), ('requested_bytes.small_pool.current', 4775936), ('requested_bytes.small_pool.freed', 2865399344924), ('requested_bytes.small_pool.peak', 79223246), ('reserved_bytes.all.allocated', 1867562090496), ('reserved_bytes.all.current', 1344274432), ('reserved_bytes.all.freed', 1866217816064), ('reserved_bytes.all.peak', 9841934336), ('reserved_bytes.large_pool.allocated', 1848452841472), ('reserved_bytes.large_pool.current', 1329594368), ('reserved_bytes.large_pool.freed', 1847123247104), ('reserved_bytes.large_pool.peak', 9751756800), ('reserved_bytes.small_pool.allocated', 19109249024), ('reserved_bytes.small_pool.current', 14680064), ('reserved_bytes.small_pool.freed', 19094568960), ('reserved_bytes.small_pool.peak', 96468992), ('segment.all.allocated', 101598), ('segment.all.current', 64), ('segment.all.freed', 101534), ('segment.all.peak', 545), ('segment.large_pool.allocated', 92486), ('segment.large_pool.current', 57), ('segment.large_pool.freed', 92429), ('segment.large_pool.peak', 502), ('segment.small_pool.allocated', 9112), ('segment.small_pool.current', 7), ('segment.small_pool.freed', 9105), ('segment.small_pool.peak', 46)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    # If experiment runs successfully, break out of the loop and continue with the next experiment.
  File "benchmark_suite.py", line 384, in call_batched
    # If path already exists for recording latencies using decorated function, it is deleted. This is to avoid rewrapping the model twice.
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 25802252), ('active.all.current', 78), ('active.all.freed', 25802174), ('active.all.peak', 456), ('active.large_pool.allocated', 2419962), ('active.large_pool.current', 64), ('active.large_pool.freed', 2419898), ('active.large_pool.peak', 188), ('active.small_pool.allocated', 23382290), ('active.small_pool.current', 14), ('active.small_pool.freed', 23382276), ('active.small_pool.peak', 341), ('active_bytes.all.allocated', 32390711847936), ('active_bytes.all.current', 1012137984), ('active_bytes.all.freed', 32389699709952), ('active_bytes.all.peak', 4089958400), ('active_bytes.large_pool.allocated', 29522133052416), ('active_bytes.large_pool.current', 1007362048), ('active_bytes.large_pool.freed', 29521125690368), ('active_bytes.large_pool.peak', 4077789184), ('active_bytes.small_pool.allocated', 2868578795520), ('active_bytes.small_pool.current', 4775936), ('active_bytes.small_pool.freed', 2868574019584), ('active_bytes.small_pool.peak', 79264256), ('allocated_bytes.all.allocated', 32390711847936), ('allocated_bytes.all.current', 1012137984), ('allocated_bytes.all.freed', 32389699709952), ('allocated_bytes.all.peak', 4089958400), ('allocated_bytes.large_pool.allocated', 29522133052416), ('allocated_bytes.large_pool.current', 1007362048), ('allocated_bytes.large_pool.freed', 29521125690368), ('allocated_bytes.large_pool.peak', 4077789184), ('allocated_bytes.small_pool.allocated', 2868578795520), ('allocated_bytes.small_pool.current', 4775936), ('allocated_bytes.small_pool.freed', 2868574019584), ('allocated_bytes.small_pool.peak', 79264256), ('allocation.all.allocated', 25802252), ('allocation.all.current', 78), ('allocation.all.freed', 25802174), ('allocation.all.peak', 456), ('allocation.large_pool.allocated', 2419962), ('allocation.large_pool.current', 64), ('allocation.large_pool.freed', 2419898), ('allocation.large_pool.peak', 188), ('allocation.small_pool.allocated', 23382290), ('allocation.small_pool.current', 14), ('allocation.small_pool.freed', 23382276), ('allocation.small_pool.peak', 341), ('inactive_split.all.allocated', 12906812), ('inactive_split.all.current', 79), ('inactive_split.all.freed', 12906733), ('inactive_split.all.peak', 212), ('inactive_split.large_pool.allocated', 1476639), ('inactive_split.large_pool.current', 59), ('inactive_split.large_pool.freed', 1476580), ('inactive_split.large_pool.peak', 187), ('inactive_split.small_pool.allocated', 11430173), ('inactive_split.small_pool.current', 20), ('inactive_split.small_pool.freed', 11430153), ('inactive_split.small_pool.peak', 66), ('inactive_split_bytes.all.allocated', 14043722753024), ('inactive_split_bytes.all.current', 332136448), ('inactive_split_bytes.all.freed', 14043390616576), ('inactive_split_bytes.all.peak', 900622848), ('inactive_split_bytes.large_pool.allocated', 11117891939328), ('inactive_split_bytes.large_pool.current', 322232320), ('inactive_split_bytes.large_pool.freed', 11117569707008), ('inactive_split_bytes.large_pool.peak', 892723200), ('inactive_split_bytes.small_pool.allocated', 2925830813696), ('inactive_split_bytes.small_pool.current', 9904128), ('inactive_split_bytes.small_pool.freed', 2925820909568), ('inactive_split_bytes.small_pool.peak', 29399552), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 31986341438876), ('requested_bytes.all.current', 1012137984), ('requested_bytes.all.freed', 31985329300892), ('requested_bytes.all.peak', 4065634728), ('requested_bytes.large_pool.allocated', 29120937318016), ('requested_bytes.large_pool.current', 1007362048), ('requested_bytes.large_pool.freed', 29119929955968), ('requested_bytes.large_pool.peak', 4053508096), ('requested_bytes.small_pool.allocated', 2865404120860), ('requested_bytes.small_pool.current', 4775936), ('requested_bytes.small_pool.freed', 2865399344924), ('requested_bytes.small_pool.peak', 79223246), ('reserved_bytes.all.allocated', 1867562090496), ('reserved_bytes.all.current', 1344274432), ('reserved_bytes.all.freed', 1866217816064), ('reserved_bytes.all.peak', 9841934336), ('reserved_bytes.large_pool.allocated', 1848452841472), ('reserved_bytes.large_pool.current', 1329594368), ('reserved_bytes.large_pool.freed', 1847123247104), ('reserved_bytes.large_pool.peak', 9751756800), ('reserved_bytes.small_pool.allocated', 19109249024), ('reserved_bytes.small_pool.current', 14680064), ('reserved_bytes.small_pool.freed', 19094568960), ('reserved_bytes.small_pool.peak', 96468992), ('segment.all.allocated', 101598), ('segment.all.current', 64), ('segment.all.freed', 101534), ('segment.all.peak', 545), ('segment.large_pool.allocated', 92486), ('segment.large_pool.current', 57), ('segment.large_pool.freed', 92429), ('segment.large_pool.peak', 502), ('segment.small_pool.allocated', 9112), ('segment.small_pool.current', 7), ('segment.small_pool.freed', 9105), ('segment.small_pool.peak', 46)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    # If experiment runs successfully, break out of the loop and continue with the next experiment.
  File "benchmark_suite.py", line 384, in call_batched
    # If path already exists for recording latencies using decorated function, it is deleted. This is to avoid rewrapping the model twice.
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 25802252), ('active.all.current', 78), ('active.all.freed', 25802174), ('active.all.peak', 456), ('active.large_pool.allocated', 2419962), ('active.large_pool.current', 64), ('active.large_pool.freed', 2419898), ('active.large_pool.peak', 188), ('active.small_pool.allocated', 23382290), ('active.small_pool.current', 14), ('active.small_pool.freed', 23382276), ('active.small_pool.peak', 341), ('active_bytes.all.allocated', 32390711847936), ('active_bytes.all.current', 1012137984), ('active_bytes.all.freed', 32389699709952), ('active_bytes.all.peak', 4089958400), ('active_bytes.large_pool.allocated', 29522133052416), ('active_bytes.large_pool.current', 1007362048), ('active_bytes.large_pool.freed', 29521125690368), ('active_bytes.large_pool.peak', 4077789184), ('active_bytes.small_pool.allocated', 2868578795520), ('active_bytes.small_pool.current', 4775936), ('active_bytes.small_pool.freed', 2868574019584), ('active_bytes.small_pool.peak', 79264256), ('allocated_bytes.all.allocated', 32390711847936), ('allocated_bytes.all.current', 1012137984), ('allocated_bytes.all.freed', 32389699709952), ('allocated_bytes.all.peak', 4089958400), ('allocated_bytes.large_pool.allocated', 29522133052416), ('allocated_bytes.large_pool.current', 1007362048), ('allocated_bytes.large_pool.freed', 29521125690368), ('allocated_bytes.large_pool.peak', 4077789184), ('allocated_bytes.small_pool.allocated', 2868578795520), ('allocated_bytes.small_pool.current', 4775936), ('allocated_bytes.small_pool.freed', 2868574019584), ('allocated_bytes.small_pool.peak', 79264256), ('allocation.all.allocated', 25802252), ('allocation.all.current', 78), ('allocation.all.freed', 25802174), ('allocation.all.peak', 456), ('allocation.large_pool.allocated', 2419962), ('allocation.large_pool.current', 64), ('allocation.large_pool.freed', 2419898), ('allocation.large_pool.peak', 188), ('allocation.small_pool.allocated', 23382290), ('allocation.small_pool.current', 14), ('allocation.small_pool.freed', 23382276), ('allocation.small_pool.peak', 341), ('inactive_split.all.allocated', 12906812), ('inactive_split.all.current', 79), ('inactive_split.all.freed', 12906733), ('inactive_split.all.peak', 212), ('inactive_split.large_pool.allocated', 1476639), ('inactive_split.large_pool.current', 59), ('inactive_split.large_pool.freed', 1476580), ('inactive_split.large_pool.peak', 187), ('inactive_split.small_pool.allocated', 11430173), ('inactive_split.small_pool.current', 20), ('inactive_split.small_pool.freed', 11430153), ('inactive_split.small_pool.peak', 66), ('inactive_split_bytes.all.allocated', 14043722753024), ('inactive_split_bytes.all.current', 332136448), ('inactive_split_bytes.all.freed', 14043390616576), ('inactive_split_bytes.all.peak', 900622848), ('inactive_split_bytes.large_pool.allocated', 11117891939328), ('inactive_split_bytes.large_pool.current', 322232320), ('inactive_split_bytes.large_pool.freed', 11117569707008), ('inactive_split_bytes.large_pool.peak', 892723200), ('inactive_split_bytes.small_pool.allocated', 2925830813696), ('inactive_split_bytes.small_pool.current', 9904128), ('inactive_split_bytes.small_pool.freed', 2925820909568), ('inactive_split_bytes.small_pool.peak', 29399552), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 31986341438876), ('requested_bytes.all.current', 1012137984), ('requested_bytes.all.freed', 31985329300892), ('requested_bytes.all.peak', 4065634728), ('requested_bytes.large_pool.allocated', 29120937318016), ('requested_bytes.large_pool.current', 1007362048), ('requested_bytes.large_pool.freed', 29119929955968), ('requested_bytes.large_pool.peak', 4053508096), ('requested_bytes.small_pool.allocated', 2865404120860), ('requested_bytes.small_pool.current', 4775936), ('requested_bytes.small_pool.freed', 2865399344924), ('requested_bytes.small_pool.peak', 79223246), ('reserved_bytes.all.allocated', 1867562090496), ('reserved_bytes.all.current', 1344274432), ('reserved_bytes.all.freed', 1866217816064), ('reserved_bytes.all.peak', 9841934336), ('reserved_bytes.large_pool.allocated', 1848452841472), ('reserved_bytes.large_pool.current', 1329594368), ('reserved_bytes.large_pool.freed', 1847123247104), ('reserved_bytes.large_pool.peak', 9751756800), ('reserved_bytes.small_pool.allocated', 19109249024), ('reserved_bytes.small_pool.current', 14680064), ('reserved_bytes.small_pool.freed', 19094568960), ('reserved_bytes.small_pool.peak', 96468992), ('segment.all.allocated', 101598), ('segment.all.current', 64), ('segment.all.freed', 101534), ('segment.all.peak', 545), ('segment.large_pool.allocated', 92486), ('segment.large_pool.current', 57), ('segment.large_pool.freed', 92429), ('segment.large_pool.peak', 502), ('segment.small_pool.allocated', 9112), ('segment.small_pool.current', 7), ('segment.small_pool.freed', 9105), ('segment.small_pool.peak', 46)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    # If experiment runs successfully, break out of the loop and continue with the next experiment.
  File "benchmark_suite.py", line 384, in call_batched
    # If path already exists for recording latencies using decorated function, it is deleted. This is to avoid rewrapping the model twice.
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 25802252), ('active.all.current', 78), ('active.all.freed', 25802174), ('active.all.peak', 456), ('active.large_pool.allocated', 2419962), ('active.large_pool.current', 64), ('active.large_pool.freed', 2419898), ('active.large_pool.peak', 188), ('active.small_pool.allocated', 23382290), ('active.small_pool.current', 14), ('active.small_pool.freed', 23382276), ('active.small_pool.peak', 341), ('active_bytes.all.allocated', 32390711847936), ('active_bytes.all.current', 1012137984), ('active_bytes.all.freed', 32389699709952), ('active_bytes.all.peak', 4089958400), ('active_bytes.large_pool.allocated', 29522133052416), ('active_bytes.large_pool.current', 1007362048), ('active_bytes.large_pool.freed', 29521125690368), ('active_bytes.large_pool.peak', 4077789184), ('active_bytes.small_pool.allocated', 2868578795520), ('active_bytes.small_pool.current', 4775936), ('active_bytes.small_pool.freed', 2868574019584), ('active_bytes.small_pool.peak', 79264256), ('allocated_bytes.all.allocated', 32390711847936), ('allocated_bytes.all.current', 1012137984), ('allocated_bytes.all.freed', 32389699709952), ('allocated_bytes.all.peak', 4089958400), ('allocated_bytes.large_pool.allocated', 29522133052416), ('allocated_bytes.large_pool.current', 1007362048), ('allocated_bytes.large_pool.freed', 29521125690368), ('allocated_bytes.large_pool.peak', 4077789184), ('allocated_bytes.small_pool.allocated', 2868578795520), ('allocated_bytes.small_pool.current', 4775936), ('allocated_bytes.small_pool.freed', 2868574019584), ('allocated_bytes.small_pool.peak', 79264256), ('allocation.all.allocated', 25802252), ('allocation.all.current', 78), ('allocation.all.freed', 25802174), ('allocation.all.peak', 456), ('allocation.large_pool.allocated', 2419962), ('allocation.large_pool.current', 64), ('allocation.large_pool.freed', 2419898), ('allocation.large_pool.peak', 188), ('allocation.small_pool.allocated', 23382290), ('allocation.small_pool.current', 14), ('allocation.small_pool.freed', 23382276), ('allocation.small_pool.peak', 341), ('inactive_split.all.allocated', 12906812), ('inactive_split.all.current', 79), ('inactive_split.all.freed', 12906733), ('inactive_split.all.peak', 212), ('inactive_split.large_pool.allocated', 1476639), ('inactive_split.large_pool.current', 59), ('inactive_split.large_pool.freed', 1476580), ('inactive_split.large_pool.peak', 187), ('inactive_split.small_pool.allocated', 11430173), ('inactive_split.small_pool.current', 20), ('inactive_split.small_pool.freed', 11430153), ('inactive_split.small_pool.peak', 66), ('inactive_split_bytes.all.allocated', 14043722753024), ('inactive_split_bytes.all.current', 332136448), ('inactive_split_bytes.all.freed', 14043390616576), ('inactive_split_bytes.all.peak', 900622848), ('inactive_split_bytes.large_pool.allocated', 11117891939328), ('inactive_split_bytes.large_pool.current', 322232320), ('inactive_split_bytes.large_pool.freed', 11117569707008), ('inactive_split_bytes.large_pool.peak', 892723200), ('inactive_split_bytes.small_pool.allocated', 2925830813696), ('inactive_split_bytes.small_pool.current', 9904128), ('inactive_split_bytes.small_pool.freed', 2925820909568), ('inactive_split_bytes.small_pool.peak', 29399552), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 31986341438876), ('requested_bytes.all.current', 1012137984), ('requested_bytes.all.freed', 31985329300892), ('requested_bytes.all.peak', 4065634728), ('requested_bytes.large_pool.allocated', 29120937318016), ('requested_bytes.large_pool.current', 1007362048), ('requested_bytes.large_pool.freed', 29119929955968), ('requested_bytes.large_pool.peak', 4053508096), ('requested_bytes.small_pool.allocated', 2865404120860), ('requested_bytes.small_pool.current', 4775936), ('requested_bytes.small_pool.freed', 2865399344924), ('requested_bytes.small_pool.peak', 79223246), ('reserved_bytes.all.allocated', 1867562090496), ('reserved_bytes.all.current', 1344274432), ('reserved_bytes.all.freed', 1866217816064), ('reserved_bytes.all.peak', 9841934336), ('reserved_bytes.large_pool.allocated', 1848452841472), ('reserved_bytes.large_pool.current', 1329594368), ('reserved_bytes.large_pool.freed', 1847123247104), ('reserved_bytes.large_pool.peak', 9751756800), ('reserved_bytes.small_pool.allocated', 19109249024), ('reserved_bytes.small_pool.current', 14680064), ('reserved_bytes.small_pool.freed', 19094568960), ('reserved_bytes.small_pool.peak', 96468992), ('segment.all.allocated', 101598), ('segment.all.current', 64), ('segment.all.freed', 101534), ('segment.all.peak', 545), ('segment.large_pool.allocated', 92486), ('segment.large_pool.current', 57), ('segment.large_pool.freed', 92429), ('segment.large_pool.peak', 502), ('segment.small_pool.allocated', 9112), ('segment.small_pool.current', 7), ('segment.small_pool.freed', 9105), ('segment.small_pool.peak', 46)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 385, in call_batched
    self.PATH_TO_LOG_LATENCY = FORWARD_TIMES_FILENAMES[self.model_name]
KeyError: 'facebook/nllb-moe-54b'
OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 0), ('requested_bytes.all.current', 0), ('requested_bytes.all.freed', 0), ('requested_bytes.all.peak', 0), ('requested_bytes.large_pool.allocated', 0), ('requested_bytes.large_pool.current', 0), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 0), ('requested_bytes.small_pool.allocated', 0), ('requested_bytes.small_pool.current', 0), ('requested_bytes.small_pool.freed', 0), ('requested_bytes.small_pool.peak', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

OrderedDict([('active.all.allocated', 9), ('active.all.current', 4), ('active.all.freed', 5), ('active.all.peak', 6), ('active.large_pool.allocated', 1), ('active.large_pool.current', 1), ('active.large_pool.freed', 0), ('active.large_pool.peak', 1), ('active.small_pool.allocated', 8), ('active.small_pool.current', 3), ('active.small_pool.freed', 5), ('active.small_pool.peak', 5), ('active_bytes.all.allocated', 98829312), ('active_bytes.all.current', 98698752), ('active_bytes.all.freed', 130560), ('active_bytes.all.peak', 98827776), ('active_bytes.large_pool.allocated', 98697216), ('active_bytes.large_pool.current', 98697216), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 98697216), ('active_bytes.small_pool.allocated', 132096), ('active_bytes.small_pool.current', 1536), ('active_bytes.small_pool.freed', 130560), ('active_bytes.small_pool.peak', 130560), ('allocated_bytes.all.allocated', 98829312), ('allocated_bytes.all.current', 98698752), ('allocated_bytes.all.freed', 130560), ('allocated_bytes.all.peak', 98827776), ('allocated_bytes.large_pool.allocated', 98697216), ('allocated_bytes.large_pool.current', 98697216), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 98697216), ('allocated_bytes.small_pool.allocated', 132096), ('allocated_bytes.small_pool.current', 1536), ('allocated_bytes.small_pool.freed', 130560), ('allocated_bytes.small_pool.peak', 130560), ('allocation.all.allocated', 9), ('allocation.all.current', 4), ('allocation.all.freed', 5), ('allocation.all.peak', 6), ('allocation.large_pool.allocated', 1), ('allocation.large_pool.current', 1), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 1), ('allocation.small_pool.allocated', 8), ('allocation.small_pool.current', 3), ('allocation.small_pool.freed', 5), ('allocation.small_pool.peak', 5), ('inactive_split.all.allocated', 4), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 2), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 3), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 4193280), ('inactive_split_bytes.all.current', 4061696), ('inactive_split_bytes.all.freed', 131584), ('inactive_split_bytes.all.peak', 4061696), ('inactive_split_bytes.large_pool.allocated', 1966080), ('inactive_split_bytes.large_pool.current', 1966080), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 1966080), ('inactive_split_bytes.small_pool.allocated', 2227200), ('inactive_split_bytes.small_pool.current', 2095616), ('inactive_split_bytes.small_pool.freed', 131584), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 98826787), ('requested_bytes.all.current', 98697720), ('requested_bytes.all.freed', 129067), ('requested_bytes.all.peak', 98826744), ('requested_bytes.large_pool.allocated', 98697216), ('requested_bytes.large_pool.current', 98697216), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 98697216), ('requested_bytes.small_pool.allocated', 129571), ('requested_bytes.small_pool.current', 504), ('requested_bytes.small_pool.freed', 129067), ('requested_bytes.small_pool.peak', 129528), ('reserved_bytes.all.allocated', 102760448), ('reserved_bytes.all.current', 102760448), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 102760448), ('reserved_bytes.large_pool.allocated', 100663296), ('reserved_bytes.large_pool.current', 100663296), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 100663296), ('reserved_bytes.small_pool.allocated', 2097152), ('reserved_bytes.small_pool.current', 2097152), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 2097152), ('segment.all.allocated', 2), ('segment.all.current', 2), ('segment.all.freed', 0), ('segment.all.peak', 2), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 1), ('segment.small_pool.current', 1), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 1)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

OrderedDict([('active.all.allocated', 9), ('active.all.current', 4), ('active.all.freed', 5), ('active.all.peak', 6), ('active.large_pool.allocated', 1), ('active.large_pool.current', 1), ('active.large_pool.freed', 0), ('active.large_pool.peak', 1), ('active.small_pool.allocated', 8), ('active.small_pool.current', 3), ('active.small_pool.freed', 5), ('active.small_pool.peak', 5), ('active_bytes.all.allocated', 98829312), ('active_bytes.all.current', 98698752), ('active_bytes.all.freed', 130560), ('active_bytes.all.peak', 98827776), ('active_bytes.large_pool.allocated', 98697216), ('active_bytes.large_pool.current', 98697216), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 98697216), ('active_bytes.small_pool.allocated', 132096), ('active_bytes.small_pool.current', 1536), ('active_bytes.small_pool.freed', 130560), ('active_bytes.small_pool.peak', 130560), ('allocated_bytes.all.allocated', 98829312), ('allocated_bytes.all.current', 98698752), ('allocated_bytes.all.freed', 130560), ('allocated_bytes.all.peak', 98827776), ('allocated_bytes.large_pool.allocated', 98697216), ('allocated_bytes.large_pool.current', 98697216), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 98697216), ('allocated_bytes.small_pool.allocated', 132096), ('allocated_bytes.small_pool.current', 1536), ('allocated_bytes.small_pool.freed', 130560), ('allocated_bytes.small_pool.peak', 130560), ('allocation.all.allocated', 9), ('allocation.all.current', 4), ('allocation.all.freed', 5), ('allocation.all.peak', 6), ('allocation.large_pool.allocated', 1), ('allocation.large_pool.current', 1), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 1), ('allocation.small_pool.allocated', 8), ('allocation.small_pool.current', 3), ('allocation.small_pool.freed', 5), ('allocation.small_pool.peak', 5), ('inactive_split.all.allocated', 4), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 2), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 3), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 4193280), ('inactive_split_bytes.all.current', 4061696), ('inactive_split_bytes.all.freed', 131584), ('inactive_split_bytes.all.peak', 4061696), ('inactive_split_bytes.large_pool.allocated', 1966080), ('inactive_split_bytes.large_pool.current', 1966080), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 1966080), ('inactive_split_bytes.small_pool.allocated', 2227200), ('inactive_split_bytes.small_pool.current', 2095616), ('inactive_split_bytes.small_pool.freed', 131584), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 98826787), ('requested_bytes.all.current', 98697720), ('requested_bytes.all.freed', 129067), ('requested_bytes.all.peak', 98826744), ('requested_bytes.large_pool.allocated', 98697216), ('requested_bytes.large_pool.current', 98697216), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 98697216), ('requested_bytes.small_pool.allocated', 129571), ('requested_bytes.small_pool.current', 504), ('requested_bytes.small_pool.freed', 129067), ('requested_bytes.small_pool.peak', 129528), ('reserved_bytes.all.allocated', 102760448), ('reserved_bytes.all.current', 102760448), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 102760448), ('reserved_bytes.large_pool.allocated', 100663296), ('reserved_bytes.large_pool.current', 100663296), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 100663296), ('reserved_bytes.small_pool.allocated', 2097152), ('reserved_bytes.small_pool.current', 2097152), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 2097152), ('segment.all.allocated', 2), ('segment.all.current', 2), ('segment.all.freed', 0), ('segment.all.peak', 2), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 1), ('segment.small_pool.current', 1), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 1)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
OrderedDict([('active.all.allocated', 3628), ('active.all.current', 281), ('active.all.freed', 3347), ('active.all.peak', 298), ('active.large_pool.allocated', 187), ('active.large_pool.current', 175), ('active.large_pool.freed', 12), ('active.large_pool.peak', 176), ('active.small_pool.allocated', 3441), ('active.small_pool.current', 106), ('active.small_pool.freed', 3335), ('active.small_pool.peak', 124), ('active_bytes.all.allocated', 19774489600), ('active_bytes.all.current', 18952400384), ('active_bytes.all.freed', 822089216), ('active_bytes.all.peak', 18957995520), ('active_bytes.large_pool.allocated', 19080617984), ('active_bytes.large_pool.current', 18917040128), ('active_bytes.large_pool.freed', 163577856), ('active_bytes.large_pool.peak', 18921234432), ('active_bytes.small_pool.allocated', 693871616), ('active_bytes.small_pool.current', 35360256), ('active_bytes.small_pool.freed', 658511360), ('active_bytes.small_pool.peak', 36761088), ('allocated_bytes.all.allocated', 19774489600), ('allocated_bytes.all.current', 18952400384), ('allocated_bytes.all.freed', 822089216), ('allocated_bytes.all.peak', 18957995520), ('allocated_bytes.large_pool.allocated', 19080617984), ('allocated_bytes.large_pool.current', 18917040128), ('allocated_bytes.large_pool.freed', 163577856), ('allocated_bytes.large_pool.peak', 18921234432), ('allocated_bytes.small_pool.allocated', 693871616), ('allocated_bytes.small_pool.current', 35360256), ('allocated_bytes.small_pool.freed', 658511360), ('allocated_bytes.small_pool.peak', 36761088), ('allocation.all.allocated', 3628), ('allocation.all.current', 281), ('allocation.all.freed', 3347), ('allocation.all.peak', 298), ('allocation.large_pool.allocated', 187), ('allocation.large_pool.current', 175), ('allocation.large_pool.freed', 12), ('allocation.large_pool.peak', 176), ('allocation.small_pool.allocated', 3441), ('allocation.small_pool.current', 106), ('allocation.small_pool.freed', 3335), ('allocation.small_pool.peak', 124), ('inactive_split.all.allocated', 1825), ('inactive_split.all.current', 172), ('inactive_split.all.freed', 1653), ('inactive_split.all.peak', 172), ('inactive_split.large_pool.allocated', 162), ('inactive_split.large_pool.current', 162), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 162), ('inactive_split.small_pool.allocated', 1663), ('inactive_split.small_pool.current', 10), ('inactive_split.small_pool.freed', 1653), ('inactive_split.small_pool.peak', 10), ('inactive_split_bytes.all.allocated', 1071412224), ('inactive_split_bytes.all.current', 345592320), ('inactive_split_bytes.all.freed', 725819904), ('inactive_split_bytes.all.peak', 345592320), ('inactive_split_bytes.large_pool.allocated', 368369664), ('inactive_split_bytes.large_pool.current', 343203840), ('inactive_split_bytes.large_pool.freed', 25165824), ('inactive_split_bytes.large_pool.peak', 343203840), ('inactive_split_bytes.small_pool.allocated', 703042560), ('inactive_split_bytes.small_pool.current', 2388480), ('inactive_split_bytes.small_pool.freed', 700654080), ('inactive_split_bytes.small_pool.peak', 3190784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 19755637823), ('requested_bytes.all.current', 18940486689), ('requested_bytes.all.freed', 815151134), ('requested_bytes.all.peak', 18946080673), ('requested_bytes.large_pool.allocated', 19062415360), ('requested_bytes.large_pool.current', 18905128960), ('requested_bytes.large_pool.freed', 157286400), ('requested_bytes.large_pool.peak', 18909323264), ('requested_bytes.small_pool.allocated', 693222463), ('requested_bytes.small_pool.current', 35357729), ('requested_bytes.small_pool.freed', 657864734), ('requested_bytes.small_pool.peak', 36757409), ('reserved_bytes.all.allocated', 19321061376), ('reserved_bytes.all.current', 19321061376), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 19321061376), ('reserved_bytes.large_pool.allocated', 19283312640), ('reserved_bytes.large_pool.current', 19283312640), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 19283312640), ('reserved_bytes.small_pool.allocated', 37748736), ('reserved_bytes.small_pool.current', 37748736), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 194), ('segment.all.current', 194), ('segment.all.freed', 0), ('segment.all.peak', 194), ('segment.large_pool.allocated', 176), ('segment.large_pool.current', 176), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 176), ('segment.small_pool.allocated', 18), ('segment.small_pool.current', 18), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
OrderedDict([('active.all.allocated', 8159), ('active.all.current', 310), ('active.all.freed', 7849), ('active.all.peak', 329), ('active.large_pool.allocated', 993), ('active.large_pool.current', 253), ('active.large_pool.freed', 740), ('active.large_pool.peak', 256), ('active.small_pool.allocated', 7166), ('active.small_pool.current', 57), ('active.small_pool.freed', 7109), ('active.small_pool.peak', 124), ('active_bytes.all.allocated', 39757706240), ('active_bytes.all.current', 24051078144), ('active_bytes.all.freed', 15706628096), ('active_bytes.all.peak', 24125646848), ('active_bytes.large_pool.allocated', 38081585152), ('active_bytes.large_pool.current', 24025317376), ('active_bytes.large_pool.freed', 14056267776), ('active_bytes.large_pool.peak', 24096620544), ('active_bytes.small_pool.allocated', 1676121088), ('active_bytes.small_pool.current', 25760768), ('active_bytes.small_pool.freed', 1650360320), ('active_bytes.small_pool.peak', 36761088), ('allocated_bytes.all.allocated', 39757706240), ('allocated_bytes.all.current', 24051078144), ('allocated_bytes.all.freed', 15706628096), ('allocated_bytes.all.peak', 24125646848), ('allocated_bytes.large_pool.allocated', 38081585152), ('allocated_bytes.large_pool.current', 24025317376), ('allocated_bytes.large_pool.freed', 14056267776), ('allocated_bytes.large_pool.peak', 24096620544), ('allocated_bytes.small_pool.allocated', 1676121088), ('allocated_bytes.small_pool.current', 25760768), ('allocated_bytes.small_pool.freed', 1650360320), ('allocated_bytes.small_pool.peak', 36761088), ('allocation.all.allocated', 8159), ('allocation.all.current', 310), ('allocation.all.freed', 7849), ('allocation.all.peak', 329), ('allocation.large_pool.allocated', 993), ('allocation.large_pool.current', 253), ('allocation.large_pool.freed', 740), ('allocation.large_pool.peak', 256), ('allocation.small_pool.allocated', 7166), ('allocation.small_pool.current', 57), ('allocation.small_pool.freed', 7109), ('allocation.small_pool.peak', 124), ('inactive_split.all.allocated', 3997), ('inactive_split.all.current', 203), ('inactive_split.all.freed', 3794), ('inactive_split.all.peak', 205), ('inactive_split.large_pool.allocated', 673), ('inactive_split.large_pool.current', 194), ('inactive_split.large_pool.freed', 479), ('inactive_split.large_pool.peak', 195), ('inactive_split.small_pool.allocated', 3324), ('inactive_split.small_pool.current', 9), ('inactive_split.small_pool.freed', 3315), ('inactive_split.small_pool.peak', 12), ('inactive_split_bytes.all.allocated', 7675862528), ('inactive_split_bytes.all.current', 466725888), ('inactive_split_bytes.all.freed', 7209136640), ('inactive_split_bytes.all.peak', 1168735232), ('inactive_split_bytes.large_pool.allocated', 5890990080), ('inactive_split_bytes.large_pool.current', 463126528), ('inactive_split_bytes.large_pool.freed', 5427863552), ('inactive_split_bytes.large_pool.peak', 1166721024), ('inactive_split_bytes.small_pool.allocated', 1784872448), ('inactive_split_bytes.small_pool.current', 3599360), ('inactive_split_bytes.small_pool.freed', 1781273088), ('inactive_split_bytes.small_pool.peak', 5115392), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 39652882212), ('requested_bytes.all.current', 24031825936), ('requested_bytes.all.freed', 15621056276), ('requested_bytes.all.peak', 24106394128), ('requested_bytes.large_pool.allocated', 37978087424), ('requested_bytes.large_pool.current', 24006066176), ('requested_bytes.large_pool.freed', 13972021248), ('requested_bytes.large_pool.peak', 24077369344), ('requested_bytes.small_pool.allocated', 1674794788), ('requested_bytes.small_pool.current', 25759760), ('requested_bytes.small_pool.freed', 1649035028), ('requested_bytes.small_pool.peak', 36757409), ('reserved_bytes.all.allocated', 24708644864), ('reserved_bytes.all.current', 24649924608), ('reserved_bytes.all.freed', 58720256), ('reserved_bytes.all.peak', 24649924608), ('reserved_bytes.large_pool.allocated', 24641536000), ('reserved_bytes.large_pool.current', 24618467328), ('reserved_bytes.large_pool.freed', 23068672), ('reserved_bytes.large_pool.peak', 24618467328), ('reserved_bytes.small_pool.allocated', 67108864), ('reserved_bytes.small_pool.current', 31457280), ('reserved_bytes.small_pool.freed', 35651584), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 258), ('segment.all.current', 240), ('segment.all.freed', 18), ('segment.all.peak', 240), ('segment.large_pool.allocated', 226), ('segment.large_pool.current', 225), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 225), ('segment.small_pool.allocated', 32), ('segment.small_pool.current', 15), ('segment.small_pool.freed', 17), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
OrderedDict([('active.all.allocated', 12199), ('active.all.current', 310), ('active.all.freed', 11889), ('active.all.peak', 329), ('active.large_pool.allocated', 1216), ('active.large_pool.current', 253), ('active.large_pool.freed', 963), ('active.large_pool.peak', 256), ('active.small_pool.allocated', 10983), ('active.small_pool.current', 57), ('active.small_pool.freed', 10926), ('active.small_pool.peak', 124), ('active_bytes.all.allocated', 49375627776), ('active_bytes.all.current', 22914945536), ('active_bytes.all.freed', 26460682240), ('active_bytes.all.peak', 24125646848), ('active_bytes.large_pool.allocated', 46879531008), ('active_bytes.large_pool.current', 22913302528), ('active_bytes.large_pool.freed', 23966228480), ('active_bytes.large_pool.peak', 24096620544), ('active_bytes.small_pool.allocated', 2496096768), ('active_bytes.small_pool.current', 1643008), ('active_bytes.small_pool.freed', 2494453760), ('active_bytes.small_pool.peak', 36761088), ('allocated_bytes.all.allocated', 49375627776), ('allocated_bytes.all.current', 22914945536), ('allocated_bytes.all.freed', 26460682240), ('allocated_bytes.all.peak', 24125646848), ('allocated_bytes.large_pool.allocated', 46879531008), ('allocated_bytes.large_pool.current', 22913302528), ('allocated_bytes.large_pool.freed', 23966228480), ('allocated_bytes.large_pool.peak', 24096620544), ('allocated_bytes.small_pool.allocated', 2496096768), ('allocated_bytes.small_pool.current', 1643008), ('allocated_bytes.small_pool.freed', 2494453760), ('allocated_bytes.small_pool.peak', 36761088), ('allocation.all.allocated', 12199), ('allocation.all.current', 310), ('allocation.all.freed', 11889), ('allocation.all.peak', 329), ('allocation.large_pool.allocated', 1216), ('allocation.large_pool.current', 253), ('allocation.large_pool.freed', 963), ('allocation.large_pool.peak', 256), ('allocation.small_pool.allocated', 10983), ('allocation.small_pool.current', 57), ('allocation.small_pool.freed', 10926), ('allocation.small_pool.peak', 124), ('inactive_split.all.allocated', 5927), ('inactive_split.all.current', 199), ('inactive_split.all.freed', 5728), ('inactive_split.all.peak', 205), ('inactive_split.large_pool.allocated', 820), ('inactive_split.large_pool.current', 193), ('inactive_split.large_pool.freed', 627), ('inactive_split.large_pool.peak', 195), ('inactive_split.small_pool.allocated', 5107), ('inactive_split.small_pool.current', 6), ('inactive_split.small_pool.freed', 5101), ('inactive_split.small_pool.peak', 12), ('inactive_split_bytes.all.allocated', 9808721408), ('inactive_split_bytes.all.current', 409579008), ('inactive_split_bytes.all.freed', 9399142400), ('inactive_split_bytes.all.peak', 1169796096), ('inactive_split_bytes.large_pool.allocated', 7142662144), ('inactive_split_bytes.large_pool.current', 409124864), ('inactive_split_bytes.large_pool.freed', 6733537280), ('inactive_split_bytes.large_pool.peak', 1166721024), ('inactive_split_bytes.small_pool.allocated', 2666059264), ('inactive_split_bytes.small_pool.current', 454144), ('inactive_split_bytes.small_pool.freed', 2665605120), ('inactive_split_bytes.small_pool.peak', 5115392), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 49262013564), ('requested_bytes.all.current', 22903032881), ('requested_bytes.all.freed', 26358980683), ('requested_bytes.all.peak', 24106394128), ('requested_bytes.large_pool.allocated', 46767939584), ('requested_bytes.large_pool.current', 22901391360), ('requested_bytes.large_pool.freed', 23866548224), ('requested_bytes.large_pool.peak', 24077369344), ('requested_bytes.small_pool.allocated', 2494073980), ('requested_bytes.small_pool.current', 1641521), ('requested_bytes.small_pool.freed', 2492432459), ('requested_bytes.small_pool.peak', 36757409), ('reserved_bytes.all.allocated', 24823988224), ('reserved_bytes.all.current', 23330816000), ('reserved_bytes.all.freed', 1493172224), ('reserved_bytes.all.peak', 24649924608), ('reserved_bytes.large_pool.allocated', 24750587904), ('reserved_bytes.large_pool.current', 23322427392), ('reserved_bytes.large_pool.freed', 1428160512), ('reserved_bytes.large_pool.peak', 24618467328), ('reserved_bytes.small_pool.allocated', 73400320), ('reserved_bytes.small_pool.current', 8388608), ('reserved_bytes.small_pool.freed', 65011712), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 264), ('segment.all.current', 211), ('segment.all.freed', 53), ('segment.all.peak', 240), ('segment.large_pool.allocated', 229), ('segment.large_pool.current', 207), ('segment.large_pool.freed', 22), ('segment.large_pool.peak', 225), ('segment.small_pool.allocated', 35), ('segment.small_pool.current', 4), ('segment.small_pool.freed', 31), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 908, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 433, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)
OrderedDict([('active.all.allocated', 17071), ('active.all.current', 310), ('active.all.freed', 16761), ('active.all.peak', 329), ('active.large_pool.allocated', 2067), ('active.large_pool.current', 253), ('active.large_pool.freed', 1814), ('active.large_pool.peak', 256), ('active.small_pool.allocated', 15004), ('active.small_pool.current', 57), ('active.small_pool.freed', 14947), ('active.small_pool.peak', 124), ('active_bytes.all.allocated', 75798797824), ('active_bytes.all.current', 24480482816), ('active_bytes.all.freed', 51318315008), ('active_bytes.all.peak', 24554760704), ('active_bytes.large_pool.allocated', 72259354624), ('active_bytes.large_pool.current', 24454709248), ('active_bytes.large_pool.freed', 47804645376), ('active_bytes.large_pool.peak', 24526012416), ('active_bytes.small_pool.allocated', 3539443200), ('active_bytes.small_pool.current', 25773568), ('active_bytes.small_pool.freed', 3513669632), ('active_bytes.small_pool.peak', 36761088), ('allocated_bytes.all.allocated', 75798797824), ('allocated_bytes.all.current', 24480482816), ('allocated_bytes.all.freed', 51318315008), ('allocated_bytes.all.peak', 24554760704), ('allocated_bytes.large_pool.allocated', 72259354624), ('allocated_bytes.large_pool.current', 24454709248), ('allocated_bytes.large_pool.freed', 47804645376), ('allocated_bytes.large_pool.peak', 24526012416), ('allocated_bytes.small_pool.allocated', 3539443200), ('allocated_bytes.small_pool.current', 25773568), ('allocated_bytes.small_pool.freed', 3513669632), ('allocated_bytes.small_pool.peak', 36761088), ('allocation.all.allocated', 17071), ('allocation.all.current', 310), ('allocation.all.freed', 16761), ('allocation.all.peak', 329), ('allocation.large_pool.allocated', 2067), ('allocation.large_pool.current', 253), ('allocation.large_pool.freed', 1814), ('allocation.large_pool.peak', 256), ('allocation.small_pool.allocated', 15004), ('allocation.small_pool.current', 57), ('allocation.small_pool.freed', 14947), ('allocation.small_pool.peak', 124), ('inactive_split.all.allocated', 8318), ('inactive_split.all.current', 201), ('inactive_split.all.freed', 8117), ('inactive_split.all.peak', 205), ('inactive_split.large_pool.allocated', 1270), ('inactive_split.large_pool.current', 192), ('inactive_split.large_pool.freed', 1078), ('inactive_split.large_pool.peak', 196), ('inactive_split.small_pool.allocated', 7048), ('inactive_split.small_pool.current', 9), ('inactive_split.small_pool.freed', 7039), ('inactive_split.small_pool.peak', 12), ('inactive_split_bytes.all.allocated', 17510569472), ('inactive_split_bytes.all.current', 439974400), ('inactive_split_bytes.all.freed', 17070595072), ('inactive_split_bytes.all.peak', 1445018112), ('inactive_split_bytes.large_pool.allocated', 13739671552), ('inactive_split_bytes.large_pool.current', 436387840), ('inactive_split_bytes.large_pool.freed', 13303283712), ('inactive_split_bytes.large_pool.peak', 1443020800), ('inactive_split_bytes.small_pool.allocated', 3770897920), ('inactive_split_bytes.small_pool.current', 3586560), ('inactive_split_bytes.small_pool.freed', 3767311360), ('inactive_split_bytes.small_pool.peak', 5115392), ('max_split_size', -1), ('num_alloc_retries', 1), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 75654118125), ('requested_bytes.all.current', 24468570896), ('requested_bytes.all.freed', 51185547229), ('requested_bytes.all.peak', 24542619152), ('requested_bytes.large_pool.allocated', 72117362688), ('requested_bytes.large_pool.current', 24442798080), ('requested_bytes.large_pool.freed', 47674564608), ('requested_bytes.large_pool.peak', 24514101248), ('requested_bytes.small_pool.allocated', 3536755437), ('requested_bytes.small_pool.current', 25772816), ('requested_bytes.small_pool.freed', 3510982621), ('requested_bytes.small_pool.peak', 36757409), ('reserved_bytes.all.allocated', 26684162048), ('reserved_bytes.all.current', 24989663232), ('reserved_bytes.all.freed', 1694498816), ('reserved_bytes.all.peak', 25006440448), ('reserved_bytes.large_pool.allocated', 26579304448), ('reserved_bytes.large_pool.current', 24958205952), ('reserved_bytes.large_pool.freed', 1621098496), ('reserved_bytes.large_pool.peak', 24974983168), ('reserved_bytes.small_pool.allocated', 104857600), ('reserved_bytes.small_pool.current', 31457280), ('reserved_bytes.small_pool.freed', 73400320), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 301), ('segment.all.current', 237), ('segment.all.freed', 64), ('segment.all.peak', 240), ('segment.large_pool.allocated', 251), ('segment.large_pool.current', 222), ('segment.large_pool.freed', 29), ('segment.large_pool.peak', 225), ('segment.small_pool.allocated', 50), ('segment.small_pool.current', 15), ('segment.small_pool.freed', 35), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7126), ('active.all.freed', 4), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7124), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7130), ('allocation.all.current', 7126), ('allocation.all.freed', 4), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839019), ('requested_bytes.all.current', 16838972), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16838976), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29035), ('requested_bytes.small_pool.current', 28988), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 28992), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7137), ('active.all.current', 7126), ('active.all.freed', 11), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7135), ('active.small_pool.current', 7124), ('active.small_pool.freed', 11), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20481536), ('active_bytes.all.current', 20474368), ('active_bytes.all.freed', 7168), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3671552), ('active_bytes.small_pool.current', 3664384), ('active_bytes.small_pool.freed', 7168), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20481536), ('allocated_bytes.all.current', 20474368), ('allocated_bytes.all.freed', 7168), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3671552), ('allocated_bytes.small_pool.current', 3664384), ('allocated_bytes.small_pool.freed', 7168), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7137), ('allocation.all.current', 7126), ('allocation.all.freed', 11), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7135), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 11), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 11), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 7), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 10), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 7), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16766976), ('inactive_split_bytes.all.current', 4691456), ('inactive_split_bytes.all.freed', 12075520), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4200448), ('inactive_split_bytes.small_pool.current', 529920), ('inactive_split_bytes.small_pool.freed', 3670528), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16859324), ('requested_bytes.all.current', 16856516), ('requested_bytes.all.freed', 2808), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 49340), ('requested_bytes.small_pool.current', 46532), ('requested_bytes.small_pool.freed', 2808), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7126), ('active.all.freed', 17), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7124), ('active.small_pool.freed', 17), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20484608), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 27136), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3674624), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 27136), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20484608), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 27136), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3674624), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 27136), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7143), ('allocation.all.current', 7126), ('allocation.all.freed', 17), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 17), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 13), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 11), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 12), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 11), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16786944), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12078592), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4220416), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3673600), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16860989), ('requested_bytes.all.current', 16840004), ('requested_bytes.all.freed', 20985), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 51005), ('requested_bytes.small_pool.current', 30020), ('requested_bytes.small_pool.freed', 20985), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7150), ('active.all.current', 7126), ('active.all.freed', 24), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7148), ('active.small_pool.current', 7124), ('active.small_pool.freed', 24), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20512768), ('active_bytes.all.current', 20480512), ('active_bytes.all.freed', 32256), ('active_bytes.all.peak', 20481536), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3702784), ('active_bytes.small_pool.current', 3670528), ('active_bytes.small_pool.freed', 32256), ('active_bytes.small_pool.peak', 3671552), ('allocated_bytes.all.allocated', 20512768), ('allocated_bytes.all.current', 20480512), ('allocated_bytes.all.freed', 32256), ('allocated_bytes.all.peak', 20481536), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3702784), ('allocated_bytes.small_pool.current', 3670528), ('allocated_bytes.small_pool.freed', 32256), ('allocated_bytes.small_pool.peak', 3671552), ('allocation.all.allocated', 7150), ('allocation.all.current', 7126), ('allocation.all.freed', 24), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7148), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 24), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 19), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 15), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 18), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 15), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16792064), ('inactive_split_bytes.all.current', 4685312), ('inactive_split_bytes.all.freed', 12106752), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4225536), ('inactive_split_bytes.small_pool.current', 523776), ('inactive_split_bytes.small_pool.freed', 3701760), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16888638), ('requested_bytes.all.current', 16863044), ('requested_bytes.all.freed', 25594), ('requested_bytes.all.peak', 16864068), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 78654), ('requested_bytes.small_pool.current', 53060), ('requested_bytes.small_pool.freed', 25594), ('requested_bytes.small_pool.peak', 54084), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7126), ('active.all.freed', 4), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7124), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7130), ('allocation.all.current', 7126), ('allocation.all.freed', 4), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839019), ('requested_bytes.all.current', 16838972), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16838976), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29035), ('requested_bytes.small_pool.current', 28988), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 28992), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7137), ('active.all.current', 7126), ('active.all.freed', 11), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7135), ('active.small_pool.current', 7124), ('active.small_pool.freed', 11), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20481536), ('active_bytes.all.current', 20474368), ('active_bytes.all.freed', 7168), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3671552), ('active_bytes.small_pool.current', 3664384), ('active_bytes.small_pool.freed', 7168), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20481536), ('allocated_bytes.all.current', 20474368), ('allocated_bytes.all.freed', 7168), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3671552), ('allocated_bytes.small_pool.current', 3664384), ('allocated_bytes.small_pool.freed', 7168), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7137), ('allocation.all.current', 7126), ('allocation.all.freed', 11), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7135), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 11), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 11), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 7), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 10), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 7), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16766976), ('inactive_split_bytes.all.current', 4691456), ('inactive_split_bytes.all.freed', 12075520), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4200448), ('inactive_split_bytes.small_pool.current', 529920), ('inactive_split_bytes.small_pool.freed', 3670528), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16859324), ('requested_bytes.all.current', 16856516), ('requested_bytes.all.freed', 2808), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 49340), ('requested_bytes.small_pool.current', 46532), ('requested_bytes.small_pool.freed', 2808), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7126), ('active.all.freed', 17), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7124), ('active.small_pool.freed', 17), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20484608), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 27136), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3674624), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 27136), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20484608), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 27136), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3674624), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 27136), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7143), ('allocation.all.current', 7126), ('allocation.all.freed', 17), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 17), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 13), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 11), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 12), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 11), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16786944), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12078592), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4220416), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3673600), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16860989), ('requested_bytes.all.current', 16840004), ('requested_bytes.all.freed', 20985), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 51005), ('requested_bytes.small_pool.current', 30020), ('requested_bytes.small_pool.freed', 20985), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7150), ('active.all.current', 7126), ('active.all.freed', 24), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7148), ('active.small_pool.current', 7124), ('active.small_pool.freed', 24), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20512768), ('active_bytes.all.current', 20480512), ('active_bytes.all.freed', 32256), ('active_bytes.all.peak', 20481536), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3702784), ('active_bytes.small_pool.current', 3670528), ('active_bytes.small_pool.freed', 32256), ('active_bytes.small_pool.peak', 3671552), ('allocated_bytes.all.allocated', 20512768), ('allocated_bytes.all.current', 20480512), ('allocated_bytes.all.freed', 32256), ('allocated_bytes.all.peak', 20481536), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3702784), ('allocated_bytes.small_pool.current', 3670528), ('allocated_bytes.small_pool.freed', 32256), ('allocated_bytes.small_pool.peak', 3671552), ('allocation.all.allocated', 7150), ('allocation.all.current', 7126), ('allocation.all.freed', 24), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7148), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 24), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 19), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 15), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 18), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 15), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16792064), ('inactive_split_bytes.all.current', 4685312), ('inactive_split_bytes.all.freed', 12106752), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4225536), ('inactive_split_bytes.small_pool.current', 523776), ('inactive_split_bytes.small_pool.freed', 3701760), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16888638), ('requested_bytes.all.current', 16863044), ('requested_bytes.all.freed', 25594), ('requested_bytes.all.peak', 16864068), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 78654), ('requested_bytes.small_pool.current', 53060), ('requested_bytes.small_pool.freed', 25594), ('requested_bytes.small_pool.peak', 54084), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7126), ('active.all.freed', 4), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7124), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7130), ('allocation.all.current', 7126), ('allocation.all.freed', 4), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839019), ('requested_bytes.all.current', 16838972), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16838976), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29035), ('requested_bytes.small_pool.current', 28988), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 28992), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7137), ('active.all.current', 7126), ('active.all.freed', 11), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7135), ('active.small_pool.current', 7124), ('active.small_pool.freed', 11), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20481536), ('active_bytes.all.current', 20474368), ('active_bytes.all.freed', 7168), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3671552), ('active_bytes.small_pool.current', 3664384), ('active_bytes.small_pool.freed', 7168), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20481536), ('allocated_bytes.all.current', 20474368), ('allocated_bytes.all.freed', 7168), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3671552), ('allocated_bytes.small_pool.current', 3664384), ('allocated_bytes.small_pool.freed', 7168), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7137), ('allocation.all.current', 7126), ('allocation.all.freed', 11), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7135), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 11), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 11), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 7), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 10), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 7), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16766976), ('inactive_split_bytes.all.current', 4691456), ('inactive_split_bytes.all.freed', 12075520), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4200448), ('inactive_split_bytes.small_pool.current', 529920), ('inactive_split_bytes.small_pool.freed', 3670528), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16859324), ('requested_bytes.all.current', 16856516), ('requested_bytes.all.freed', 2808), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 49340), ('requested_bytes.small_pool.current', 46532), ('requested_bytes.small_pool.freed', 2808), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7126), ('active.all.freed', 17), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7124), ('active.small_pool.freed', 17), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20484608), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 27136), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3674624), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 27136), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20484608), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 27136), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3674624), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 27136), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7143), ('allocation.all.current', 7126), ('allocation.all.freed', 17), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 17), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 13), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 11), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 12), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 11), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16786944), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12078592), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4220416), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3673600), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16860989), ('requested_bytes.all.current', 16840004), ('requested_bytes.all.freed', 20985), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 51005), ('requested_bytes.small_pool.current', 30020), ('requested_bytes.small_pool.freed', 20985), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 912, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 437, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7150), ('active.all.current', 7126), ('active.all.freed', 24), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7148), ('active.small_pool.current', 7124), ('active.small_pool.freed', 24), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20512768), ('active_bytes.all.current', 20480512), ('active_bytes.all.freed', 32256), ('active_bytes.all.peak', 20481536), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3702784), ('active_bytes.small_pool.current', 3670528), ('active_bytes.small_pool.freed', 32256), ('active_bytes.small_pool.peak', 3671552), ('allocated_bytes.all.allocated', 20512768), ('allocated_bytes.all.current', 20480512), ('allocated_bytes.all.freed', 32256), ('allocated_bytes.all.peak', 20481536), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3702784), ('allocated_bytes.small_pool.current', 3670528), ('allocated_bytes.small_pool.freed', 32256), ('allocated_bytes.small_pool.peak', 3671552), ('allocation.all.allocated', 7150), ('allocation.all.current', 7126), ('allocation.all.freed', 24), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7148), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 24), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 19), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 15), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 18), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 15), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16792064), ('inactive_split_bytes.all.current', 4685312), ('inactive_split_bytes.all.freed', 12106752), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4225536), ('inactive_split_bytes.small_pool.current', 523776), ('inactive_split_bytes.small_pool.freed', 3701760), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16888638), ('requested_bytes.all.current', 16863044), ('requested_bytes.all.freed', 25594), ('requested_bytes.all.peak', 16864068), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 78654), ('requested_bytes.small_pool.current', 53060), ('requested_bytes.small_pool.freed', 25594), ('requested_bytes.small_pool.peak', 54084), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 913, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7126), ('active.all.freed', 4), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7124), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7130), ('allocation.all.current', 7126), ('allocation.all.freed', 4), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839019), ('requested_bytes.all.current', 16838972), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16838976), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29035), ('requested_bytes.small_pool.current', 28988), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 28992), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 913, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7137), ('active.all.current', 7126), ('active.all.freed', 11), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7135), ('active.small_pool.current', 7124), ('active.small_pool.freed', 11), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20481536), ('active_bytes.all.current', 20474368), ('active_bytes.all.freed', 7168), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3671552), ('active_bytes.small_pool.current', 3664384), ('active_bytes.small_pool.freed', 7168), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20481536), ('allocated_bytes.all.current', 20474368), ('allocated_bytes.all.freed', 7168), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3671552), ('allocated_bytes.small_pool.current', 3664384), ('allocated_bytes.small_pool.freed', 7168), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7137), ('allocation.all.current', 7126), ('allocation.all.freed', 11), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7135), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 11), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 11), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 7), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 10), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 7), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16766976), ('inactive_split_bytes.all.current', 4691456), ('inactive_split_bytes.all.freed', 12075520), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4200448), ('inactive_split_bytes.small_pool.current', 529920), ('inactive_split_bytes.small_pool.freed', 3670528), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16859324), ('requested_bytes.all.current', 16856516), ('requested_bytes.all.freed', 2808), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 49340), ('requested_bytes.small_pool.current', 46532), ('requested_bytes.small_pool.freed', 2808), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 913, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7126), ('active.all.freed', 17), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7124), ('active.small_pool.freed', 17), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20484608), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 27136), ('active_bytes.all.peak', 20475392), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3674624), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 27136), ('active_bytes.small_pool.peak', 3665408), ('allocated_bytes.all.allocated', 20484608), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 27136), ('allocated_bytes.all.peak', 20475392), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3674624), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 27136), ('allocated_bytes.small_pool.peak', 3665408), ('allocation.all.allocated', 7143), ('allocation.all.current', 7126), ('allocation.all.freed', 17), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 17), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 13), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 11), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 12), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 11), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16786944), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12078592), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4220416), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3673600), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16860989), ('requested_bytes.all.current', 16840004), ('requested_bytes.all.freed', 20985), ('requested_bytes.all.peak', 16857268), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 51005), ('requested_bytes.small_pool.current', 30020), ('requested_bytes.small_pool.freed', 20985), ('requested_bytes.small_pool.peak', 47284), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 913, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7150), ('active.all.current', 7126), ('active.all.freed', 24), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7148), ('active.small_pool.current', 7124), ('active.small_pool.freed', 24), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20512768), ('active_bytes.all.current', 20480512), ('active_bytes.all.freed', 32256), ('active_bytes.all.peak', 20481536), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3702784), ('active_bytes.small_pool.current', 3670528), ('active_bytes.small_pool.freed', 32256), ('active_bytes.small_pool.peak', 3671552), ('allocated_bytes.all.allocated', 20512768), ('allocated_bytes.all.current', 20480512), ('allocated_bytes.all.freed', 32256), ('allocated_bytes.all.peak', 20481536), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3702784), ('allocated_bytes.small_pool.current', 3670528), ('allocated_bytes.small_pool.freed', 32256), ('allocated_bytes.small_pool.peak', 3671552), ('allocation.all.allocated', 7150), ('allocation.all.current', 7126), ('allocation.all.freed', 24), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7148), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 24), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 19), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 15), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 18), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 15), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16792064), ('inactive_split_bytes.all.current', 4685312), ('inactive_split_bytes.all.freed', 12106752), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4225536), ('inactive_split_bytes.small_pool.current', 523776), ('inactive_split_bytes.small_pool.freed', 3701760), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16888638), ('requested_bytes.all.current', 16863044), ('requested_bytes.all.freed', 25594), ('requested_bytes.all.peak', 16864068), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 78654), ('requested_bytes.small_pool.current', 53060), ('requested_bytes.small_pool.freed', 25594), ('requested_bytes.small_pool.peak', 54084), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
UnboundLocalError: local variable 'inputs' referenced before assignment
OrderedDict([('active.all.allocated', 7127), ('active.all.current', 7126), ('active.all.freed', 1), ('active.all.peak', 7127), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7125), ('active.small_pool.current', 7124), ('active.small_pool.freed', 1), ('active.small_pool.peak', 7125), ('active_bytes.all.allocated', 20457984), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 512), ('active_bytes.all.peak', 20457984), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3648000), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 512), ('active_bytes.small_pool.peak', 3648000), ('allocated_bytes.all.allocated', 20457984), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 512), ('allocated_bytes.all.peak', 20457984), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3648000), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 512), ('allocated_bytes.small_pool.peak', 3648000), ('allocation.all.allocated', 7127), ('allocation.all.current', 7126), ('allocation.all.freed', 1), ('allocation.all.peak', 7127), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7125), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 1), ('allocation.small_pool.peak', 7125), ('inactive_split.all.allocated', 4), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 1), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 3), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 1), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760320), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12051968), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4193792), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3646976), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16838829), ('requested_bytes.all.current', 16838808), ('requested_bytes.all.freed', 21), ('requested_bytes.all.peak', 16838829), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 28845), ('requested_bytes.small_pool.current', 28824), ('requested_bytes.small_pool.freed', 21), ('requested_bytes.small_pool.peak', 28845), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
UnboundLocalError: local variable 'inputs' referenced before assignment
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7126), ('active.all.freed', 4), ('active.all.peak', 7127), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7124), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7125), ('active_bytes.all.allocated', 20471296), ('active_bytes.all.current', 20468736), ('active_bytes.all.freed', 2560), ('active_bytes.all.peak', 20469760), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3661312), ('active_bytes.small_pool.current', 3658752), ('active_bytes.small_pool.freed', 2560), ('active_bytes.small_pool.peak', 3659776), ('allocated_bytes.all.allocated', 20471296), ('allocated_bytes.all.current', 20468736), ('allocated_bytes.all.freed', 2560), ('allocated_bytes.all.peak', 20469760), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3661312), ('allocated_bytes.small_pool.current', 3658752), ('allocated_bytes.small_pool.freed', 2560), ('allocated_bytes.small_pool.peak', 3659776), ('allocation.all.allocated', 7130), ('allocation.all.current', 7126), ('allocation.all.freed', 4), ('allocation.all.peak', 7127), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7125), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16762368), ('inactive_split_bytes.all.current', 4697088), ('inactive_split_bytes.all.freed', 12065280), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195840), ('inactive_split_bytes.small_pool.current', 535552), ('inactive_split_bytes.small_pool.freed', 3660288), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16851613), ('requested_bytes.all.current', 16850504), ('requested_bytes.all.freed', 1109), ('requested_bytes.all.peak', 16851256), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 41629), ('requested_bytes.small_pool.current', 40520), ('requested_bytes.small_pool.freed', 1109), ('requested_bytes.small_pool.peak', 41272), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
UnboundLocalError: local variable 'inputs' referenced before assignment
OrderedDict([('active.all.allocated', 7133), ('active.all.current', 7126), ('active.all.freed', 7), ('active.all.peak', 7127), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7131), ('active.small_pool.current', 7124), ('active.small_pool.freed', 7), ('active.small_pool.peak', 7125), ('active_bytes.all.allocated', 20472832), ('active_bytes.all.current', 20457472), ('active_bytes.all.freed', 15360), ('active_bytes.all.peak', 20469760), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3662848), ('active_bytes.small_pool.current', 3647488), ('active_bytes.small_pool.freed', 15360), ('active_bytes.small_pool.peak', 3659776), ('allocated_bytes.all.allocated', 20472832), ('allocated_bytes.all.current', 20457472), ('allocated_bytes.all.freed', 15360), ('allocated_bytes.all.peak', 20469760), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3662848), ('allocated_bytes.small_pool.current', 3647488), ('allocated_bytes.small_pool.freed', 15360), ('allocated_bytes.small_pool.peak', 3659776), ('allocation.all.allocated', 7133), ('allocation.all.current', 7126), ('allocation.all.freed', 7), ('allocation.all.peak', 7127), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7131), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 7), ('allocation.small_pool.peak', 7125), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16775168), ('inactive_split_bytes.all.current', 4708352), ('inactive_split_bytes.all.freed', 12066816), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4208640), ('inactive_split_bytes.small_pool.current', 546816), ('inactive_split_bytes.small_pool.freed', 3661824), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16852701), ('requested_bytes.all.current', 16839496), ('requested_bytes.all.freed', 13205), ('requested_bytes.all.peak', 16851256), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 42717), ('requested_bytes.small_pool.current', 29512), ('requested_bytes.small_pool.freed', 13205), ('requested_bytes.small_pool.peak', 41272), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 438, in call_batched
    inputs = {k: v.to("cuda:0") for k, v in batch.items()}
UnboundLocalError: local variable 'inputs' referenced before assignment
OrderedDict([('active.all.allocated', 7136), ('active.all.current', 7126), ('active.all.freed', 10), ('active.all.peak', 7127), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7134), ('active.small_pool.current', 7124), ('active.small_pool.freed', 10), ('active.small_pool.peak', 7125), ('active_bytes.all.allocated', 20490240), ('active_bytes.all.current', 20472832), ('active_bytes.all.freed', 17408), ('active_bytes.all.peak', 20473856), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3680256), ('active_bytes.small_pool.current', 3662848), ('active_bytes.small_pool.freed', 17408), ('active_bytes.small_pool.peak', 3663872), ('allocated_bytes.all.allocated', 20490240), ('allocated_bytes.all.current', 20472832), ('allocated_bytes.all.freed', 17408), ('allocated_bytes.all.peak', 20473856), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3680256), ('allocated_bytes.small_pool.current', 3662848), ('allocated_bytes.small_pool.freed', 17408), ('allocated_bytes.small_pool.peak', 3663872), ('allocation.all.allocated', 7136), ('allocation.all.current', 7126), ('allocation.all.freed', 10), ('allocation.all.peak', 7127), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7134), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 10), ('allocation.small_pool.peak', 7125), ('inactive_split.all.allocated', 7), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 4), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 6), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 4), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16777216), ('inactive_split_bytes.all.current', 4692992), ('inactive_split_bytes.all.freed', 12084224), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4210688), ('inactive_split_bytes.small_pool.current', 531456), ('inactive_split_bytes.small_pool.freed', 3679232), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16870109), ('requested_bytes.all.current', 16854856), ('requested_bytes.all.freed', 15253), ('requested_bytes.all.peak', 16855880), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 60125), ('requested_bytes.small_pool.current', 44872), ('requested_bytes.small_pool.freed', 15253), ('requested_bytes.small_pool.peak', 45896), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 439, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 439, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7136), ('active.all.current', 7128), ('active.all.freed', 8), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7134), ('active.small_pool.current', 7126), ('active.small_pool.freed', 8), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20492288), ('active_bytes.all.current', 20487680), ('active_bytes.all.freed', 4608), ('active_bytes.all.peak', 20487680), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3682304), ('active_bytes.small_pool.current', 3677696), ('active_bytes.small_pool.freed', 4608), ('active_bytes.small_pool.peak', 3677696), ('allocated_bytes.all.allocated', 20492288), ('allocated_bytes.all.current', 20487680), ('allocated_bytes.all.freed', 4608), ('allocated_bytes.all.peak', 20487680), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3682304), ('allocated_bytes.small_pool.current', 3677696), ('allocated_bytes.small_pool.freed', 4608), ('allocated_bytes.small_pool.peak', 3677696), ('allocation.all.allocated', 7136), ('allocation.all.current', 7128), ('allocation.all.freed', 8), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7134), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 8), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 8), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 4), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 7), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 4), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16764416), ('inactive_split_bytes.all.current', 4678144), ('inactive_split_bytes.all.freed', 12086272), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4197888), ('inactive_split_bytes.small_pool.current', 516608), ('inactive_split_bytes.small_pool.freed', 3681280), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16871301), ('requested_bytes.all.current', 16869700), ('requested_bytes.all.freed', 1601), ('requested_bytes.all.peak', 16869700), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 61317), ('requested_bytes.small_pool.current', 59716), ('requested_bytes.small_pool.freed', 1601), ('requested_bytes.small_pool.peak', 59716), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 439, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7142), ('active.all.current', 7128), ('active.all.freed', 14), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7140), ('active.small_pool.current', 7126), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20495872), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 36864), ('active_bytes.all.peak', 20487680), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3685888), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 36864), ('active_bytes.small_pool.peak', 3677696), ('allocated_bytes.all.allocated', 20495872), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 36864), ('allocated_bytes.all.peak', 20487680), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3685888), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 36864), ('allocated_bytes.small_pool.peak', 3677696), ('allocation.all.allocated', 7142), ('allocation.all.current', 7128), ('allocation.all.freed', 14), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7140), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 10), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 9), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16796672), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12089856), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4230144), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3684864), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16874213), ('requested_bytes.all.current', 16841316), ('requested_bytes.all.freed', 32897), ('requested_bytes.all.peak', 16869700), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 64229), ('requested_bytes.small_pool.current', 31332), ('requested_bytes.small_pool.freed', 32897), ('requested_bytes.small_pool.peak', 59716), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 917, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 439, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7148), ('active.all.current', 7128), ('active.all.freed', 20), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7146), ('active.small_pool.current', 7126), ('active.small_pool.freed', 20), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20542464), ('active_bytes.all.current', 20501504), ('active_bytes.all.freed', 40960), ('active_bytes.all.peak', 20501504), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3732480), ('active_bytes.small_pool.current', 3691520), ('active_bytes.small_pool.freed', 40960), ('active_bytes.small_pool.peak', 3691520), ('allocated_bytes.all.allocated', 20542464), ('allocated_bytes.all.current', 20501504), ('allocated_bytes.all.freed', 40960), ('allocated_bytes.all.peak', 20501504), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3732480), ('allocated_bytes.small_pool.current', 3691520), ('allocated_bytes.small_pool.freed', 40960), ('allocated_bytes.small_pool.peak', 3691520), ('allocation.all.allocated', 7148), ('allocation.all.current', 7128), ('allocation.all.freed', 20), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7146), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 20), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 14), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 13), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 10), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16800768), ('inactive_split_bytes.all.current', 4664320), ('inactive_split_bytes.all.freed', 12136448), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4234240), ('inactive_split_bytes.small_pool.current', 502784), ('inactive_split_bytes.small_pool.freed', 3731456), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16920805), ('requested_bytes.all.current', 16884036), ('requested_bytes.all.freed', 36769), ('requested_bytes.all.peak', 16884036), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 110821), ('requested_bytes.small_pool.current', 74052), ('requested_bytes.small_pool.freed', 36769), ('requested_bytes.small_pool.peak', 74052), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 17), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 9), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 17), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 9), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 543, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7183), ('active.all.current', 7132), ('active.all.freed', 51), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 3), ('active.large_pool.freed', 4), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7176), ('active.small_pool.current', 7129), ('active.small_pool.freed', 47), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2147583488), ('active_bytes.all.current', 2121836032), ('active_bytes.all.freed', 25747456), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2116059136), ('active_bytes.large_pool.freed', 24641536), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 6882816), ('active_bytes.small_pool.current', 5776896), ('active_bytes.small_pool.freed', 1105920), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2147583488), ('allocated_bytes.all.current', 2121836032), ('allocated_bytes.all.freed', 25747456), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2116059136), ('allocated_bytes.large_pool.freed', 24641536), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 6882816), ('allocated_bytes.small_pool.current', 5776896), ('allocated_bytes.small_pool.freed', 1105920), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7183), ('allocation.all.current', 7132), ('allocation.all.freed', 51), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 3), ('allocation.large_pool.freed', 4), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7176), ('allocation.small_pool.current', 7129), ('allocation.small_pool.freed', 47), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 29), ('inactive_split.all.current', 6), ('inactive_split.all.freed', 23), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 4), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 3), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 25), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 20), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 52730880), ('inactive_split_bytes.all.current', 4676096), ('inactive_split_bytes.all.freed', 48054784), ('inactive_split_bytes.all.peak', 19490304), ('inactive_split_bytes.large_pool.allocated', 45858816), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 41697280), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 6872064), ('inactive_split_bytes.small_pool.current', 514560), ('inactive_split_bytes.small_pool.freed', 6357504), ('inactive_split_bytes.small_pool.peak', 2102784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2143547364), ('requested_bytes.all.current', 2117810772), ('requested_bytes.all.freed', 25736592), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2115649536), ('requested_bytes.large_pool.freed', 24641536), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 3256292), ('requested_bytes.small_pool.current', 2161236), ('requested_bytes.small_pool.freed', 1095056), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2147483648), ('reserved_bytes.all.current', 2126512128), ('reserved_bytes.all.freed', 20971520), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2120220672), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 6291456), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 6), ('segment.all.current', 5), ('segment.all.freed', 1), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 2), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 3), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 918, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 440, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 568, in forward
    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7215), ('active.all.current', 7134), ('active.all.freed', 81), ('active.all.peak', 7137), ('active.large_pool.allocated', 16), ('active.large_pool.current', 10), ('active.large_pool.freed', 6), ('active.large_pool.peak', 11), ('active.small_pool.allocated', 7199), ('active.small_pool.current', 7124), ('active.small_pool.freed', 75), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2233614336), ('active_bytes.all.current', 2187274240), ('active_bytes.all.freed', 46340096), ('active_bytes.all.peak', 2195663872), ('active_bytes.large_pool.allocated', 2224717824), ('active_bytes.large_pool.current', 2183299072), ('active_bytes.large_pool.freed', 41418752), ('active_bytes.large_pool.peak', 2191687680), ('active_bytes.small_pool.allocated', 8896512), ('active_bytes.small_pool.current', 3975168), ('active_bytes.small_pool.freed', 4921344), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2233614336), ('allocated_bytes.all.current', 2187274240), ('allocated_bytes.all.freed', 46340096), ('allocated_bytes.all.peak', 2195663872), ('allocated_bytes.large_pool.allocated', 2224717824), ('allocated_bytes.large_pool.current', 2183299072), ('allocated_bytes.large_pool.freed', 41418752), ('allocated_bytes.large_pool.peak', 2191687680), ('allocated_bytes.small_pool.allocated', 8896512), ('allocated_bytes.small_pool.current', 3975168), ('allocated_bytes.small_pool.freed', 4921344), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7215), ('allocation.all.current', 7134), ('allocation.all.freed', 81), ('allocation.all.peak', 7137), ('allocation.large_pool.allocated', 16), ('allocation.large_pool.current', 10), ('allocation.large_pool.freed', 6), ('allocation.large_pool.peak', 11), ('allocation.small_pool.allocated', 7199), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 75), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 47), ('inactive_split.all.current', 13), ('inactive_split.all.freed', 34), ('inactive_split.all.peak', 13), ('inactive_split.large_pool.allocated', 10), ('inactive_split.large_pool.current', 6), ('inactive_split.large_pool.freed', 4), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 37), ('inactive_split.small_pool.current', 7), ('inactive_split.small_pool.freed', 30), ('inactive_split.small_pool.peak', 8), ('inactive_split_bytes.all.allocated', 124965888), ('inactive_split_bytes.all.current', 39901184), ('inactive_split_bytes.all.freed', 85064704), ('inactive_split_bytes.all.peak', 39901184), ('inactive_split_bytes.large_pool.allocated', 112967680), ('inactive_split_bytes.large_pool.current', 37584896), ('inactive_split_bytes.large_pool.freed', 75382784), ('inactive_split_bytes.large_pool.peak', 37584896), ('inactive_split_bytes.small_pool.allocated', 11998208), ('inactive_split_bytes.small_pool.current', 2316288), ('inactive_split_bytes.small_pool.freed', 9681920), ('inactive_split_bytes.small_pool.peak', 2316288), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2229578212), ('requested_bytes.all.current', 2183250220), ('requested_bytes.all.freed', 46327992), ('requested_bytes.all.peak', 2191638836), ('requested_bytes.large_pool.allocated', 2224308224), ('requested_bytes.large_pool.current', 2182889472), ('requested_bytes.large_pool.freed', 41418752), ('requested_bytes.large_pool.peak', 2191278080), ('requested_bytes.small_pool.allocated', 5269988), ('requested_bytes.small_pool.current', 360748), ('requested_bytes.small_pool.freed', 4909240), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2250244096), ('reserved_bytes.all.current', 2227175424), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 2227175424), ('reserved_bytes.large_pool.allocated', 2241855488), ('reserved_bytes.large_pool.current', 2220883968), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2220883968), ('reserved_bytes.small_pool.allocated', 8388608), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 2097152), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 12), ('segment.all.current', 10), ('segment.all.freed', 2), ('segment.all.peak', 10), ('segment.large_pool.allocated', 8), ('segment.large_pool.current', 7), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 7), ('segment.small_pool.allocated', 4), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 1), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 923, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 445, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14660608), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 923, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 445, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 15), ('inactive_split.all.current', 6), ('inactive_split.all.freed', 9), ('inactive_split.all.peak', 6), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 12), ('inactive_split.small_pool.current', 4), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 923, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 445, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 543, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7183), ('active.all.current', 7132), ('active.all.freed', 51), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 3), ('active.large_pool.freed', 4), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7176), ('active.small_pool.current', 7129), ('active.small_pool.freed', 47), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2147583488), ('active_bytes.all.current', 2121836032), ('active_bytes.all.freed', 25747456), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2116059136), ('active_bytes.large_pool.freed', 24641536), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 6882816), ('active_bytes.small_pool.current', 5776896), ('active_bytes.small_pool.freed', 1105920), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2147583488), ('allocated_bytes.all.current', 2121836032), ('allocated_bytes.all.freed', 25747456), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2116059136), ('allocated_bytes.large_pool.freed', 24641536), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 6882816), ('allocated_bytes.small_pool.current', 5776896), ('allocated_bytes.small_pool.freed', 1105920), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7183), ('allocation.all.current', 7132), ('allocation.all.freed', 51), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 3), ('allocation.large_pool.freed', 4), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7176), ('allocation.small_pool.current', 7129), ('allocation.small_pool.freed', 47), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 29), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 22), ('inactive_split.all.peak', 7), ('inactive_split.large_pool.allocated', 4), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 3), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 25), ('inactive_split.small_pool.current', 6), ('inactive_split.small_pool.freed', 19), ('inactive_split.small_pool.peak', 6), ('inactive_split_bytes.all.allocated', 52730880), ('inactive_split_bytes.all.current', 4676096), ('inactive_split_bytes.all.freed', 48054784), ('inactive_split_bytes.all.peak', 19490304), ('inactive_split_bytes.large_pool.allocated', 45858816), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 41697280), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 6872064), ('inactive_split_bytes.small_pool.current', 514560), ('inactive_split_bytes.small_pool.freed', 6357504), ('inactive_split_bytes.small_pool.peak', 2102784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2143547364), ('requested_bytes.all.current', 2117810772), ('requested_bytes.all.freed', 25736592), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2115649536), ('requested_bytes.large_pool.freed', 24641536), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 3256292), ('requested_bytes.small_pool.current', 2161236), ('requested_bytes.small_pool.freed', 1095056), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2147483648), ('reserved_bytes.all.current', 2126512128), ('reserved_bytes.all.freed', 20971520), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2120220672), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 6291456), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 6), ('segment.all.current', 5), ('segment.all.freed', 1), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 2), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 3), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 923, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 445, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 568, in forward
    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7215), ('active.all.current', 7134), ('active.all.freed', 81), ('active.all.peak', 7137), ('active.large_pool.allocated', 16), ('active.large_pool.current', 10), ('active.large_pool.freed', 6), ('active.large_pool.peak', 11), ('active.small_pool.allocated', 7199), ('active.small_pool.current', 7124), ('active.small_pool.freed', 75), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2233614336), ('active_bytes.all.current', 2187274240), ('active_bytes.all.freed', 46340096), ('active_bytes.all.peak', 2195663872), ('active_bytes.large_pool.allocated', 2224717824), ('active_bytes.large_pool.current', 2183299072), ('active_bytes.large_pool.freed', 41418752), ('active_bytes.large_pool.peak', 2191687680), ('active_bytes.small_pool.allocated', 8896512), ('active_bytes.small_pool.current', 3975168), ('active_bytes.small_pool.freed', 4921344), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2233614336), ('allocated_bytes.all.current', 2187274240), ('allocated_bytes.all.freed', 46340096), ('allocated_bytes.all.peak', 2195663872), ('allocated_bytes.large_pool.allocated', 2224717824), ('allocated_bytes.large_pool.current', 2183299072), ('allocated_bytes.large_pool.freed', 41418752), ('allocated_bytes.large_pool.peak', 2191687680), ('allocated_bytes.small_pool.allocated', 8896512), ('allocated_bytes.small_pool.current', 3975168), ('allocated_bytes.small_pool.freed', 4921344), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7215), ('allocation.all.current', 7134), ('allocation.all.freed', 81), ('allocation.all.peak', 7137), ('allocation.large_pool.allocated', 16), ('allocation.large_pool.current', 10), ('allocation.large_pool.freed', 6), ('allocation.large_pool.peak', 11), ('allocation.small_pool.allocated', 7199), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 75), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 46), ('inactive_split.all.current', 12), ('inactive_split.all.freed', 34), ('inactive_split.all.peak', 12), ('inactive_split.large_pool.allocated', 10), ('inactive_split.large_pool.current', 6), ('inactive_split.large_pool.freed', 4), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 36), ('inactive_split.small_pool.current', 6), ('inactive_split.small_pool.freed', 30), ('inactive_split.small_pool.peak', 7), ('inactive_split_bytes.all.allocated', 124965888), ('inactive_split_bytes.all.current', 39901184), ('inactive_split_bytes.all.freed', 85064704), ('inactive_split_bytes.all.peak', 39901184), ('inactive_split_bytes.large_pool.allocated', 112967680), ('inactive_split_bytes.large_pool.current', 37584896), ('inactive_split_bytes.large_pool.freed', 75382784), ('inactive_split_bytes.large_pool.peak', 37584896), ('inactive_split_bytes.small_pool.allocated', 11998208), ('inactive_split_bytes.small_pool.current', 2316288), ('inactive_split_bytes.small_pool.freed', 9681920), ('inactive_split_bytes.small_pool.peak', 2316288), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2229578212), ('requested_bytes.all.current', 2183250220), ('requested_bytes.all.freed', 46327992), ('requested_bytes.all.peak', 2191638836), ('requested_bytes.large_pool.allocated', 2224308224), ('requested_bytes.large_pool.current', 2182889472), ('requested_bytes.large_pool.freed', 41418752), ('requested_bytes.large_pool.peak', 2191278080), ('requested_bytes.small_pool.allocated', 5269988), ('requested_bytes.small_pool.current', 360748), ('requested_bytes.small_pool.freed', 4909240), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2250244096), ('reserved_bytes.all.current', 2227175424), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 2227175424), ('reserved_bytes.large_pool.allocated', 2241855488), ('reserved_bytes.large_pool.current', 2220883968), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2220883968), ('reserved_bytes.small_pool.allocated', 8388608), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 2097152), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 12), ('segment.all.current', 10), ('segment.all.freed', 2), ('segment.all.peak', 10), ('segment.large_pool.allocated', 8), ('segment.large_pool.current', 7), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 7), ('segment.small_pool.allocated', 4), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 1), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 17), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 9), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 543, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7183), ('active.all.current', 7132), ('active.all.freed', 51), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 3), ('active.large_pool.freed', 4), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7176), ('active.small_pool.current', 7129), ('active.small_pool.freed', 47), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2147583488), ('active_bytes.all.current', 2121836032), ('active_bytes.all.freed', 25747456), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2116059136), ('active_bytes.large_pool.freed', 24641536), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 6882816), ('active_bytes.small_pool.current', 5776896), ('active_bytes.small_pool.freed', 1105920), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2147583488), ('allocated_bytes.all.current', 2121836032), ('allocated_bytes.all.freed', 25747456), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2116059136), ('allocated_bytes.large_pool.freed', 24641536), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 6882816), ('allocated_bytes.small_pool.current', 5776896), ('allocated_bytes.small_pool.freed', 1105920), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7183), ('allocation.all.current', 7132), ('allocation.all.freed', 51), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 3), ('allocation.large_pool.freed', 4), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7176), ('allocation.small_pool.current', 7129), ('allocation.small_pool.freed', 47), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 29), ('inactive_split.all.current', 6), ('inactive_split.all.freed', 23), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 4), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 3), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 25), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 20), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 52730880), ('inactive_split_bytes.all.current', 4676096), ('inactive_split_bytes.all.freed', 48054784), ('inactive_split_bytes.all.peak', 19490304), ('inactive_split_bytes.large_pool.allocated', 45858816), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 41697280), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 6872064), ('inactive_split_bytes.small_pool.current', 514560), ('inactive_split_bytes.small_pool.freed', 6357504), ('inactive_split_bytes.small_pool.peak', 2102784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2143547364), ('requested_bytes.all.current', 2117810772), ('requested_bytes.all.freed', 25736592), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2115649536), ('requested_bytes.large_pool.freed', 24641536), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 3256292), ('requested_bytes.small_pool.current', 2161236), ('requested_bytes.small_pool.freed', 1095056), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2147483648), ('reserved_bytes.all.current', 2126512128), ('reserved_bytes.all.freed', 20971520), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2120220672), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 6291456), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 6), ('segment.all.current', 5), ('segment.all.freed', 1), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 2), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 3), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 568, in forward
    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7215), ('active.all.current', 7134), ('active.all.freed', 81), ('active.all.peak', 7137), ('active.large_pool.allocated', 16), ('active.large_pool.current', 10), ('active.large_pool.freed', 6), ('active.large_pool.peak', 11), ('active.small_pool.allocated', 7199), ('active.small_pool.current', 7124), ('active.small_pool.freed', 75), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2233614336), ('active_bytes.all.current', 2187274240), ('active_bytes.all.freed', 46340096), ('active_bytes.all.peak', 2195663872), ('active_bytes.large_pool.allocated', 2224717824), ('active_bytes.large_pool.current', 2183299072), ('active_bytes.large_pool.freed', 41418752), ('active_bytes.large_pool.peak', 2191687680), ('active_bytes.small_pool.allocated', 8896512), ('active_bytes.small_pool.current', 3975168), ('active_bytes.small_pool.freed', 4921344), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2233614336), ('allocated_bytes.all.current', 2187274240), ('allocated_bytes.all.freed', 46340096), ('allocated_bytes.all.peak', 2195663872), ('allocated_bytes.large_pool.allocated', 2224717824), ('allocated_bytes.large_pool.current', 2183299072), ('allocated_bytes.large_pool.freed', 41418752), ('allocated_bytes.large_pool.peak', 2191687680), ('allocated_bytes.small_pool.allocated', 8896512), ('allocated_bytes.small_pool.current', 3975168), ('allocated_bytes.small_pool.freed', 4921344), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7215), ('allocation.all.current', 7134), ('allocation.all.freed', 81), ('allocation.all.peak', 7137), ('allocation.large_pool.allocated', 16), ('allocation.large_pool.current', 10), ('allocation.large_pool.freed', 6), ('allocation.large_pool.peak', 11), ('allocation.small_pool.allocated', 7199), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 75), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 47), ('inactive_split.all.current', 13), ('inactive_split.all.freed', 34), ('inactive_split.all.peak', 13), ('inactive_split.large_pool.allocated', 10), ('inactive_split.large_pool.current', 6), ('inactive_split.large_pool.freed', 4), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 37), ('inactive_split.small_pool.current', 7), ('inactive_split.small_pool.freed', 30), ('inactive_split.small_pool.peak', 8), ('inactive_split_bytes.all.allocated', 124965888), ('inactive_split_bytes.all.current', 39901184), ('inactive_split_bytes.all.freed', 85064704), ('inactive_split_bytes.all.peak', 39901184), ('inactive_split_bytes.large_pool.allocated', 112967680), ('inactive_split_bytes.large_pool.current', 37584896), ('inactive_split_bytes.large_pool.freed', 75382784), ('inactive_split_bytes.large_pool.peak', 37584896), ('inactive_split_bytes.small_pool.allocated', 11998208), ('inactive_split_bytes.small_pool.current', 2316288), ('inactive_split_bytes.small_pool.freed', 9681920), ('inactive_split_bytes.small_pool.peak', 2316288), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2229578212), ('requested_bytes.all.current', 2183250220), ('requested_bytes.all.freed', 46327992), ('requested_bytes.all.peak', 2191638836), ('requested_bytes.large_pool.allocated', 2224308224), ('requested_bytes.large_pool.current', 2182889472), ('requested_bytes.large_pool.freed', 41418752), ('requested_bytes.large_pool.peak', 2191278080), ('requested_bytes.small_pool.allocated', 5269988), ('requested_bytes.small_pool.current', 360748), ('requested_bytes.small_pool.freed', 4909240), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2250244096), ('reserved_bytes.all.current', 2227175424), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 2227175424), ('reserved_bytes.large_pool.allocated', 2241855488), ('reserved_bytes.large_pool.current', 2220883968), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2220883968), ('reserved_bytes.small_pool.allocated', 8388608), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 2097152), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 12), ('segment.all.current', 10), ('segment.all.freed', 2), ('segment.all.peak', 10), ('segment.large_pool.allocated', 8), ('segment.large_pool.current', 7), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 7), ('segment.small_pool.allocated', 4), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 1), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 17), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 9), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 543, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7183), ('active.all.current', 7132), ('active.all.freed', 51), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 3), ('active.large_pool.freed', 4), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7176), ('active.small_pool.current', 7129), ('active.small_pool.freed', 47), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2147583488), ('active_bytes.all.current', 2121836032), ('active_bytes.all.freed', 25747456), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2116059136), ('active_bytes.large_pool.freed', 24641536), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 6882816), ('active_bytes.small_pool.current', 5776896), ('active_bytes.small_pool.freed', 1105920), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2147583488), ('allocated_bytes.all.current', 2121836032), ('allocated_bytes.all.freed', 25747456), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2116059136), ('allocated_bytes.large_pool.freed', 24641536), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 6882816), ('allocated_bytes.small_pool.current', 5776896), ('allocated_bytes.small_pool.freed', 1105920), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7183), ('allocation.all.current', 7132), ('allocation.all.freed', 51), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 3), ('allocation.large_pool.freed', 4), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7176), ('allocation.small_pool.current', 7129), ('allocation.small_pool.freed', 47), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 29), ('inactive_split.all.current', 6), ('inactive_split.all.freed', 23), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 4), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 3), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 25), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 20), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 52730880), ('inactive_split_bytes.all.current', 4676096), ('inactive_split_bytes.all.freed', 48054784), ('inactive_split_bytes.all.peak', 19490304), ('inactive_split_bytes.large_pool.allocated', 45858816), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 41697280), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 6872064), ('inactive_split_bytes.small_pool.current', 514560), ('inactive_split_bytes.small_pool.freed', 6357504), ('inactive_split_bytes.small_pool.peak', 2102784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2143547364), ('requested_bytes.all.current', 2117810772), ('requested_bytes.all.freed', 25736592), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2115649536), ('requested_bytes.large_pool.freed', 24641536), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 3256292), ('requested_bytes.small_pool.current', 2161236), ('requested_bytes.small_pool.freed', 1095056), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2147483648), ('reserved_bytes.all.current', 2126512128), ('reserved_bytes.all.freed', 20971520), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2120220672), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 6291456), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 6), ('segment.all.current', 5), ('segment.all.freed', 1), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 2), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 3), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 922, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 444, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 568, in forward
    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7215), ('active.all.current', 7134), ('active.all.freed', 81), ('active.all.peak', 7137), ('active.large_pool.allocated', 16), ('active.large_pool.current', 10), ('active.large_pool.freed', 6), ('active.large_pool.peak', 11), ('active.small_pool.allocated', 7199), ('active.small_pool.current', 7124), ('active.small_pool.freed', 75), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2233614336), ('active_bytes.all.current', 2187274240), ('active_bytes.all.freed', 46340096), ('active_bytes.all.peak', 2195663872), ('active_bytes.large_pool.allocated', 2224717824), ('active_bytes.large_pool.current', 2183299072), ('active_bytes.large_pool.freed', 41418752), ('active_bytes.large_pool.peak', 2191687680), ('active_bytes.small_pool.allocated', 8896512), ('active_bytes.small_pool.current', 3975168), ('active_bytes.small_pool.freed', 4921344), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2233614336), ('allocated_bytes.all.current', 2187274240), ('allocated_bytes.all.freed', 46340096), ('allocated_bytes.all.peak', 2195663872), ('allocated_bytes.large_pool.allocated', 2224717824), ('allocated_bytes.large_pool.current', 2183299072), ('allocated_bytes.large_pool.freed', 41418752), ('allocated_bytes.large_pool.peak', 2191687680), ('allocated_bytes.small_pool.allocated', 8896512), ('allocated_bytes.small_pool.current', 3975168), ('allocated_bytes.small_pool.freed', 4921344), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7215), ('allocation.all.current', 7134), ('allocation.all.freed', 81), ('allocation.all.peak', 7137), ('allocation.large_pool.allocated', 16), ('allocation.large_pool.current', 10), ('allocation.large_pool.freed', 6), ('allocation.large_pool.peak', 11), ('allocation.small_pool.allocated', 7199), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 75), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 47), ('inactive_split.all.current', 13), ('inactive_split.all.freed', 34), ('inactive_split.all.peak', 13), ('inactive_split.large_pool.allocated', 10), ('inactive_split.large_pool.current', 6), ('inactive_split.large_pool.freed', 4), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 37), ('inactive_split.small_pool.current', 7), ('inactive_split.small_pool.freed', 30), ('inactive_split.small_pool.peak', 8), ('inactive_split_bytes.all.allocated', 124965888), ('inactive_split_bytes.all.current', 39901184), ('inactive_split_bytes.all.freed', 85064704), ('inactive_split_bytes.all.peak', 39901184), ('inactive_split_bytes.large_pool.allocated', 112967680), ('inactive_split_bytes.large_pool.current', 37584896), ('inactive_split_bytes.large_pool.freed', 75382784), ('inactive_split_bytes.large_pool.peak', 37584896), ('inactive_split_bytes.small_pool.allocated', 11998208), ('inactive_split_bytes.small_pool.current', 2316288), ('inactive_split_bytes.small_pool.freed', 9681920), ('inactive_split_bytes.small_pool.peak', 2316288), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2229578212), ('requested_bytes.all.current', 2183250220), ('requested_bytes.all.freed', 46327992), ('requested_bytes.all.peak', 2191638836), ('requested_bytes.large_pool.allocated', 2224308224), ('requested_bytes.large_pool.current', 2182889472), ('requested_bytes.large_pool.freed', 41418752), ('requested_bytes.large_pool.peak', 2191278080), ('requested_bytes.small_pool.allocated', 5269988), ('requested_bytes.small_pool.current', 360748), ('requested_bytes.small_pool.freed', 4909240), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2250244096), ('reserved_bytes.all.current', 2227175424), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 2227175424), ('reserved_bytes.large_pool.allocated', 2241855488), ('reserved_bytes.large_pool.current', 2220883968), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2220883968), ('reserved_bytes.small_pool.allocated', 8388608), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 2097152), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 12), ('segment.all.current', 10), ('segment.all.freed', 2), ('segment.all.peak', 10), ('segment.large_pool.allocated', 8), ('segment.large_pool.current', 7), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 7), ('segment.small_pool.allocated', 4), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 1), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 925, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 925, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 687, in forward
    hidden_states = self.self_attn_layer_norm(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA__native_layer_norm)
OrderedDict([('active.all.allocated', 7154), ('active.all.current', 7131), ('active.all.freed', 23), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 6), ('active.large_pool.freed', 1), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7147), ('active.small_pool.current', 7125), ('active.small_pool.freed', 22), ('active.small_pool.peak', 7130), ('active_bytes.all.allocated', 2144882176), ('active_bytes.all.current', 2138358784), ('active_bytes.all.freed', 6523392), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2134540288), ('active_bytes.large_pool.freed', 6160384), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 4181504), ('active_bytes.small_pool.current', 3818496), ('active_bytes.small_pool.freed', 363008), ('active_bytes.small_pool.peak', 4139008), ('allocated_bytes.all.allocated', 2144882176), ('allocated_bytes.all.current', 2138358784), ('allocated_bytes.all.freed', 6523392), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2134540288), ('allocated_bytes.large_pool.freed', 6160384), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 4181504), ('allocated_bytes.small_pool.current', 3818496), ('allocated_bytes.small_pool.freed', 363008), ('allocated_bytes.small_pool.peak', 4139008), ('allocation.all.allocated', 7154), ('allocation.all.current', 7131), ('allocation.all.freed', 23), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 6), ('allocation.large_pool.freed', 1), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7147), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 22), ('allocation.small_pool.peak', 7130), ('inactive_split.all.allocated', 17), ('inactive_split.all.current', 7), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 3), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 1), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 9), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 38094336), ('inactive_split_bytes.all.current', 7027712), ('inactive_split_bytes.all.freed', 31066624), ('inactive_split_bytes.all.peak', 19489280), ('inactive_split_bytes.large_pool.allocated', 33538048), ('inactive_split_bytes.large_pool.current', 6651904), ('inactive_split_bytes.large_pool.freed', 26886144), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 4556288), ('inactive_split_bytes.small_pool.current', 375808), ('inactive_split_bytes.small_pool.freed', 4180480), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2140848709), ('requested_bytes.all.current', 2134331772), ('requested_bytes.all.freed', 6516937), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2134130688), ('requested_bytes.large_pool.freed', 6160384), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 557637), ('requested_bytes.small_pool.current', 201084), ('requested_bytes.small_pool.freed', 356553), ('requested_bytes.small_pool.peak', 519188), ('reserved_bytes.all.allocated', 2145386496), ('reserved_bytes.all.current', 2145386496), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2141192192), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 5), ('segment.all.current', 5), ('segment.all.freed', 0), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 3), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 925, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 543, in forward
    query_states = self.q_proj(hidden_states) * self.scaling
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7183), ('active.all.current', 7132), ('active.all.freed', 51), ('active.all.peak', 7136), ('active.large_pool.allocated', 7), ('active.large_pool.current', 3), ('active.large_pool.freed', 4), ('active.large_pool.peak', 6), ('active.small_pool.allocated', 7176), ('active.small_pool.current', 7129), ('active.small_pool.freed', 47), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2147583488), ('active_bytes.all.current', 2121836032), ('active_bytes.all.freed', 25747456), ('active_bytes.all.peak', 2138679296), ('active_bytes.large_pool.allocated', 2140700672), ('active_bytes.large_pool.current', 2116059136), ('active_bytes.large_pool.freed', 24641536), ('active_bytes.large_pool.peak', 2134540288), ('active_bytes.small_pool.allocated', 6882816), ('active_bytes.small_pool.current', 5776896), ('active_bytes.small_pool.freed', 1105920), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2147583488), ('allocated_bytes.all.current', 2121836032), ('allocated_bytes.all.freed', 25747456), ('allocated_bytes.all.peak', 2138679296), ('allocated_bytes.large_pool.allocated', 2140700672), ('allocated_bytes.large_pool.current', 2116059136), ('allocated_bytes.large_pool.freed', 24641536), ('allocated_bytes.large_pool.peak', 2134540288), ('allocated_bytes.small_pool.allocated', 6882816), ('allocated_bytes.small_pool.current', 5776896), ('allocated_bytes.small_pool.freed', 1105920), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7183), ('allocation.all.current', 7132), ('allocation.all.freed', 51), ('allocation.all.peak', 7136), ('allocation.large_pool.allocated', 7), ('allocation.large_pool.current', 3), ('allocation.large_pool.freed', 4), ('allocation.large_pool.peak', 6), ('allocation.small_pool.allocated', 7176), ('allocation.small_pool.current', 7129), ('allocation.small_pool.freed', 47), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 29), ('inactive_split.all.current', 6), ('inactive_split.all.freed', 23), ('inactive_split.all.peak', 8), ('inactive_split.large_pool.allocated', 4), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 3), ('inactive_split.large_pool.peak', 3), ('inactive_split.small_pool.allocated', 25), ('inactive_split.small_pool.current', 5), ('inactive_split.small_pool.freed', 20), ('inactive_split.small_pool.peak', 5), ('inactive_split_bytes.all.allocated', 52730880), ('inactive_split_bytes.all.current', 4676096), ('inactive_split_bytes.all.freed', 48054784), ('inactive_split_bytes.all.peak', 19490304), ('inactive_split_bytes.large_pool.allocated', 45858816), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 41697280), ('inactive_split_bytes.large_pool.peak', 18972672), ('inactive_split_bytes.small_pool.allocated', 6872064), ('inactive_split_bytes.small_pool.current', 514560), ('inactive_split_bytes.small_pool.freed', 6357504), ('inactive_split_bytes.small_pool.peak', 2102784), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2143547364), ('requested_bytes.all.current', 2117810772), ('requested_bytes.all.freed', 25736592), ('requested_bytes.all.peak', 2134649876), ('requested_bytes.large_pool.allocated', 2140291072), ('requested_bytes.large_pool.current', 2115649536), ('requested_bytes.large_pool.freed', 24641536), ('requested_bytes.large_pool.peak', 2134130688), ('requested_bytes.small_pool.allocated', 3256292), ('requested_bytes.small_pool.current', 2161236), ('requested_bytes.small_pool.freed', 1095056), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2147483648), ('reserved_bytes.all.current', 2126512128), ('reserved_bytes.all.freed', 20971520), ('reserved_bytes.all.peak', 2145386496), ('reserved_bytes.large_pool.allocated', 2141192192), ('reserved_bytes.large_pool.current', 2120220672), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2141192192), ('reserved_bytes.small_pool.allocated', 6291456), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 6), ('segment.all.current', 5), ('segment.all.freed', 1), ('segment.all.peak', 5), ('segment.large_pool.allocated', 3), ('segment.large_pool.current', 2), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 3), ('segment.small_pool.allocated', 3), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 925, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 688, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 568, in forward
    key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
OrderedDict([('active.all.allocated', 7215), ('active.all.current', 7134), ('active.all.freed', 81), ('active.all.peak', 7137), ('active.large_pool.allocated', 16), ('active.large_pool.current', 10), ('active.large_pool.freed', 6), ('active.large_pool.peak', 11), ('active.small_pool.allocated', 7199), ('active.small_pool.current', 7124), ('active.small_pool.freed', 75), ('active.small_pool.peak', 7133), ('active_bytes.all.allocated', 2233614336), ('active_bytes.all.current', 2187274240), ('active_bytes.all.freed', 46340096), ('active_bytes.all.peak', 2195663872), ('active_bytes.large_pool.allocated', 2224717824), ('active_bytes.large_pool.current', 2183299072), ('active_bytes.large_pool.freed', 41418752), ('active_bytes.large_pool.peak', 2191687680), ('active_bytes.small_pool.allocated', 8896512), ('active_bytes.small_pool.current', 3975168), ('active_bytes.small_pool.freed', 4921344), ('active_bytes.small_pool.peak', 5778944), ('allocated_bytes.all.allocated', 2233614336), ('allocated_bytes.all.current', 2187274240), ('allocated_bytes.all.freed', 46340096), ('allocated_bytes.all.peak', 2195663872), ('allocated_bytes.large_pool.allocated', 2224717824), ('allocated_bytes.large_pool.current', 2183299072), ('allocated_bytes.large_pool.freed', 41418752), ('allocated_bytes.large_pool.peak', 2191687680), ('allocated_bytes.small_pool.allocated', 8896512), ('allocated_bytes.small_pool.current', 3975168), ('allocated_bytes.small_pool.freed', 4921344), ('allocated_bytes.small_pool.peak', 5778944), ('allocation.all.allocated', 7215), ('allocation.all.current', 7134), ('allocation.all.freed', 81), ('allocation.all.peak', 7137), ('allocation.large_pool.allocated', 16), ('allocation.large_pool.current', 10), ('allocation.large_pool.freed', 6), ('allocation.large_pool.peak', 11), ('allocation.small_pool.allocated', 7199), ('allocation.small_pool.current', 7124), ('allocation.small_pool.freed', 75), ('allocation.small_pool.peak', 7133), ('inactive_split.all.allocated', 47), ('inactive_split.all.current', 13), ('inactive_split.all.freed', 34), ('inactive_split.all.peak', 13), ('inactive_split.large_pool.allocated', 10), ('inactive_split.large_pool.current', 6), ('inactive_split.large_pool.freed', 4), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 37), ('inactive_split.small_pool.current', 7), ('inactive_split.small_pool.freed', 30), ('inactive_split.small_pool.peak', 8), ('inactive_split_bytes.all.allocated', 124965888), ('inactive_split_bytes.all.current', 39901184), ('inactive_split_bytes.all.freed', 85064704), ('inactive_split_bytes.all.peak', 39901184), ('inactive_split_bytes.large_pool.allocated', 112967680), ('inactive_split_bytes.large_pool.current', 37584896), ('inactive_split_bytes.large_pool.freed', 75382784), ('inactive_split_bytes.large_pool.peak', 37584896), ('inactive_split_bytes.small_pool.allocated', 11998208), ('inactive_split_bytes.small_pool.current', 2316288), ('inactive_split_bytes.small_pool.freed', 9681920), ('inactive_split_bytes.small_pool.peak', 2316288), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 2229578212), ('requested_bytes.all.current', 2183250220), ('requested_bytes.all.freed', 46327992), ('requested_bytes.all.peak', 2191638836), ('requested_bytes.large_pool.allocated', 2224308224), ('requested_bytes.large_pool.current', 2182889472), ('requested_bytes.large_pool.freed', 41418752), ('requested_bytes.large_pool.peak', 2191278080), ('requested_bytes.small_pool.allocated', 5269988), ('requested_bytes.small_pool.current', 360748), ('requested_bytes.small_pool.freed', 4909240), ('requested_bytes.small_pool.peak', 2161756), ('reserved_bytes.all.allocated', 2250244096), ('reserved_bytes.all.current', 2227175424), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 2227175424), ('reserved_bytes.large_pool.allocated', 2241855488), ('reserved_bytes.large_pool.current', 2220883968), ('reserved_bytes.large_pool.freed', 20971520), ('reserved_bytes.large_pool.peak', 2220883968), ('reserved_bytes.small_pool.allocated', 8388608), ('reserved_bytes.small_pool.current', 6291456), ('reserved_bytes.small_pool.freed', 2097152), ('reserved_bytes.small_pool.peak', 6291456), ('segment.all.allocated', 12), ('segment.all.current', 10), ('segment.all.freed', 2), ('segment.all.peak', 10), ('segment.large_pool.allocated', 8), ('segment.large_pool.current', 7), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 7), ('segment.small_pool.allocated', 4), ('segment.small_pool.current', 3), ('segment.small_pool.freed', 1), ('segment.small_pool.peak', 3)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7136), ('active.all.current', 7128), ('active.all.freed', 8), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7134), ('active.small_pool.current', 7126), ('active.small_pool.freed', 8), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20492288), ('active_bytes.all.current', 20487680), ('active_bytes.all.freed', 4608), ('active_bytes.all.peak', 20487680), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3682304), ('active_bytes.small_pool.current', 3677696), ('active_bytes.small_pool.freed', 4608), ('active_bytes.small_pool.peak', 3677696), ('allocated_bytes.all.allocated', 20492288), ('allocated_bytes.all.current', 20487680), ('allocated_bytes.all.freed', 4608), ('allocated_bytes.all.peak', 20487680), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3682304), ('allocated_bytes.small_pool.current', 3677696), ('allocated_bytes.small_pool.freed', 4608), ('allocated_bytes.small_pool.peak', 3677696), ('allocation.all.allocated', 7136), ('allocation.all.current', 7128), ('allocation.all.freed', 8), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7134), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 8), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 8), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 4), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 7), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 4), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16764416), ('inactive_split_bytes.all.current', 4678144), ('inactive_split_bytes.all.freed', 12086272), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4197888), ('inactive_split_bytes.small_pool.current', 516608), ('inactive_split_bytes.small_pool.freed', 3681280), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16871301), ('requested_bytes.all.current', 16869700), ('requested_bytes.all.freed', 1601), ('requested_bytes.all.peak', 16869700), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 61317), ('requested_bytes.small_pool.current', 59716), ('requested_bytes.small_pool.freed', 1601), ('requested_bytes.small_pool.peak', 59716), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7142), ('active.all.current', 7128), ('active.all.freed', 14), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7140), ('active.small_pool.current', 7126), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20495872), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 36864), ('active_bytes.all.peak', 20487680), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3685888), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 36864), ('active_bytes.small_pool.peak', 3677696), ('allocated_bytes.all.allocated', 20495872), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 36864), ('allocated_bytes.all.peak', 20487680), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3685888), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 36864), ('allocated_bytes.small_pool.peak', 3677696), ('allocation.all.allocated', 7142), ('allocation.all.current', 7128), ('allocation.all.freed', 14), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7140), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 10), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 9), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16796672), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12089856), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4230144), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3684864), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16874213), ('requested_bytes.all.current', 16841316), ('requested_bytes.all.freed', 32897), ('requested_bytes.all.peak', 16869700), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 64229), ('requested_bytes.small_pool.current', 31332), ('requested_bytes.small_pool.freed', 32897), ('requested_bytes.small_pool.peak', 59716), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7148), ('active.all.current', 7128), ('active.all.freed', 20), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7146), ('active.small_pool.current', 7126), ('active.small_pool.freed', 20), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20542464), ('active_bytes.all.current', 20501504), ('active_bytes.all.freed', 40960), ('active_bytes.all.peak', 20501504), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3732480), ('active_bytes.small_pool.current', 3691520), ('active_bytes.small_pool.freed', 40960), ('active_bytes.small_pool.peak', 3691520), ('allocated_bytes.all.allocated', 20542464), ('allocated_bytes.all.current', 20501504), ('allocated_bytes.all.freed', 40960), ('allocated_bytes.all.peak', 20501504), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3732480), ('allocated_bytes.small_pool.current', 3691520), ('allocated_bytes.small_pool.freed', 40960), ('allocated_bytes.small_pool.peak', 3691520), ('allocation.all.allocated', 7148), ('allocation.all.current', 7128), ('allocation.all.freed', 20), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7146), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 20), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 14), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 10), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 13), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 10), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16800768), ('inactive_split_bytes.all.current', 4664320), ('inactive_split_bytes.all.freed', 12136448), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4234240), ('inactive_split_bytes.small_pool.current', 502784), ('inactive_split_bytes.small_pool.freed', 3731456), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16920805), ('requested_bytes.all.current', 16884036), ('requested_bytes.all.freed', 36769), ('requested_bytes.all.peak', 16884036), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 110821), ('requested_bytes.small_pool.current', 74052), ('requested_bytes.small_pool.freed', 36769), ('requested_bytes.small_pool.peak', 74052), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7133), ('active.all.current', 7129), ('active.all.freed', 4), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7131), ('active.small_pool.current', 7127), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20461056), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20460032), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651072), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3650048), ('allocated_bytes.all.allocated', 20461056), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20460032), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651072), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3650048), ('allocation.all.allocated', 7133), ('allocation.all.current', 7129), ('allocation.all.freed', 4), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7131), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12055040), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3650048), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839507), ('requested_bytes.all.current', 16839460), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16839464), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29523), ('requested_bytes.small_pool.current', 29476), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 29480), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7129), ('active.all.freed', 14), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7127), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20502528), ('active_bytes.all.current', 20493824), ('active_bytes.all.freed', 8704), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3692544), ('active_bytes.small_pool.current', 3683840), ('active_bytes.small_pool.freed', 8704), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20502528), ('allocated_bytes.all.current', 20493824), ('allocated_bytes.all.freed', 8704), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3692544), ('allocated_bytes.small_pool.current', 3683840), ('allocated_bytes.small_pool.freed', 8704), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7143), ('allocation.all.current', 7129), ('allocation.all.freed', 14), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 12), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 11), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16768512), ('inactive_split_bytes.all.current', 4672000), ('inactive_split_bytes.all.freed', 12096512), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4201984), ('inactive_split_bytes.small_pool.current', 510464), ('inactive_split_bytes.small_pool.freed', 3691520), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16879012), ('requested_bytes.all.current', 16875716), ('requested_bytes.all.freed', 3296), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 69028), ('requested_bytes.small_pool.current', 65732), ('requested_bytes.small_pool.freed', 3296), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7152), ('active.all.current', 7129), ('active.all.freed', 23), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7150), ('active.small_pool.current', 7127), ('active.small_pool.freed', 23), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20507648), ('active_bytes.all.current', 20459520), ('active_bytes.all.freed', 48128), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3697664), ('active_bytes.small_pool.current', 3649536), ('active_bytes.small_pool.freed', 48128), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20507648), ('allocated_bytes.all.current', 20459520), ('allocated_bytes.all.freed', 48128), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3697664), ('allocated_bytes.small_pool.current', 3649536), ('allocated_bytes.small_pool.freed', 48128), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7152), ('allocation.all.current', 7129), ('allocation.all.freed', 23), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7150), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 23), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 15), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 13), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 13), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16807936), ('inactive_split_bytes.all.current', 4706304), ('inactive_split_bytes.all.freed', 12101632), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4241408), ('inactive_split_bytes.small_pool.current', 544768), ('inactive_split_bytes.small_pool.freed', 3696640), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16882501), ('requested_bytes.all.current', 16841828), ('requested_bytes.all.freed', 40673), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 72517), ('requested_bytes.small_pool.current', 31844), ('requested_bytes.small_pool.freed', 40673), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7162), ('active.all.current', 7129), ('active.all.freed', 33), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7160), ('active.small_pool.current', 7127), ('active.small_pool.freed', 33), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20564992), ('active_bytes.all.current', 20509696), ('active_bytes.all.freed', 55296), ('active_bytes.all.peak', 20510720), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3755008), ('active_bytes.small_pool.current', 3699712), ('active_bytes.small_pool.freed', 55296), ('active_bytes.small_pool.peak', 3700736), ('allocated_bytes.all.allocated', 20564992), ('allocated_bytes.all.current', 20509696), ('allocated_bytes.all.freed', 55296), ('allocated_bytes.all.peak', 20510720), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3755008), ('allocated_bytes.small_pool.current', 3699712), ('allocated_bytes.small_pool.freed', 55296), ('allocated_bytes.small_pool.peak', 3700736), ('allocation.all.allocated', 7162), ('allocation.all.current', 7129), ('allocation.all.freed', 33), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7160), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 33), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 22), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 18), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 21), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 18), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16815104), ('inactive_split_bytes.all.current', 4656128), ('inactive_split_bytes.all.freed', 12158976), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4248576), ('inactive_split_bytes.small_pool.current', 494592), ('inactive_split_bytes.small_pool.freed', 3753984), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16939334), ('requested_bytes.all.current', 16892228), ('requested_bytes.all.freed', 47106), ('requested_bytes.all.peak', 16893252), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 129350), ('requested_bytes.small_pool.current', 82244), ('requested_bytes.small_pool.freed', 47106), ('requested_bytes.small_pool.peak', 83268), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7133), ('active.all.current', 7129), ('active.all.freed', 4), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7131), ('active.small_pool.current', 7127), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20461056), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20460032), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651072), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3650048), ('allocated_bytes.all.allocated', 20461056), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20460032), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651072), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3650048), ('allocation.all.allocated', 7133), ('allocation.all.current', 7129), ('allocation.all.freed', 4), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7131), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12055040), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3650048), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839507), ('requested_bytes.all.current', 16839460), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16839464), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29523), ('requested_bytes.small_pool.current', 29476), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 29480), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7129), ('active.all.freed', 14), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7127), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20502528), ('active_bytes.all.current', 20493824), ('active_bytes.all.freed', 8704), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3692544), ('active_bytes.small_pool.current', 3683840), ('active_bytes.small_pool.freed', 8704), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20502528), ('allocated_bytes.all.current', 20493824), ('allocated_bytes.all.freed', 8704), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3692544), ('allocated_bytes.small_pool.current', 3683840), ('allocated_bytes.small_pool.freed', 8704), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7143), ('allocation.all.current', 7129), ('allocation.all.freed', 14), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 12), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 11), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16768512), ('inactive_split_bytes.all.current', 4672000), ('inactive_split_bytes.all.freed', 12096512), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4201984), ('inactive_split_bytes.small_pool.current', 510464), ('inactive_split_bytes.small_pool.freed', 3691520), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16879012), ('requested_bytes.all.current', 16875716), ('requested_bytes.all.freed', 3296), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 69028), ('requested_bytes.small_pool.current', 65732), ('requested_bytes.small_pool.freed', 3296), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7152), ('active.all.current', 7129), ('active.all.freed', 23), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7150), ('active.small_pool.current', 7127), ('active.small_pool.freed', 23), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20507648), ('active_bytes.all.current', 20459520), ('active_bytes.all.freed', 48128), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3697664), ('active_bytes.small_pool.current', 3649536), ('active_bytes.small_pool.freed', 48128), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20507648), ('allocated_bytes.all.current', 20459520), ('allocated_bytes.all.freed', 48128), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3697664), ('allocated_bytes.small_pool.current', 3649536), ('allocated_bytes.small_pool.freed', 48128), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7152), ('allocation.all.current', 7129), ('allocation.all.freed', 23), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7150), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 23), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 15), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 13), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 13), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16807936), ('inactive_split_bytes.all.current', 4706304), ('inactive_split_bytes.all.freed', 12101632), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4241408), ('inactive_split_bytes.small_pool.current', 544768), ('inactive_split_bytes.small_pool.freed', 3696640), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16882501), ('requested_bytes.all.current', 16841828), ('requested_bytes.all.freed', 40673), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 72517), ('requested_bytes.small_pool.current', 31844), ('requested_bytes.small_pool.freed', 40673), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7162), ('active.all.current', 7129), ('active.all.freed', 33), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7160), ('active.small_pool.current', 7127), ('active.small_pool.freed', 33), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20564992), ('active_bytes.all.current', 20509696), ('active_bytes.all.freed', 55296), ('active_bytes.all.peak', 20510720), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3755008), ('active_bytes.small_pool.current', 3699712), ('active_bytes.small_pool.freed', 55296), ('active_bytes.small_pool.peak', 3700736), ('allocated_bytes.all.allocated', 20564992), ('allocated_bytes.all.current', 20509696), ('allocated_bytes.all.freed', 55296), ('allocated_bytes.all.peak', 20510720), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3755008), ('allocated_bytes.small_pool.current', 3699712), ('allocated_bytes.small_pool.freed', 55296), ('allocated_bytes.small_pool.peak', 3700736), ('allocation.all.allocated', 7162), ('allocation.all.current', 7129), ('allocation.all.freed', 33), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7160), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 33), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 22), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 18), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 21), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 18), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16815104), ('inactive_split_bytes.all.current', 4656128), ('inactive_split_bytes.all.freed', 12158976), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4248576), ('inactive_split_bytes.small_pool.current', 494592), ('inactive_split_bytes.small_pool.freed', 3753984), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16939334), ('requested_bytes.all.current', 16892228), ('requested_bytes.all.freed', 47106), ('requested_bytes.all.peak', 16893252), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 129350), ('requested_bytes.small_pool.current', 82244), ('requested_bytes.small_pool.freed', 47106), ('requested_bytes.small_pool.peak', 83268), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 448, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7128), ('active.all.freed', 2), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7126), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20458496), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3648512), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20458496), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3648512), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7128), ('allocation.all.freed', 2), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707328), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 545792), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16839292), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 29308), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 718, in convert_to_tensors
    tensor = as_tensor(value)
ValueError: expected sequence of length 21 at dim 1 (got 23)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 412, in call_batched
    tokenized_datasets = filtered_dataset.map(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 578, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 543, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3073, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3449, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3330, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "benchmark_suite.py", line 685, in preprocess_function_with_text_target
    model_inputs = self.tokenizer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2634, in _call_one
    return self.batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 476, in _batch_encode_plus
    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 211, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 734, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
OrderedDict([('active.all.allocated', 7130), ('active.all.current', 7123), ('active.all.freed', 7), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7128), ('active.small_pool.current', 7121), ('active.small_pool.freed', 7), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20459520), ('active_bytes.all.current', 20455936), ('active_bytes.all.freed', 3584), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649536), ('active_bytes.small_pool.current', 3645952), ('active_bytes.small_pool.freed', 3584), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20459520), ('allocated_bytes.all.current', 20455936), ('allocated_bytes.all.freed', 3584), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649536), ('allocated_bytes.small_pool.current', 3645952), ('allocated_bytes.small_pool.freed', 3584), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7130), ('allocation.all.current', 7123), ('allocation.all.freed', 7), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7128), ('allocation.small_pool.current', 7121), ('allocation.small_pool.freed', 7), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 7), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 4), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 6), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 4), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16763392), ('inactive_split_bytes.all.current', 4709888), ('inactive_split_bytes.all.freed', 12053504), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4196864), ('inactive_split_bytes.small_pool.current', 548352), ('inactive_split_bytes.small_pool.freed', 3648512), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839317), ('requested_bytes.all.current', 16838468), ('requested_bytes.all.freed', 849), ('requested_bytes.all.peak', 16839296), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29333), ('requested_bytes.small_pool.current', 28484), ('requested_bytes.small_pool.freed', 849), ('requested_bytes.small_pool.peak', 29312), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 448, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7136), ('active.all.current', 7128), ('active.all.freed', 8), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7134), ('active.small_pool.current', 7126), ('active.small_pool.freed', 8), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20463104), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 4096), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3653120), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 4096), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20463104), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 4096), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3653120), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 4096), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7136), ('allocation.all.current', 7128), ('allocation.all.freed', 8), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7134), ('allocation.small_pool.current', 7126), ('allocation.small_pool.freed', 8), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 8), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 6), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 7), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 6), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16763904), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12057088), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4197376), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3652096), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16842229), ('requested_bytes.all.current', 16841316), ('requested_bytes.all.freed', 913), ('requested_bytes.all.peak', 16841316), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 32245), ('requested_bytes.small_pool.current', 31332), ('requested_bytes.small_pool.freed', 913), ('requested_bytes.small_pool.peak', 31332), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 718, in convert_to_tensors
    tensor = as_tensor(value)
ValueError: expected sequence of length 64 at dim 1 (got 58)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 412, in call_batched
    tokenized_datasets = filtered_dataset.map(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 578, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 543, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3073, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3449, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3330, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "benchmark_suite.py", line 685, in preprocess_function_with_text_target
    model_inputs = self.tokenizer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2634, in _call_one
    return self.batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 476, in _batch_encode_plus
    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 211, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 734, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
OrderedDict([('active.all.allocated', 7136), ('active.all.current', 7123), ('active.all.freed', 13), ('active.all.peak', 7129), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7134), ('active.small_pool.current', 7121), ('active.small_pool.freed', 13), ('active.small_pool.peak', 7127), ('active_bytes.all.allocated', 20463104), ('active_bytes.all.current', 20455936), ('active_bytes.all.freed', 7168), ('active_bytes.all.peak', 20459008), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3653120), ('active_bytes.small_pool.current', 3645952), ('active_bytes.small_pool.freed', 7168), ('active_bytes.small_pool.peak', 3649024), ('allocated_bytes.all.allocated', 20463104), ('allocated_bytes.all.current', 20455936), ('allocated_bytes.all.freed', 7168), ('allocated_bytes.all.peak', 20459008), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3653120), ('allocated_bytes.small_pool.current', 3645952), ('allocated_bytes.small_pool.freed', 7168), ('allocated_bytes.small_pool.peak', 3649024), ('allocation.all.allocated', 7136), ('allocation.all.current', 7123), ('allocation.all.freed', 13), ('allocation.all.peak', 7129), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7134), ('allocation.small_pool.current', 7121), ('allocation.small_pool.freed', 13), ('allocation.small_pool.peak', 7127), ('inactive_split.all.allocated', 11), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 10), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16766976), ('inactive_split_bytes.all.current', 4709888), ('inactive_split_bytes.all.freed', 12057088), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4200448), ('inactive_split_bytes.small_pool.current', 548352), ('inactive_split_bytes.small_pool.freed', 3652096), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16842229), ('requested_bytes.all.current', 16838468), ('requested_bytes.all.freed', 3761), ('requested_bytes.all.peak', 16841316), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 32245), ('requested_bytes.small_pool.current', 28484), ('requested_bytes.small_pool.freed', 3761), ('requested_bytes.small_pool.peak', 31332), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 930, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 448, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7129), ('active.all.current', 7127), ('active.all.freed', 2), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7127), ('active.small_pool.current', 7125), ('active.small_pool.freed', 2), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459008), ('active_bytes.all.current', 20457984), ('active_bytes.all.freed', 1024), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649024), ('active_bytes.small_pool.current', 3648000), ('active_bytes.small_pool.freed', 1024), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459008), ('allocated_bytes.all.current', 20457984), ('allocated_bytes.all.freed', 1024), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649024), ('allocated_bytes.small_pool.current', 3648000), ('allocated_bytes.small_pool.freed', 1024), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7129), ('allocation.all.current', 7127), ('allocation.all.freed', 2), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7127), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 2), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 5), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 2), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 4), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 2), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16760832), ('inactive_split_bytes.all.current', 4707840), ('inactive_split_bytes.all.freed', 12052992), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4194304), ('inactive_split_bytes.small_pool.current', 546304), ('inactive_split_bytes.small_pool.freed', 3648000), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839165), ('requested_bytes.all.current', 16839140), ('requested_bytes.all.freed', 25), ('requested_bytes.all.peak', 16839144), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29181), ('requested_bytes.small_pool.current', 29156), ('requested_bytes.small_pool.freed', 25), ('requested_bytes.small_pool.peak', 29160), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 718, in convert_to_tensors
    tensor = as_tensor(value)
ValueError: expected sequence of length 21 at dim 1 (got 23)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "benchmark_suite.py", line 930, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 412, in call_batched
    tokenized_datasets = filtered_dataset.map(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 578, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 543, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3073, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3449, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3330, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "benchmark_suite.py", line 678, in preprocess_function_with_text_target
    inputs = self.tokenizer(inputs, return_tensors="pt")
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2634, in _call_one
    return self.batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 476, in _batch_encode_plus
    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 211, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 734, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
OrderedDict([('active.all.allocated', 7129), ('active.all.current', 7123), ('active.all.freed', 6), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7127), ('active.small_pool.current', 7121), ('active.small_pool.freed', 6), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20459008), ('active_bytes.all.current', 20455936), ('active_bytes.all.freed', 3072), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3649024), ('active_bytes.small_pool.current', 3645952), ('active_bytes.small_pool.freed', 3072), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20459008), ('allocated_bytes.all.current', 20455936), ('allocated_bytes.all.freed', 3072), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3649024), ('allocated_bytes.small_pool.current', 3645952), ('allocated_bytes.small_pool.freed', 3072), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7129), ('allocation.all.current', 7123), ('allocation.all.freed', 6), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7127), ('allocation.small_pool.current', 7121), ('allocation.small_pool.freed', 6), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16762880), ('inactive_split_bytes.all.current', 4709888), ('inactive_split_bytes.all.freed', 12052992), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4196352), ('inactive_split_bytes.small_pool.current', 548352), ('inactive_split_bytes.small_pool.freed', 3648000), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839165), ('requested_bytes.all.current', 16838468), ('requested_bytes.all.freed', 697), ('requested_bytes.all.peak', 16839144), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29181), ('requested_bytes.small_pool.current', 28484), ('requested_bytes.small_pool.freed', 697), ('requested_bytes.small_pool.peak', 29160), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 930, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 448, in call_batched
    outputs = self.model.generate(**batch, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7134), ('active.all.current', 7127), ('active.all.freed', 7), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7132), ('active.small_pool.current', 7125), ('active.small_pool.freed', 7), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20461568), ('active_bytes.all.current', 20457984), ('active_bytes.all.freed', 3584), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651584), ('active_bytes.small_pool.current', 3648000), ('active_bytes.small_pool.freed', 3584), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20461568), ('allocated_bytes.all.current', 20457984), ('allocated_bytes.all.freed', 3584), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651584), ('allocated_bytes.small_pool.current', 3648000), ('allocated_bytes.small_pool.freed', 3584), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7134), ('allocation.all.current', 7127), ('allocation.all.freed', 7), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7132), ('allocation.small_pool.current', 7125), ('allocation.small_pool.freed', 7), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 7), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 5), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 6), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 5), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16763392), ('inactive_split_bytes.all.current', 4707840), ('inactive_split_bytes.all.freed', 12055552), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4196864), ('inactive_split_bytes.small_pool.current', 546304), ('inactive_split_bytes.small_pool.freed', 3650560), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16841277), ('requested_bytes.all.current', 16840516), ('requested_bytes.all.freed', 761), ('requested_bytes.all.peak', 16840516), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 31293), ('requested_bytes.small_pool.current', 30532), ('requested_bytes.small_pool.freed', 761), ('requested_bytes.small_pool.peak', 30532), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 718, in convert_to_tensors
    tensor = as_tensor(value)
ValueError: expected sequence of length 64 at dim 1 (got 58)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "benchmark_suite.py", line 930, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 412, in call_batched
    tokenized_datasets = filtered_dataset.map(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 578, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 543, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3073, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3449, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3330, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "benchmark_suite.py", line 678, in preprocess_function_with_text_target
    inputs = self.tokenizer(inputs, return_tensors="pt")
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2548, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2634, in _call_one
    return self.batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2825, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 476, in _batch_encode_plus
    return BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type=return_tensors)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 211, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 734, in convert_to_tensors
    raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
OrderedDict([('active.all.allocated', 7134), ('active.all.current', 7123), ('active.all.freed', 11), ('active.all.peak', 7128), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7132), ('active.small_pool.current', 7121), ('active.small_pool.freed', 11), ('active.small_pool.peak', 7126), ('active_bytes.all.allocated', 20461568), ('active_bytes.all.current', 20455936), ('active_bytes.all.freed', 5632), ('active_bytes.all.peak', 20458496), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651584), ('active_bytes.small_pool.current', 3645952), ('active_bytes.small_pool.freed', 5632), ('active_bytes.small_pool.peak', 3648512), ('allocated_bytes.all.allocated', 20461568), ('allocated_bytes.all.current', 20455936), ('allocated_bytes.all.freed', 5632), ('allocated_bytes.all.peak', 20458496), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651584), ('allocated_bytes.small_pool.current', 3645952), ('allocated_bytes.small_pool.freed', 5632), ('allocated_bytes.small_pool.peak', 3648512), ('allocation.all.allocated', 7134), ('allocation.all.current', 7123), ('allocation.all.freed', 11), ('allocation.all.peak', 7128), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7132), ('allocation.small_pool.current', 7121), ('allocation.small_pool.freed', 11), ('allocation.small_pool.peak', 7126), ('inactive_split.all.allocated', 9), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 6), ('inactive_split.all.peak', 4), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 8), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 6), ('inactive_split.small_pool.peak', 3), ('inactive_split_bytes.all.allocated', 16765440), ('inactive_split_bytes.all.current', 4709888), ('inactive_split_bytes.all.freed', 12055552), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4198912), ('inactive_split_bytes.small_pool.current', 548352), ('inactive_split_bytes.small_pool.freed', 3650560), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16841277), ('requested_bytes.all.current', 16838468), ('requested_bytes.all.freed', 2809), ('requested_bytes.all.peak', 16840516), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 31293), ('requested_bytes.small_pool.current', 28484), ('requested_bytes.small_pool.freed', 2809), ('requested_bytes.small_pool.peak', 30532), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7133), ('active.all.current', 7129), ('active.all.freed', 4), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7131), ('active.small_pool.current', 7127), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20461056), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20460032), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651072), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3650048), ('allocated_bytes.all.allocated', 20461056), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20460032), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651072), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3650048), ('allocation.all.allocated', 7133), ('allocation.all.current', 7129), ('allocation.all.freed', 4), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7131), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12055040), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3650048), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839507), ('requested_bytes.all.current', 16839460), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16839464), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29523), ('requested_bytes.small_pool.current', 29476), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 29480), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7129), ('active.all.freed', 14), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7127), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20502528), ('active_bytes.all.current', 20493824), ('active_bytes.all.freed', 8704), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3692544), ('active_bytes.small_pool.current', 3683840), ('active_bytes.small_pool.freed', 8704), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20502528), ('allocated_bytes.all.current', 20493824), ('allocated_bytes.all.freed', 8704), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3692544), ('allocated_bytes.small_pool.current', 3683840), ('allocated_bytes.small_pool.freed', 8704), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7143), ('allocation.all.current', 7129), ('allocation.all.freed', 14), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 12), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 11), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16768512), ('inactive_split_bytes.all.current', 4672000), ('inactive_split_bytes.all.freed', 12096512), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4201984), ('inactive_split_bytes.small_pool.current', 510464), ('inactive_split_bytes.small_pool.freed', 3691520), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16879012), ('requested_bytes.all.current', 16875716), ('requested_bytes.all.freed', 3296), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 69028), ('requested_bytes.small_pool.current', 65732), ('requested_bytes.small_pool.freed', 3296), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7152), ('active.all.current', 7129), ('active.all.freed', 23), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7150), ('active.small_pool.current', 7127), ('active.small_pool.freed', 23), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20507648), ('active_bytes.all.current', 20459520), ('active_bytes.all.freed', 48128), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3697664), ('active_bytes.small_pool.current', 3649536), ('active_bytes.small_pool.freed', 48128), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20507648), ('allocated_bytes.all.current', 20459520), ('allocated_bytes.all.freed', 48128), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3697664), ('allocated_bytes.small_pool.current', 3649536), ('allocated_bytes.small_pool.freed', 48128), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7152), ('allocation.all.current', 7129), ('allocation.all.freed', 23), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7150), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 23), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 15), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 13), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 13), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16807936), ('inactive_split_bytes.all.current', 4706304), ('inactive_split_bytes.all.freed', 12101632), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4241408), ('inactive_split_bytes.small_pool.current', 544768), ('inactive_split_bytes.small_pool.freed', 3696640), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16882501), ('requested_bytes.all.current', 16841828), ('requested_bytes.all.freed', 40673), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 72517), ('requested_bytes.small_pool.current', 31844), ('requested_bytes.small_pool.freed', 40673), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7162), ('active.all.current', 7129), ('active.all.freed', 33), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7160), ('active.small_pool.current', 7127), ('active.small_pool.freed', 33), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20564992), ('active_bytes.all.current', 20509696), ('active_bytes.all.freed', 55296), ('active_bytes.all.peak', 20510720), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3755008), ('active_bytes.small_pool.current', 3699712), ('active_bytes.small_pool.freed', 55296), ('active_bytes.small_pool.peak', 3700736), ('allocated_bytes.all.allocated', 20564992), ('allocated_bytes.all.current', 20509696), ('allocated_bytes.all.freed', 55296), ('allocated_bytes.all.peak', 20510720), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3755008), ('allocated_bytes.small_pool.current', 3699712), ('allocated_bytes.small_pool.freed', 55296), ('allocated_bytes.small_pool.peak', 3700736), ('allocation.all.allocated', 7162), ('allocation.all.current', 7129), ('allocation.all.freed', 33), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7160), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 33), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 22), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 18), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 21), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 18), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16815104), ('inactive_split_bytes.all.current', 4656128), ('inactive_split_bytes.all.freed', 12158976), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4248576), ('inactive_split_bytes.small_pool.current', 494592), ('inactive_split_bytes.small_pool.freed', 3753984), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16939334), ('requested_bytes.all.current', 16892228), ('requested_bytes.all.freed', 47106), ('requested_bytes.all.peak', 16893252), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 129350), ('requested_bytes.small_pool.current', 82244), ('requested_bytes.small_pool.freed', 47106), ('requested_bytes.small_pool.peak', 83268), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
=--
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7133), ('active.all.current', 7129), ('active.all.freed', 4), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7131), ('active.small_pool.current', 7127), ('active.small_pool.freed', 4), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20461056), ('active_bytes.all.current', 20459008), ('active_bytes.all.freed', 2048), ('active_bytes.all.peak', 20460032), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3651072), ('active_bytes.small_pool.current', 3649024), ('active_bytes.small_pool.freed', 2048), ('active_bytes.small_pool.peak', 3650048), ('allocated_bytes.all.allocated', 20461056), ('allocated_bytes.all.current', 20459008), ('allocated_bytes.all.freed', 2048), ('allocated_bytes.all.peak', 20460032), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3651072), ('allocated_bytes.small_pool.current', 3649024), ('allocated_bytes.small_pool.freed', 2048), ('allocated_bytes.small_pool.peak', 3650048), ('allocation.all.allocated', 7133), ('allocation.all.current', 7129), ('allocation.all.freed', 4), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7131), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 4), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 6), ('inactive_split.all.current', 3), ('inactive_split.all.freed', 3), ('inactive_split.all.peak', 3), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 5), ('inactive_split.small_pool.current', 2), ('inactive_split.small_pool.freed', 3), ('inactive_split.small_pool.peak', 2), ('inactive_split_bytes.all.allocated', 16761856), ('inactive_split_bytes.all.current', 4706816), ('inactive_split_bytes.all.freed', 12055040), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4195328), ('inactive_split_bytes.small_pool.current', 545280), ('inactive_split_bytes.small_pool.freed', 3650048), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16839507), ('requested_bytes.all.current', 16839460), ('requested_bytes.all.freed', 47), ('requested_bytes.all.peak', 16839464), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 29523), ('requested_bytes.small_pool.current', 29476), ('requested_bytes.small_pool.freed', 47), ('requested_bytes.small_pool.peak', 29480), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7143), ('active.all.current', 7129), ('active.all.freed', 14), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7141), ('active.small_pool.current', 7127), ('active.small_pool.freed', 14), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20502528), ('active_bytes.all.current', 20493824), ('active_bytes.all.freed', 8704), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3692544), ('active_bytes.small_pool.current', 3683840), ('active_bytes.small_pool.freed', 8704), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20502528), ('allocated_bytes.all.current', 20493824), ('allocated_bytes.all.freed', 8704), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3692544), ('allocated_bytes.small_pool.current', 3683840), ('allocated_bytes.small_pool.freed', 8704), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7143), ('allocation.all.current', 7129), ('allocation.all.freed', 14), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7141), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 14), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 12), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 8), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 11), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 8), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16768512), ('inactive_split_bytes.all.current', 4672000), ('inactive_split_bytes.all.freed', 12096512), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4201984), ('inactive_split_bytes.small_pool.current', 510464), ('inactive_split_bytes.small_pool.freed', 3691520), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16879012), ('requested_bytes.all.current', 16875716), ('requested_bytes.all.freed', 3296), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 69028), ('requested_bytes.small_pool.current', 65732), ('requested_bytes.small_pool.freed', 3296), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7152), ('active.all.current', 7129), ('active.all.freed', 23), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7150), ('active.small_pool.current', 7127), ('active.small_pool.freed', 23), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20507648), ('active_bytes.all.current', 20459520), ('active_bytes.all.freed', 48128), ('active_bytes.all.peak', 20494848), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3697664), ('active_bytes.small_pool.current', 3649536), ('active_bytes.small_pool.freed', 48128), ('active_bytes.small_pool.peak', 3684864), ('allocated_bytes.all.allocated', 20507648), ('allocated_bytes.all.current', 20459520), ('allocated_bytes.all.freed', 48128), ('allocated_bytes.all.peak', 20494848), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3697664), ('allocated_bytes.small_pool.current', 3649536), ('allocated_bytes.small_pool.freed', 48128), ('allocated_bytes.small_pool.peak', 3684864), ('allocation.all.allocated', 7152), ('allocation.all.current', 7129), ('allocation.all.freed', 23), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7150), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 23), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 15), ('inactive_split.all.current', 2), ('inactive_split.all.freed', 13), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 14), ('inactive_split.small_pool.current', 1), ('inactive_split.small_pool.freed', 13), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16807936), ('inactive_split_bytes.all.current', 4706304), ('inactive_split_bytes.all.freed', 12101632), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4241408), ('inactive_split_bytes.small_pool.current', 544768), ('inactive_split_bytes.small_pool.freed', 3696640), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16882501), ('requested_bytes.all.current', 16841828), ('requested_bytes.all.freed', 40673), ('requested_bytes.all.peak', 16876468), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 72517), ('requested_bytes.small_pool.current', 31844), ('requested_bytes.small_pool.freed', 40673), ('requested_bytes.small_pool.peak', 66484), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= flores200 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 924, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1117, in forward
    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)
OrderedDict([('active.all.allocated', 7162), ('active.all.current', 7129), ('active.all.freed', 33), ('active.all.peak', 7131), ('active.large_pool.allocated', 2), ('active.large_pool.current', 2), ('active.large_pool.freed', 0), ('active.large_pool.peak', 2), ('active.small_pool.allocated', 7160), ('active.small_pool.current', 7127), ('active.small_pool.freed', 33), ('active.small_pool.peak', 7129), ('active_bytes.all.allocated', 20564992), ('active_bytes.all.current', 20509696), ('active_bytes.all.freed', 55296), ('active_bytes.all.peak', 20510720), ('active_bytes.large_pool.allocated', 16809984), ('active_bytes.large_pool.current', 16809984), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 16809984), ('active_bytes.small_pool.allocated', 3755008), ('active_bytes.small_pool.current', 3699712), ('active_bytes.small_pool.freed', 55296), ('active_bytes.small_pool.peak', 3700736), ('allocated_bytes.all.allocated', 20564992), ('allocated_bytes.all.current', 20509696), ('allocated_bytes.all.freed', 55296), ('allocated_bytes.all.peak', 20510720), ('allocated_bytes.large_pool.allocated', 16809984), ('allocated_bytes.large_pool.current', 16809984), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 16809984), ('allocated_bytes.small_pool.allocated', 3755008), ('allocated_bytes.small_pool.current', 3699712), ('allocated_bytes.small_pool.freed', 55296), ('allocated_bytes.small_pool.peak', 3700736), ('allocation.all.allocated', 7162), ('allocation.all.current', 7129), ('allocation.all.freed', 33), ('allocation.all.peak', 7131), ('allocation.large_pool.allocated', 2), ('allocation.large_pool.current', 2), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 2), ('allocation.small_pool.allocated', 7160), ('allocation.small_pool.current', 7127), ('allocation.small_pool.freed', 33), ('allocation.small_pool.peak', 7129), ('inactive_split.all.allocated', 22), ('inactive_split.all.current', 4), ('inactive_split.all.freed', 18), ('inactive_split.all.peak', 5), ('inactive_split.large_pool.allocated', 1), ('inactive_split.large_pool.current', 1), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 1), ('inactive_split.small_pool.allocated', 21), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 18), ('inactive_split.small_pool.peak', 4), ('inactive_split_bytes.all.allocated', 16815104), ('inactive_split_bytes.all.current', 4656128), ('inactive_split_bytes.all.freed', 12158976), ('inactive_split_bytes.all.peak', 14663168), ('inactive_split_bytes.large_pool.allocated', 12566528), ('inactive_split_bytes.large_pool.current', 4161536), ('inactive_split_bytes.large_pool.freed', 8404992), ('inactive_split_bytes.large_pool.peak', 12566528), ('inactive_split_bytes.small_pool.allocated', 4248576), ('inactive_split_bytes.small_pool.current', 494592), ('inactive_split_bytes.small_pool.freed', 3753984), ('inactive_split_bytes.small_pool.peak', 2096640), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 16939334), ('requested_bytes.all.current', 16892228), ('requested_bytes.all.freed', 47106), ('requested_bytes.all.peak', 16893252), ('requested_bytes.large_pool.allocated', 16809984), ('requested_bytes.large_pool.current', 16809984), ('requested_bytes.large_pool.freed', 0), ('requested_bytes.large_pool.peak', 16809984), ('requested_bytes.small_pool.allocated', 129350), ('requested_bytes.small_pool.current', 82244), ('requested_bytes.small_pool.freed', 47106), ('requested_bytes.small_pool.peak', 83268), ('reserved_bytes.all.allocated', 25165824), ('reserved_bytes.all.current', 25165824), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 25165824), ('reserved_bytes.large_pool.allocated', 20971520), ('reserved_bytes.large_pool.current', 20971520), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 20971520), ('reserved_bytes.small_pool.allocated', 4194304), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 4194304), ('segment.all.allocated', 3), ('segment.all.current', 3), ('segment.all.freed', 0), ('segment.all.peak', 3), ('segment.large_pool.allocated', 1), ('segment.large_pool.current', 1), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 1), ('segment.small_pool.allocated', 2), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 2)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1724, in forward
    outputs = self.model(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1606, in forward
    decoder_outputs = self.decoder(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1442, in forward
    layer_outputs = decoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 832, in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 276, in forward
    expert_output = expert(masked_hidden_states[idx,
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 413, in forward
    hidden_states = self.fc1(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.69 GiB total capacity; 22.89 GiB already allocated; 1.12 MiB free; 23.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 181840), ('active.all.current', 469), ('active.all.freed', 181371), ('active.all.peak', 476), ('active.large_pool.allocated', 8842), ('active.large_pool.current', 345), ('active.large_pool.freed', 8497), ('active.large_pool.peak', 345), ('active.small_pool.allocated', 172998), ('active.small_pool.current', 124), ('active.small_pool.freed', 172874), ('active.small_pool.peak', 276), ('active_bytes.all.allocated', 80992036352), ('active_bytes.all.current', 24576654336), ('active_bytes.all.freed', 56415382016), ('active_bytes.all.peak', 24576785408), ('active_bytes.large_pool.allocated', 47978063872), ('active_bytes.large_pool.current', 24509658112), ('active_bytes.large_pool.freed', 23468405760), ('active_bytes.large_pool.peak', 24509658112), ('active_bytes.small_pool.allocated', 33013972480), ('active_bytes.small_pool.current', 66996224), ('active_bytes.small_pool.freed', 32946976256), ('active_bytes.small_pool.peak', 216212992), ('allocated_bytes.all.allocated', 80992036352), ('allocated_bytes.all.current', 24576654336), ('allocated_bytes.all.freed', 56415382016), ('allocated_bytes.all.peak', 24576785408), ('allocated_bytes.large_pool.allocated', 47978063872), ('allocated_bytes.large_pool.current', 24509658112), ('allocated_bytes.large_pool.freed', 23468405760), ('allocated_bytes.large_pool.peak', 24509658112), ('allocated_bytes.small_pool.allocated', 33013972480), ('allocated_bytes.small_pool.current', 66996224), ('allocated_bytes.small_pool.freed', 32946976256), ('allocated_bytes.small_pool.peak', 216212992), ('allocation.all.allocated', 181840), ('allocation.all.current', 469), ('allocation.all.freed', 181371), ('allocation.all.peak', 476), ('allocation.large_pool.allocated', 8842), ('allocation.large_pool.current', 345), ('allocation.large_pool.freed', 8497), ('allocation.large_pool.peak', 345), ('allocation.small_pool.allocated', 172998), ('allocation.small_pool.current', 124), ('allocation.small_pool.freed', 172874), ('allocation.small_pool.peak', 276), ('inactive_split.all.allocated', 125190), ('inactive_split.all.current', 247), ('inactive_split.all.freed', 124943), ('inactive_split.all.peak', 322), ('inactive_split.large_pool.allocated', 4781), ('inactive_split.large_pool.current', 216), ('inactive_split.large_pool.freed', 4565), ('inactive_split.large_pool.peak', 216), ('inactive_split.small_pool.allocated', 120409), ('inactive_split.small_pool.current', 31), ('inactive_split.small_pool.freed', 120378), ('inactive_split.small_pool.peak', 148), ('inactive_split_bytes.all.allocated', 67838254080), ('inactive_split_bytes.all.current', 482214912), ('inactive_split_bytes.all.freed', 67356039168), ('inactive_split_bytes.all.peak', 583476224), ('inactive_split_bytes.large_pool.allocated', 34353555456), ('inactive_split_bytes.large_pool.current', 480005120), ('inactive_split_bytes.large_pool.freed', 33873550336), ('inactive_split_bytes.large_pool.peak', 548031488), ('inactive_split_bytes.small_pool.allocated', 33484698624), ('inactive_split_bytes.small_pool.current', 2209792), ('inactive_split_bytes.small_pool.freed', 33482488832), ('inactive_split_bytes.small_pool.peak', 96788992), ('max_split_size', -1), ('num_alloc_retries', 3), ('num_ooms', 1), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 79330939498), ('requested_bytes.all.current', 24562378969), ('requested_bytes.all.freed', 54768560529), ('requested_bytes.all.peak', 24562510041), ('requested_bytes.large_pool.allocated', 46344529352), ('requested_bytes.large_pool.current', 24495389120), ('requested_bytes.large_pool.freed', 21849140232), ('requested_bytes.large_pool.peak', 24495389120), ('requested_bytes.small_pool.allocated', 32986410146), ('requested_bytes.small_pool.current', 66989849), ('requested_bytes.small_pool.freed', 32919420297), ('requested_bytes.small_pool.peak', 216206778), ('reserved_bytes.all.allocated', 25253904384), ('reserved_bytes.all.current', 25058869248), ('reserved_bytes.all.freed', 195035136), ('reserved_bytes.all.peak', 25058869248), ('reserved_bytes.large_pool.allocated', 24989663232), ('reserved_bytes.large_pool.current', 24989663232), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 24989663232), ('reserved_bytes.small_pool.allocated', 264241152), ('reserved_bytes.small_pool.current', 69206016), ('reserved_bytes.small_pool.freed', 195035136), ('reserved_bytes.small_pool.peak', 262144000), ('segment.all.allocated', 354), ('segment.all.current', 261), ('segment.all.freed', 93), ('segment.all.peak', 343), ('segment.large_pool.allocated', 228), ('segment.large_pool.current', 228), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 228), ('segment.small_pool.allocated', 126), ('segment.small_pool.current', 33), ('segment.small_pool.freed', 93), ('segment.small_pool.peak', 125)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 16, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 926, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1322, in generate
    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 638, in _prepare_encoder_decoder_kwargs_for_generation
    model_kwargs["encoder_outputs"]: ModelOutput = encoder(**encoder_kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1165, in forward
    layer_outputs = encoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 701, in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 238, in forward
    masked_hidden_states = torch.einsum("bm,be->ebm", hidden_states,
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/functional.py", line 378, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 752.00 MiB (GPU 0; 23.69 GiB total capacity; 22.35 GiB already allocated; 645.12 MiB free; 22.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 182014), ('active.all.current', 213), ('active.all.freed', 181801), ('active.all.peak', 476), ('active.large_pool.allocated', 8922), ('active.large_pool.current', 203), ('active.large_pool.freed', 8719), ('active.large_pool.peak', 345), ('active.small_pool.allocated', 173092), ('active.small_pool.current', 10), ('active.small_pool.freed', 173082), ('active.small_pool.peak', 276), ('active_bytes.all.allocated', 81589850624), ('active_bytes.all.current', 24002364928), ('active_bytes.all.freed', 57587485696), ('active_bytes.all.peak', 24576785408), ('active_bytes.large_pool.allocated', 48541018112), ('active_bytes.large_pool.current', 24000913408), ('active_bytes.large_pool.freed', 24540104704), ('active_bytes.large_pool.peak', 24509658112), ('active_bytes.small_pool.allocated', 33048832512), ('active_bytes.small_pool.current', 1451520), ('active_bytes.small_pool.freed', 33047380992), ('active_bytes.small_pool.peak', 216212992), ('allocated_bytes.all.allocated', 81589850624), ('allocated_bytes.all.current', 24002364928), ('allocated_bytes.all.freed', 57587485696), ('allocated_bytes.all.peak', 24576785408), ('allocated_bytes.large_pool.allocated', 48541018112), ('allocated_bytes.large_pool.current', 24000913408), ('allocated_bytes.large_pool.freed', 24540104704), ('allocated_bytes.large_pool.peak', 24509658112), ('allocated_bytes.small_pool.allocated', 33048832512), ('allocated_bytes.small_pool.current', 1451520), ('allocated_bytes.small_pool.freed', 33047380992), ('allocated_bytes.small_pool.peak', 216212992), ('allocation.all.allocated', 182014), ('allocation.all.current', 213), ('allocation.all.freed', 181801), ('allocation.all.peak', 476), ('allocation.large_pool.allocated', 8922), ('allocation.large_pool.current', 203), ('allocation.large_pool.freed', 8719), ('allocation.large_pool.peak', 345), ('allocation.small_pool.allocated', 173092), ('allocation.small_pool.current', 10), ('allocation.small_pool.freed', 173082), ('allocation.small_pool.peak', 276), ('inactive_split.all.allocated', 125312), ('inactive_split.all.current', 190), ('inactive_split.all.freed', 125122), ('inactive_split.all.peak', 322), ('inactive_split.large_pool.allocated', 4835), ('inactive_split.large_pool.current', 184), ('inactive_split.large_pool.freed', 4651), ('inactive_split.large_pool.peak', 216), ('inactive_split.small_pool.allocated', 120477), ('inactive_split.small_pool.current', 6), ('inactive_split.small_pool.freed', 120471), ('inactive_split.small_pool.peak', 148), ('inactive_split_bytes.all.allocated', 68839804928), ('inactive_split_bytes.all.current', 381221376), ('inactive_split_bytes.all.freed', 68458583552), ('inactive_split_bytes.all.peak', 583476224), ('inactive_split_bytes.large_pool.allocated', 35273895424), ('inactive_split_bytes.large_pool.current', 378478592), ('inactive_split_bytes.large_pool.freed', 34895416832), ('inactive_split_bytes.large_pool.peak', 548031488), ('inactive_split_bytes.small_pool.allocated', 33565909504), ('inactive_split_bytes.small_pool.current', 2742784), ('inactive_split_bytes.small_pool.freed', 33563166720), ('inactive_split_bytes.small_pool.peak', 96788992), ('max_split_size', -1), ('num_alloc_retries', 4), ('num_ooms', 2), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 79922729899), ('requested_bytes.all.current', 23989912256), ('requested_bytes.all.freed', 55932817643), ('requested_bytes.all.peak', 24562510041), ('requested_bytes.large_pool.allocated', 46901466568), ('requested_bytes.large_pool.current', 23988461568), ('requested_bytes.large_pool.freed', 22913005000), ('requested_bytes.large_pool.peak', 24495389120), ('requested_bytes.small_pool.allocated', 33021263331), ('requested_bytes.small_pool.current', 1450688), ('requested_bytes.small_pool.freed', 33019812643), ('requested_bytes.small_pool.peak', 216206778), ('reserved_bytes.all.allocated', 25373442048), ('reserved_bytes.all.current', 24383586304), ('reserved_bytes.all.freed', 989855744), ('reserved_bytes.all.peak', 25058869248), ('reserved_bytes.large_pool.allocated', 25102909440), ('reserved_bytes.large_pool.current', 24379392000), ('reserved_bytes.large_pool.freed', 723517440), ('reserved_bytes.large_pool.peak', 24989663232), ('reserved_bytes.small_pool.allocated', 270532608), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 266338304), ('reserved_bytes.small_pool.peak', 262144000), ('segment.all.allocated', 362), ('segment.all.current', 201), ('segment.all.freed', 161), ('segment.all.peak', 343), ('segment.large_pool.allocated', 233), ('segment.large_pool.current', 199), ('segment.large_pool.freed', 34), ('segment.large_pool.peak', 228), ('segment.small_pool.allocated', 129), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 127), ('segment.small_pool.peak', 125)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 4, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 927, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1724, in forward
    outputs = self.model(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1606, in forward
    decoder_outputs = self.decoder(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1442, in forward
    layer_outputs = decoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 832, in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 276, in forward
    expert_output = expert(masked_hidden_states[idx,
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 413, in forward
    hidden_states = self.fc1(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.69 GiB total capacity; 22.89 GiB already allocated; 1.12 MiB free; 23.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 181840), ('active.all.current', 469), ('active.all.freed', 181371), ('active.all.peak', 476), ('active.large_pool.allocated', 8842), ('active.large_pool.current', 345), ('active.large_pool.freed', 8497), ('active.large_pool.peak', 345), ('active.small_pool.allocated', 172998), ('active.small_pool.current', 124), ('active.small_pool.freed', 172874), ('active.small_pool.peak', 276), ('active_bytes.all.allocated', 80992036352), ('active_bytes.all.current', 24576654336), ('active_bytes.all.freed', 56415382016), ('active_bytes.all.peak', 24576785408), ('active_bytes.large_pool.allocated', 47978063872), ('active_bytes.large_pool.current', 24509658112), ('active_bytes.large_pool.freed', 23468405760), ('active_bytes.large_pool.peak', 24509658112), ('active_bytes.small_pool.allocated', 33013972480), ('active_bytes.small_pool.current', 66996224), ('active_bytes.small_pool.freed', 32946976256), ('active_bytes.small_pool.peak', 216212992), ('allocated_bytes.all.allocated', 80992036352), ('allocated_bytes.all.current', 24576654336), ('allocated_bytes.all.freed', 56415382016), ('allocated_bytes.all.peak', 24576785408), ('allocated_bytes.large_pool.allocated', 47978063872), ('allocated_bytes.large_pool.current', 24509658112), ('allocated_bytes.large_pool.freed', 23468405760), ('allocated_bytes.large_pool.peak', 24509658112), ('allocated_bytes.small_pool.allocated', 33013972480), ('allocated_bytes.small_pool.current', 66996224), ('allocated_bytes.small_pool.freed', 32946976256), ('allocated_bytes.small_pool.peak', 216212992), ('allocation.all.allocated', 181840), ('allocation.all.current', 469), ('allocation.all.freed', 181371), ('allocation.all.peak', 476), ('allocation.large_pool.allocated', 8842), ('allocation.large_pool.current', 345), ('allocation.large_pool.freed', 8497), ('allocation.large_pool.peak', 345), ('allocation.small_pool.allocated', 172998), ('allocation.small_pool.current', 124), ('allocation.small_pool.freed', 172874), ('allocation.small_pool.peak', 276), ('inactive_split.all.allocated', 125190), ('inactive_split.all.current', 247), ('inactive_split.all.freed', 124943), ('inactive_split.all.peak', 322), ('inactive_split.large_pool.allocated', 4781), ('inactive_split.large_pool.current', 216), ('inactive_split.large_pool.freed', 4565), ('inactive_split.large_pool.peak', 216), ('inactive_split.small_pool.allocated', 120409), ('inactive_split.small_pool.current', 31), ('inactive_split.small_pool.freed', 120378), ('inactive_split.small_pool.peak', 148), ('inactive_split_bytes.all.allocated', 67838254080), ('inactive_split_bytes.all.current', 482214912), ('inactive_split_bytes.all.freed', 67356039168), ('inactive_split_bytes.all.peak', 583476224), ('inactive_split_bytes.large_pool.allocated', 34353555456), ('inactive_split_bytes.large_pool.current', 480005120), ('inactive_split_bytes.large_pool.freed', 33873550336), ('inactive_split_bytes.large_pool.peak', 548031488), ('inactive_split_bytes.small_pool.allocated', 33484698624), ('inactive_split_bytes.small_pool.current', 2209792), ('inactive_split_bytes.small_pool.freed', 33482488832), ('inactive_split_bytes.small_pool.peak', 96788992), ('max_split_size', -1), ('num_alloc_retries', 3), ('num_ooms', 1), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 79330939498), ('requested_bytes.all.current', 24562378969), ('requested_bytes.all.freed', 54768560529), ('requested_bytes.all.peak', 24562510041), ('requested_bytes.large_pool.allocated', 46344529352), ('requested_bytes.large_pool.current', 24495389120), ('requested_bytes.large_pool.freed', 21849140232), ('requested_bytes.large_pool.peak', 24495389120), ('requested_bytes.small_pool.allocated', 32986410146), ('requested_bytes.small_pool.current', 66989849), ('requested_bytes.small_pool.freed', 32919420297), ('requested_bytes.small_pool.peak', 216206778), ('reserved_bytes.all.allocated', 25253904384), ('reserved_bytes.all.current', 25058869248), ('reserved_bytes.all.freed', 195035136), ('reserved_bytes.all.peak', 25058869248), ('reserved_bytes.large_pool.allocated', 24989663232), ('reserved_bytes.large_pool.current', 24989663232), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 24989663232), ('reserved_bytes.small_pool.allocated', 264241152), ('reserved_bytes.small_pool.current', 69206016), ('reserved_bytes.small_pool.freed', 195035136), ('reserved_bytes.small_pool.peak', 262144000), ('segment.all.allocated', 354), ('segment.all.current', 261), ('segment.all.freed', 93), ('segment.all.peak', 343), ('segment.large_pool.allocated', 228), ('segment.large_pool.current', 228), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 228), ('segment.small_pool.allocated', 126), ('segment.small_pool.current', 33), ('segment.small_pool.freed', 93), ('segment.small_pool.peak', 125)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 927, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1724, in forward
    outputs = self.model(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1606, in forward
    decoder_outputs = self.decoder(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "benchmark_suite.py", line 99, in timer_forward
    result = orig_forward(cls, *args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1442, in forward
    layer_outputs = decoder_layer(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 832, in forward
    hidden_states, router_states = self.ffn(hidden_states, attention_mask)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 276, in forward
    expert_output = expert(masked_hidden_states[idx,
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 413, in forward
    hidden_states = self.fc1(hidden_states)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.69 GiB total capacity; 22.89 GiB already allocated; 1.12 MiB free; 23.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 181840), ('active.all.current', 469), ('active.all.freed', 181371), ('active.all.peak', 476), ('active.large_pool.allocated', 8842), ('active.large_pool.current', 345), ('active.large_pool.freed', 8497), ('active.large_pool.peak', 345), ('active.small_pool.allocated', 172998), ('active.small_pool.current', 124), ('active.small_pool.freed', 172874), ('active.small_pool.peak', 276), ('active_bytes.all.allocated', 80992036352), ('active_bytes.all.current', 24576654336), ('active_bytes.all.freed', 56415382016), ('active_bytes.all.peak', 24576785408), ('active_bytes.large_pool.allocated', 47978063872), ('active_bytes.large_pool.current', 24509658112), ('active_bytes.large_pool.freed', 23468405760), ('active_bytes.large_pool.peak', 24509658112), ('active_bytes.small_pool.allocated', 33013972480), ('active_bytes.small_pool.current', 66996224), ('active_bytes.small_pool.freed', 32946976256), ('active_bytes.small_pool.peak', 216212992), ('allocated_bytes.all.allocated', 80992036352), ('allocated_bytes.all.current', 24576654336), ('allocated_bytes.all.freed', 56415382016), ('allocated_bytes.all.peak', 24576785408), ('allocated_bytes.large_pool.allocated', 47978063872), ('allocated_bytes.large_pool.current', 24509658112), ('allocated_bytes.large_pool.freed', 23468405760), ('allocated_bytes.large_pool.peak', 24509658112), ('allocated_bytes.small_pool.allocated', 33013972480), ('allocated_bytes.small_pool.current', 66996224), ('allocated_bytes.small_pool.freed', 32946976256), ('allocated_bytes.small_pool.peak', 216212992), ('allocation.all.allocated', 181840), ('allocation.all.current', 469), ('allocation.all.freed', 181371), ('allocation.all.peak', 476), ('allocation.large_pool.allocated', 8842), ('allocation.large_pool.current', 345), ('allocation.large_pool.freed', 8497), ('allocation.large_pool.peak', 345), ('allocation.small_pool.allocated', 172998), ('allocation.small_pool.current', 124), ('allocation.small_pool.freed', 172874), ('allocation.small_pool.peak', 276), ('inactive_split.all.allocated', 125190), ('inactive_split.all.current', 247), ('inactive_split.all.freed', 124943), ('inactive_split.all.peak', 322), ('inactive_split.large_pool.allocated', 4781), ('inactive_split.large_pool.current', 216), ('inactive_split.large_pool.freed', 4565), ('inactive_split.large_pool.peak', 216), ('inactive_split.small_pool.allocated', 120409), ('inactive_split.small_pool.current', 31), ('inactive_split.small_pool.freed', 120378), ('inactive_split.small_pool.peak', 148), ('inactive_split_bytes.all.allocated', 67838254080), ('inactive_split_bytes.all.current', 482214912), ('inactive_split_bytes.all.freed', 67356039168), ('inactive_split_bytes.all.peak', 583476224), ('inactive_split_bytes.large_pool.allocated', 34353555456), ('inactive_split_bytes.large_pool.current', 480005120), ('inactive_split_bytes.large_pool.freed', 33873550336), ('inactive_split_bytes.large_pool.peak', 548031488), ('inactive_split_bytes.small_pool.allocated', 33484698624), ('inactive_split_bytes.small_pool.current', 2209792), ('inactive_split_bytes.small_pool.freed', 33482488832), ('inactive_split_bytes.small_pool.peak', 96788992), ('max_split_size', -1), ('num_alloc_retries', 3), ('num_ooms', 1), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 79330939498), ('requested_bytes.all.current', 24562378969), ('requested_bytes.all.freed', 54768560529), ('requested_bytes.all.peak', 24562510041), ('requested_bytes.large_pool.allocated', 46344529352), ('requested_bytes.large_pool.current', 24495389120), ('requested_bytes.large_pool.freed', 21849140232), ('requested_bytes.large_pool.peak', 24495389120), ('requested_bytes.small_pool.allocated', 32986410146), ('requested_bytes.small_pool.current', 66989849), ('requested_bytes.small_pool.freed', 32919420297), ('requested_bytes.small_pool.peak', 216206778), ('reserved_bytes.all.allocated', 25253904384), ('reserved_bytes.all.current', 25058869248), ('reserved_bytes.all.freed', 195035136), ('reserved_bytes.all.peak', 25058869248), ('reserved_bytes.large_pool.allocated', 24989663232), ('reserved_bytes.large_pool.current', 24989663232), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 24989663232), ('reserved_bytes.small_pool.allocated', 264241152), ('reserved_bytes.small_pool.current', 69206016), ('reserved_bytes.small_pool.freed', 195035136), ('reserved_bytes.small_pool.peak', 262144000), ('segment.all.allocated', 354), ('segment.all.current', 261), ('segment.all.freed', 93), ('segment.all.peak', 343), ('segment.large_pool.allocated', 228), ('segment.large_pool.current', 228), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 228), ('segment.small_pool.allocated', 126), ('segment.small_pool.current', 33), ('segment.small_pool.freed', 93), ('segment.small_pool.peak', 125)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 927, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    result = hook(self, args)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 1220, in _pre_forward_module_hook
    param.data = param.data.to("cuda:0")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 23.69 GiB total capacity; 21.28 GiB already allocated; 1.64 GiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 7036), ('active.all.current', 311), ('active.all.freed', 6725), ('active.all.peak', 329), ('active.large_pool.allocated', 329), ('active.large_pool.current', 204), ('active.large_pool.freed', 125), ('active.large_pool.peak', 205), ('active.small_pool.allocated', 6707), ('active.small_pool.current', 107), ('active.small_pool.freed', 6600), ('active.small_pool.peak', 125), ('active_bytes.all.allocated', 39138367488), ('active_bytes.all.current', 22845870080), ('active_bytes.all.freed', 16292497408), ('active_bytes.all.peak', 22910808576), ('active_bytes.large_pool.allocated', 38145318912), ('active_bytes.large_pool.current', 22810509312), ('active_bytes.large_pool.freed', 15334809600), ('active_bytes.large_pool.peak', 22875594752), ('active_bytes.small_pool.allocated', 993048576), ('active_bytes.small_pool.current', 35360768), ('active_bytes.small_pool.freed', 957687808), ('active_bytes.small_pool.peak', 36761600), ('allocated_bytes.all.allocated', 39138367488), ('allocated_bytes.all.current', 22845870080), ('allocated_bytes.all.freed', 16292497408), ('allocated_bytes.all.peak', 22910808576), ('allocated_bytes.large_pool.allocated', 38145318912), ('allocated_bytes.large_pool.current', 22810509312), ('allocated_bytes.large_pool.freed', 15334809600), ('allocated_bytes.large_pool.peak', 22875594752), ('allocated_bytes.small_pool.allocated', 993048576), ('allocated_bytes.small_pool.current', 35360768), ('allocated_bytes.small_pool.freed', 957687808), ('allocated_bytes.small_pool.peak', 36761600), ('allocation.all.allocated', 7036), ('allocation.all.current', 311), ('allocation.all.freed', 6725), ('allocation.all.peak', 329), ('allocation.large_pool.allocated', 329), ('allocation.large_pool.current', 204), ('allocation.large_pool.freed', 125), ('allocation.large_pool.peak', 205), ('allocation.small_pool.allocated', 6707), ('allocation.small_pool.current', 107), ('allocation.small_pool.freed', 6600), ('allocation.small_pool.peak', 125), ('inactive_split.all.allocated', 3209), ('inactive_split.all.current', 179), ('inactive_split.all.freed', 3030), ('inactive_split.all.peak', 184), ('inactive_split.large_pool.allocated', 283), ('inactive_split.large_pool.current', 169), ('inactive_split.large_pool.freed', 114), ('inactive_split.large_pool.peak', 176), ('inactive_split.small_pool.allocated', 2926), ('inactive_split.small_pool.current', 10), ('inactive_split.small_pool.freed', 2916), ('inactive_split.small_pool.peak', 10), ('inactive_split_bytes.all.allocated', 3156116992), ('inactive_split_bytes.all.current', 426225664), ('inactive_split_bytes.all.freed', 2729891328), ('inactive_split_bytes.all.peak', 442265600), ('inactive_split_bytes.large_pool.allocated', 2159403008), ('inactive_split_bytes.large_pool.current', 423837696), ('inactive_split_bytes.large_pool.freed', 1735565312), ('inactive_split_bytes.large_pool.peak', 439623680), ('inactive_split_bytes.small_pool.allocated', 996713984), ('inactive_split_bytes.small_pool.current', 2387968), ('inactive_split_bytes.small_pool.freed', 994326016), ('inactive_split_bytes.small_pool.peak', 3190272), ('max_split_size', -1), ('num_alloc_retries', 1), ('num_ooms', 1), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 39118505052), ('requested_bytes.all.current', 22833988793), ('requested_bytes.all.freed', 16284516259), ('requested_bytes.all.peak', 22899861001), ('requested_bytes.large_pool.allocated', 38127149056), ('requested_bytes.large_pool.current', 22798630912), ('requested_bytes.large_pool.freed', 15328518144), ('requested_bytes.large_pool.peak', 22864650240), ('requested_bytes.small_pool.allocated', 991355996), ('requested_bytes.small_pool.current', 35357881), ('requested_bytes.small_pool.freed', 955998115), ('requested_bytes.small_pool.peak', 36757561), ('reserved_bytes.all.allocated', 23295164416), ('reserved_bytes.all.current', 23272095744), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 23295164416), ('reserved_bytes.large_pool.allocated', 23257415680), ('reserved_bytes.large_pool.current', 23234347008), ('reserved_bytes.large_pool.freed', 23068672), ('reserved_bytes.large_pool.peak', 23257415680), ('reserved_bytes.small_pool.allocated', 37748736), ('reserved_bytes.small_pool.current', 37748736), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 201), ('segment.all.current', 200), ('segment.all.freed', 1), ('segment.all.peak', 201), ('segment.large_pool.allocated', 183), ('segment.large_pool.current', 182), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 183), ('segment.small_pool.allocated', 18), ('segment.small_pool.current', 18), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: all, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 927, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 447, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    result = hook(self, args)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 1220, in _pre_forward_module_hook
    param.data = param.data.to("cuda:0")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 23.69 GiB total capacity; 21.34 GiB already allocated; 1.66 GiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 16576), ('active.all.current', 311), ('active.all.freed', 16265), ('active.all.peak', 330), ('active.large_pool.allocated', 778), ('active.large_pool.current', 253), ('active.large_pool.freed', 525), ('active.large_pool.peak', 254), ('active.small_pool.allocated', 15798), ('active.small_pool.current', 58), ('active.small_pool.freed', 15740), ('active.small_pool.peak', 125), ('active_bytes.all.allocated', 81817642496), ('active_bytes.all.current', 22912774656), ('active_bytes.all.freed', 58904867840), ('active_bytes.all.peak', 22922718720), ('active_bytes.large_pool.allocated', 79565242368), ('active_bytes.large_pool.current', 22909526016), ('active_bytes.large_pool.freed', 56655716352), ('active_bytes.large_pool.peak', 22917914624), ('active_bytes.small_pool.allocated', 2252400128), ('active_bytes.small_pool.current', 3248640), ('active_bytes.small_pool.freed', 2249151488), ('active_bytes.small_pool.peak', 36761600), ('allocated_bytes.all.allocated', 81817642496), ('allocated_bytes.all.current', 22912774656), ('allocated_bytes.all.freed', 58904867840), ('allocated_bytes.all.peak', 22922718720), ('allocated_bytes.large_pool.allocated', 79565242368), ('allocated_bytes.large_pool.current', 22909526016), ('allocated_bytes.large_pool.freed', 56655716352), ('allocated_bytes.large_pool.peak', 22917914624), ('allocated_bytes.small_pool.allocated', 2252400128), ('allocated_bytes.small_pool.current', 3248640), ('allocated_bytes.small_pool.freed', 2249151488), ('allocated_bytes.small_pool.peak', 36761600), ('allocation.all.allocated', 16576), ('allocation.all.current', 311), ('allocation.all.freed', 16265), ('allocation.all.peak', 330), ('allocation.large_pool.allocated', 778), ('allocation.large_pool.current', 253), ('allocation.large_pool.freed', 525), ('allocation.large_pool.peak', 254), ('allocation.small_pool.allocated', 15798), ('allocation.small_pool.current', 58), ('allocation.small_pool.freed', 15740), ('allocation.small_pool.peak', 125), ('inactive_split.all.allocated', 6744), ('inactive_split.all.current', 128), ('inactive_split.all.freed', 6616), ('inactive_split.all.peak', 184), ('inactive_split.large_pool.allocated', 671), ('inactive_split.large_pool.current', 120), ('inactive_split.large_pool.freed', 551), ('inactive_split.large_pool.peak', 176), ('inactive_split.small_pool.allocated', 6073), ('inactive_split.small_pool.current', 8), ('inactive_split.small_pool.freed', 6065), ('inactive_split.small_pool.peak', 11), ('inactive_split_bytes.all.allocated', 5611729408), ('inactive_split_bytes.all.current', 325766656), ('inactive_split_bytes.all.freed', 5285962752), ('inactive_split_bytes.all.peak', 442265600), ('inactive_split_bytes.large_pool.allocated', 3290750976), ('inactive_split_bytes.large_pool.current', 324820992), ('inactive_split_bytes.large_pool.freed', 2965929984), ('inactive_split_bytes.large_pool.peak', 439623680), ('inactive_split_bytes.small_pool.allocated', 2320978432), ('inactive_split_bytes.small_pool.current', 945664), ('inactive_split_bytes.small_pool.freed', 2320032768), ('inactive_split_bytes.small_pool.peak', 3190272), ('max_split_size', -1), ('num_alloc_retries', 2), ('num_ooms', 2), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 81730306121), ('requested_bytes.all.current', 22875736770), ('requested_bytes.all.freed', 58854569351), ('requested_bytes.all.peak', 22899861001), ('requested_bytes.large_pool.allocated', 79482003456), ('requested_bytes.large_pool.current', 22872489984), ('requested_bytes.large_pool.freed', 56609513472), ('requested_bytes.large_pool.peak', 22880878592), ('requested_bytes.small_pool.allocated', 2248302665), ('requested_bytes.small_pool.current', 3246786), ('requested_bytes.small_pool.freed', 2245055879), ('requested_bytes.small_pool.peak', 36757561), ('reserved_bytes.all.allocated', 23299358720), ('reserved_bytes.all.current', 23238541312), ('reserved_bytes.all.freed', 60817408), ('reserved_bytes.all.peak', 23295164416), ('reserved_bytes.large_pool.allocated', 23257415680), ('reserved_bytes.large_pool.current', 23234347008), ('reserved_bytes.large_pool.freed', 23068672), ('reserved_bytes.large_pool.peak', 23257415680), ('reserved_bytes.small_pool.allocated', 41943040), ('reserved_bytes.small_pool.current', 4194304), ('reserved_bytes.small_pool.freed', 37748736), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 203), ('segment.all.current', 184), ('segment.all.freed', 19), ('segment.all.peak', 201), ('segment.large_pool.allocated', 183), ('segment.large_pool.current', 182), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 183), ('segment.small_pool.allocated', 20), ('segment.small_pool.current', 2), ('segment.small_pool.freed', 18), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 928, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 446, in call_batched
    outputs = self.model.generate(input_ids, forced_bos_token_id=self.tokenizer.lang_code_to_id["fra_Latn"], do_sample=False, max_length=max_gen_length, num_beams=beam_size)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 1604, in generate
    return self.beam_search(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/generation/utils.py", line 2902, in beam_search
    outputs = self(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/transformers/models/nllb_moe/modeling_nllb_moe.py", line 1742, in forward
    lm_logits = self.lm_head(outputs[0])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    result = hook(self, args)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/wassim/Archer/archer/runtime/archer_offload.py", line 1220, in _pre_forward_module_hook
    param.data = param.data.to("cuda:0")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 23.69 GiB total capacity; 21.28 GiB already allocated; 1.64 GiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
OrderedDict([('active.all.allocated', 7034), ('active.all.current', 309), ('active.all.freed', 6725), ('active.all.peak', 327), ('active.large_pool.allocated', 329), ('active.large_pool.current', 204), ('active.large_pool.freed', 125), ('active.large_pool.peak', 205), ('active.small_pool.allocated', 6705), ('active.small_pool.current', 105), ('active.small_pool.freed', 6600), ('active.small_pool.peak', 123), ('active_bytes.all.allocated', 39138366464), ('active_bytes.all.current', 22845869056), ('active_bytes.all.freed', 16292497408), ('active_bytes.all.peak', 22910807552), ('active_bytes.large_pool.allocated', 38145318912), ('active_bytes.large_pool.current', 22810509312), ('active_bytes.large_pool.freed', 15334809600), ('active_bytes.large_pool.peak', 22875594752), ('active_bytes.small_pool.allocated', 993047552), ('active_bytes.small_pool.current', 35359744), ('active_bytes.small_pool.freed', 957687808), ('active_bytes.small_pool.peak', 36760576), ('allocated_bytes.all.allocated', 39138366464), ('allocated_bytes.all.current', 22845869056), ('allocated_bytes.all.freed', 16292497408), ('allocated_bytes.all.peak', 22910807552), ('allocated_bytes.large_pool.allocated', 38145318912), ('allocated_bytes.large_pool.current', 22810509312), ('allocated_bytes.large_pool.freed', 15334809600), ('allocated_bytes.large_pool.peak', 22875594752), ('allocated_bytes.small_pool.allocated', 993047552), ('allocated_bytes.small_pool.current', 35359744), ('allocated_bytes.small_pool.freed', 957687808), ('allocated_bytes.small_pool.peak', 36760576), ('allocation.all.allocated', 7034), ('allocation.all.current', 309), ('allocation.all.freed', 6725), ('allocation.all.peak', 327), ('allocation.large_pool.allocated', 329), ('allocation.large_pool.current', 204), ('allocation.large_pool.freed', 125), ('allocation.large_pool.peak', 205), ('allocation.small_pool.allocated', 6705), ('allocation.small_pool.current', 105), ('allocation.small_pool.freed', 6600), ('allocation.small_pool.peak', 123), ('inactive_split.all.allocated', 3209), ('inactive_split.all.current', 179), ('inactive_split.all.freed', 3030), ('inactive_split.all.peak', 184), ('inactive_split.large_pool.allocated', 283), ('inactive_split.large_pool.current', 169), ('inactive_split.large_pool.freed', 114), ('inactive_split.large_pool.peak', 176), ('inactive_split.small_pool.allocated', 2926), ('inactive_split.small_pool.current', 10), ('inactive_split.small_pool.freed', 2916), ('inactive_split.small_pool.peak', 10), ('inactive_split_bytes.all.allocated', 3156116992), ('inactive_split_bytes.all.current', 426226688), ('inactive_split_bytes.all.freed', 2729890304), ('inactive_split_bytes.all.peak', 442266624), ('inactive_split_bytes.large_pool.allocated', 2159403008), ('inactive_split_bytes.large_pool.current', 423837696), ('inactive_split_bytes.large_pool.freed', 1735565312), ('inactive_split_bytes.large_pool.peak', 439623680), ('inactive_split_bytes.small_pool.allocated', 996713984), ('inactive_split_bytes.small_pool.current', 2388992), ('inactive_split_bytes.small_pool.freed', 994324992), ('inactive_split_bytes.small_pool.peak', 3191296), ('max_split_size', -1), ('num_alloc_retries', 1), ('num_ooms', 1), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 39118504732), ('requested_bytes.all.current', 22833988473), ('requested_bytes.all.freed', 16284516259), ('requested_bytes.all.peak', 22899860681), ('requested_bytes.large_pool.allocated', 38127149056), ('requested_bytes.large_pool.current', 22798630912), ('requested_bytes.large_pool.freed', 15328518144), ('requested_bytes.large_pool.peak', 22864650240), ('requested_bytes.small_pool.allocated', 991355676), ('requested_bytes.small_pool.current', 35357561), ('requested_bytes.small_pool.freed', 955998115), ('requested_bytes.small_pool.peak', 36757241), ('reserved_bytes.all.allocated', 23295164416), ('reserved_bytes.all.current', 23272095744), ('reserved_bytes.all.freed', 23068672), ('reserved_bytes.all.peak', 23295164416), ('reserved_bytes.large_pool.allocated', 23257415680), ('reserved_bytes.large_pool.current', 23234347008), ('reserved_bytes.large_pool.freed', 23068672), ('reserved_bytes.large_pool.peak', 23257415680), ('reserved_bytes.small_pool.allocated', 37748736), ('reserved_bytes.small_pool.current', 37748736), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 37748736), ('segment.all.allocated', 201), ('segment.all.current', 200), ('segment.all.freed', 1), ('segment.all.peak', 201), ('segment.large_pool.allocated', 183), ('segment.large_pool.current', 182), ('segment.large_pool.freed', 1), ('segment.large_pool.peak', 183), ('segment.small_pool.allocated', 18), ('segment.small_pool.current', 18), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 18)])
**********
End of traceback message
Attempting to reduce batch size and retrying...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 949, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 518, in call_batched
    forward_times_df = pd.read_csv(self.PATH_TO_LOG_LATENCY, names=["layer_type", "latency_s"])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './metric_logs/nllb-moe_latencies.csv'
OrderedDict([('active.all.allocated', 141542), ('active.all.current', 583), ('active.all.freed', 140959), ('active.all.peak', 846), ('active.large_pool.allocated', 9745), ('active.large_pool.current', 243), ('active.large_pool.freed', 9502), ('active.large_pool.peak', 390), ('active.small_pool.allocated', 131797), ('active.small_pool.current', 340), ('active.small_pool.freed', 131457), ('active.small_pool.peak', 601), ('active_bytes.all.allocated', 76324760064), ('active_bytes.all.current', 15487632384), ('active_bytes.all.freed', 60837127680), ('active_bytes.all.peak', 16171341824), ('active_bytes.large_pool.allocated', 41820433920), ('active_bytes.large_pool.current', 15483666432), ('active_bytes.large_pool.freed', 26336767488), ('active_bytes.large_pool.peak', 16100585984), ('active_bytes.small_pool.allocated', 34504326144), ('active_bytes.small_pool.current', 3965952), ('active_bytes.small_pool.freed', 34500360192), ('active_bytes.small_pool.peak', 220144128), ('allocated_bytes.all.allocated', 76324760064), ('allocated_bytes.all.current', 15487632384), ('allocated_bytes.all.freed', 60837127680), ('allocated_bytes.all.peak', 16171341824), ('allocated_bytes.large_pool.allocated', 41820433920), ('allocated_bytes.large_pool.current', 15483666432), ('allocated_bytes.large_pool.freed', 26336767488), ('allocated_bytes.large_pool.peak', 16100585984), ('allocated_bytes.small_pool.allocated', 34504326144), ('allocated_bytes.small_pool.current', 3965952), ('allocated_bytes.small_pool.freed', 34500360192), ('allocated_bytes.small_pool.peak', 220144128), ('allocation.all.allocated', 141542), ('allocation.all.current', 583), ('allocation.all.freed', 140959), ('allocation.all.peak', 846), ('allocation.large_pool.allocated', 9745), ('allocation.large_pool.current', 243), ('allocation.large_pool.freed', 9502), ('allocation.large_pool.peak', 390), ('allocation.small_pool.allocated', 131797), ('allocation.small_pool.current', 340), ('allocation.small_pool.freed', 131457), ('allocation.small_pool.peak', 601), ('inactive_split.all.allocated', 86539), ('inactive_split.all.current', 101), ('inactive_split.all.freed', 86438), ('inactive_split.all.peak', 227), ('inactive_split.large_pool.allocated', 3036), ('inactive_split.large_pool.current', 49), ('inactive_split.large_pool.freed', 2987), ('inactive_split.large_pool.peak', 84), ('inactive_split.small_pool.allocated', 83503), ('inactive_split.small_pool.current', 52), ('inactive_split.small_pool.freed', 83451), ('inactive_split.small_pool.peak', 178), ('inactive_split_bytes.all.allocated', 70266359808), ('inactive_split_bytes.all.current', 144538624), ('inactive_split_bytes.all.freed', 70121821184), ('inactive_split_bytes.all.peak', 333947904), ('inactive_split_bytes.large_pool.allocated', 35292461568), ('inactive_split_bytes.large_pool.current', 110755840), ('inactive_split_bytes.large_pool.freed', 35181705728), ('inactive_split_bytes.large_pool.peak', 285456896), ('inactive_split_bytes.small_pool.allocated', 34973898240), ('inactive_split_bytes.small_pool.current', 33782784), ('inactive_split_bytes.small_pool.freed', 34940115456), ('inactive_split_bytes.small_pool.peak', 103343616), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 75444295710), ('requested_bytes.all.current', 15486812176), ('requested_bytes.all.freed', 59957483534), ('requested_bytes.all.peak', 16162011865), ('requested_bytes.large_pool.allocated', 40954261448), ('requested_bytes.large_pool.current', 15482847232), ('requested_bytes.large_pool.freed', 25471414216), ('requested_bytes.large_pool.peak', 16091260576), ('requested_bytes.small_pool.allocated', 34490034262), ('requested_bytes.small_pool.current', 3964944), ('requested_bytes.small_pool.freed', 34486069318), ('requested_bytes.small_pool.peak', 220138618), ('reserved_bytes.all.allocated', 16489906176), ('reserved_bytes.all.current', 15632171008), ('reserved_bytes.all.freed', 857735168), ('reserved_bytes.all.peak', 16489906176), ('reserved_bytes.large_pool.allocated', 16223567872), ('reserved_bytes.large_pool.current', 15594422272), ('reserved_bytes.large_pool.freed', 629145600), ('reserved_bytes.large_pool.peak', 16223567872), ('reserved_bytes.small_pool.allocated', 266338304), ('reserved_bytes.small_pool.current', 37748736), ('reserved_bytes.small_pool.freed', 228589568), ('reserved_bytes.small_pool.peak', 266338304), ('segment.all.allocated', 400), ('segment.all.current', 261), ('segment.all.freed', 139), ('segment.all.peak', 400), ('segment.large_pool.allocated', 273), ('segment.large_pool.current', 243), ('segment.large_pool.freed', 30), ('segment.large_pool.peak', 273), ('segment.small_pool.allocated', 127), ('segment.small_pool.current', 18), ('segment.small_pool.freed', 109), ('segment.small_pool.peak', 127)])
**********
End of traceback message
Continuing with errors...
ERROR: Exception occured during runtime!
Hyperparameters: model_name: facebook/nllb-moe-54b, Batch_size: 2, dataset_size: 1, max_gen_length: 128, beam_size: 4, max_input_seq_length: -1, tokenizer_padding_setting: pad_to_max_length, dataset_name= wmt14 
Full traceback message:
**********Traceback (most recent call last):
  File "benchmark_suite.py", line 945, in <module>
    evalEngine.call_batched(batch_size=batch_size,
  File "benchmark_suite.py", line 514, in call_batched
    forward_times_df = pd.read_csv(self.PATH_TO_LOG_LATENCY, names=["layer_type", "latency_s"])
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/home/wassim/miniconda3/envs/archer/lib/python3.8/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: './metric_logs/nllb-moe_latencies.csv'
OrderedDict([('active.all.allocated', 141542), ('active.all.current', 583), ('active.all.freed', 140959), ('active.all.peak', 846), ('active.large_pool.allocated', 9745), ('active.large_pool.current', 243), ('active.large_pool.freed', 9502), ('active.large_pool.peak', 390), ('active.small_pool.allocated', 131797), ('active.small_pool.current', 340), ('active.small_pool.freed', 131457), ('active.small_pool.peak', 601), ('active_bytes.all.allocated', 76324760064), ('active_bytes.all.current', 15487632384), ('active_bytes.all.freed', 60837127680), ('active_bytes.all.peak', 16171341824), ('active_bytes.large_pool.allocated', 41820433920), ('active_bytes.large_pool.current', 15483666432), ('active_bytes.large_pool.freed', 26336767488), ('active_bytes.large_pool.peak', 16100585984), ('active_bytes.small_pool.allocated', 34504326144), ('active_bytes.small_pool.current', 3965952), ('active_bytes.small_pool.freed', 34500360192), ('active_bytes.small_pool.peak', 220144128), ('allocated_bytes.all.allocated', 76324760064), ('allocated_bytes.all.current', 15487632384), ('allocated_bytes.all.freed', 60837127680), ('allocated_bytes.all.peak', 16171341824), ('allocated_bytes.large_pool.allocated', 41820433920), ('allocated_bytes.large_pool.current', 15483666432), ('allocated_bytes.large_pool.freed', 26336767488), ('allocated_bytes.large_pool.peak', 16100585984), ('allocated_bytes.small_pool.allocated', 34504326144), ('allocated_bytes.small_pool.current', 3965952), ('allocated_bytes.small_pool.freed', 34500360192), ('allocated_bytes.small_pool.peak', 220144128), ('allocation.all.allocated', 141542), ('allocation.all.current', 583), ('allocation.all.freed', 140959), ('allocation.all.peak', 846), ('allocation.large_pool.allocated', 9745), ('allocation.large_pool.current', 243), ('allocation.large_pool.freed', 9502), ('allocation.large_pool.peak', 390), ('allocation.small_pool.allocated', 131797), ('allocation.small_pool.current', 340), ('allocation.small_pool.freed', 131457), ('allocation.small_pool.peak', 601), ('inactive_split.all.allocated', 86539), ('inactive_split.all.current', 101), ('inactive_split.all.freed', 86438), ('inactive_split.all.peak', 227), ('inactive_split.large_pool.allocated', 3036), ('inactive_split.large_pool.current', 49), ('inactive_split.large_pool.freed', 2987), ('inactive_split.large_pool.peak', 84), ('inactive_split.small_pool.allocated', 83503), ('inactive_split.small_pool.current', 52), ('inactive_split.small_pool.freed', 83451), ('inactive_split.small_pool.peak', 178), ('inactive_split_bytes.all.allocated', 70266359808), ('inactive_split_bytes.all.current', 144538624), ('inactive_split_bytes.all.freed', 70121821184), ('inactive_split_bytes.all.peak', 333947904), ('inactive_split_bytes.large_pool.allocated', 35292461568), ('inactive_split_bytes.large_pool.current', 110755840), ('inactive_split_bytes.large_pool.freed', 35181705728), ('inactive_split_bytes.large_pool.peak', 285456896), ('inactive_split_bytes.small_pool.allocated', 34973898240), ('inactive_split_bytes.small_pool.current', 33782784), ('inactive_split_bytes.small_pool.freed', 34940115456), ('inactive_split_bytes.small_pool.peak', 103343616), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('requested_bytes.all.allocated', 75444295710), ('requested_bytes.all.current', 15486812176), ('requested_bytes.all.freed', 59957483534), ('requested_bytes.all.peak', 16162011865), ('requested_bytes.large_pool.allocated', 40954261448), ('requested_bytes.large_pool.current', 15482847232), ('requested_bytes.large_pool.freed', 25471414216), ('requested_bytes.large_pool.peak', 16091260576), ('requested_bytes.small_pool.allocated', 34490034262), ('requested_bytes.small_pool.current', 3964944), ('requested_bytes.small_pool.freed', 34486069318), ('requested_bytes.small_pool.peak', 220138618), ('reserved_bytes.all.allocated', 16489906176), ('reserved_bytes.all.current', 15632171008), ('reserved_bytes.all.freed', 857735168), ('reserved_bytes.all.peak', 16489906176), ('reserved_bytes.large_pool.allocated', 16223567872), ('reserved_bytes.large_pool.current', 15594422272), ('reserved_bytes.large_pool.freed', 629145600), ('reserved_bytes.large_pool.peak', 16223567872), ('reserved_bytes.small_pool.allocated', 266338304), ('reserved_bytes.small_pool.current', 37748736), ('reserved_bytes.small_pool.freed', 228589568), ('reserved_bytes.small_pool.peak', 266338304), ('segment.all.allocated', 400), ('segment.all.current', 261), ('segment.all.freed', 139), ('segment.all.peak', 400), ('segment.large_pool.allocated', 273), ('segment.large_pool.current', 243), ('segment.large_pool.freed', 30), ('segment.large_pool.peak', 273), ('segment.small_pool.allocated', 127), ('segment.small_pool.current', 18), ('segment.small_pool.freed', 109), ('segment.small_pool.peak', 127)])
**********
End of traceback message
Continuing with errors...
